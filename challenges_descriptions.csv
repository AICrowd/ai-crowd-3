id,description
224,"<p><img src=""https://i.imgur.com/adoKrv5.png"" alt=""deepracer_scene"" /></p>

<h1 class=""mt-0"" id=""introduction"">Introduction</h1>

<p>AWS DeepRacer provides hardware and cloud-based service for end-to-end experimentation with reinforcement learning (RL) and can be used to systematically investigate the key challenges in building intelligent control systems in real-world.
As an educational and experimental tool for RL, it is the first successful customizable large-scale deployment of deep reinforcement learning (DRL) on a vision-based robotic control agent, 20 AWS Summit Races thus far with 1600 participants (that is on leaderboard so all can see) and in console, also visible to all, we have had 4500+ participants. 
The robocar uses only raw camera image as observations and a model-free learning method to perform robust path planning.</p>

<p>One of the major challenges for deployment in the real world of RL agents is in the simulation to real world transfer (sim2real). In the context of DeepRacer, we can divide the sim2real problem into two parts</p>

<ul>
  <li>Perception</li>
  <li>Dynamics</li>
</ul>

<p>Simulators may not have the visual fidelity as we do in the real world and not be able to capture the physics of the real world. Both of these factors can affect the representing the real environment in the simulator effectively. As a result, sometimes even after obtaining desired objective in simulation environment, when running the model in the real world, we may experience failures.</p>

<p>In DeepRacer, we realize that the simulation fidelity is not an exact match of the real world and the physics engine uses approximations of the friction coefficients and other inertial parameters. But the beauty of DRL is that we do not require everything to be perfect.</p>

<p>To mitigate large perceptual change affecting the car, we make two major assumptions</p>

<ul>
  <li>instead of using an RGB image, we grayscale the image to make the perceptual differences between simulator and the real-world narrower</li>
  <li>we intentionally use a shallow image feature embedder, i.e. we only use a few CNN layers, this helps the network not learn the simulation environment entirely but enough to adjust to small variations in environment.</li>
</ul>

<p>By default the DeepRacer model uses a clipped PPO algorithm that runs in asynchronous mode. Using these parameters, over 5000 developers around the world have built models in the simulator and successfully navigated a racing track in the real world.</p>

<p>In this challenge, we’ll take robustness in the real world to a new level. While the models so far have been trained in the simulator and tested on visually similar tracks in the real world, we introduce the following modifications</p>

<ul>
  <li>Visual changes to the track, partial track to be made with tape</li>
  <li>Track dimensions to vary</li>
  <li>Variations in lighting</li>
  <li>(May be we can add snow like objects on the track to distract the cars?)</li>
</ul>

<h1 id=""problem-statement"">Problem Statement</h1>

<p>In this challenge, you will train a model which will be evaluated on a undisclosed test track, which will be significantly different from the tracks that will be available in the simulator. Each model will be evaluated on the test track, where it will have five attempts to complete a lap and the fastest completed lap will be recorded as the submission.</p>

<p>We’ll focus on both perception and dynamics. We’ll introduce lighting variations in the track environment such as directed head lights and LED light array. In addition to test model robustness in dynamics, we’ll inject random actions at inference time on the DeepRacer car. A good model is expected to overcome advanced perceptual changes, and random perturbations in the action space and navigate the track successfully. Each model will have three attempts to clock successful lap times, with the fastest lap being considered for the submission.</p>

<h1 id=""available-resources"">Available Resources</h1>

<ul>
  <li><a href=""https://github.com/awslabs/amazon-sagemaker-examples/tree/master/reinforcement_learning/rl_deepracer_robomaker_coach_gazebo"">Github code for Amazon SageMaker notebook environment</a></li>
  <li><a href=""https://docs.aws.amazon.com/deepracer/latest/developerguide/train-evaluate-models-using-sagemaker-notebook.html"">Documentation on Amazon SageMaker DeepRacer notebook</a></li>
  <li><a href=""https://arxiv.org/abs/1911.01562"">DeepRacer: Educational Autonomous Racing Platform for Experimentation with Sim2Real Reinforcement Learning</a></li>
  <li><a href=""https://docs.aws.amazon.com/deepracer/latest/developerguide/what-is-deepracer.html"">Tutorial and documentation on DeepRacer</a></li>
  <li><a href=""https://github.com/aws-robotics/aws-robomaker-sample-application-deepracer"">Github code for AWS RoboMaker DeepRacer simulation application and documentation</a></li>
  <li><a href=""https://neurips-aido3-awsdeepracer.s3.us-east-2.amazonaws.com/neurips_deepracer_dataset.tar.gz"">Track Image Dataset</a></li>
</ul>

<h1 id=""potential-avenues-to-explore"">Potential avenues to explore</h1>

<ul>
  <li>Optimize PPO algorithm</li>
  <li><a href=""https://github.com/aws-samples/aws-deepracer-workshops/tree/master/Advanced%20workshops/AI%20Driving%20Olympics%202019/challenge_train_DQN"">Explore other RL algorithms</a></li>
  <li><a href=""https://github.com/aws-samples/aws-deepracer-workshops/tree/master/Advanced%20workshops/AI%20Driving%20Olympics%202019/challenge_train_w_PPO"">Data Augmentation outside of the simulation</a></li>
  <li><a href=""https://github.com/aws-samples/aws-deepracer-workshops/tree/master/Advanced%20workshops/AI%20Driving%20Olympics%202019/challenge_train_w_PPO"">Modifying the assets in simulation</a></li>
  <li>Regularization and other hyper parameter optimization</li>
  <li>Split the learning process ( Learn to see; learn to act )</li>
</ul>

<h1 id=""timeline"">Timeline</h1>

<p>Competition starts in the first week with March 2020 with the finals in May 2020.</p>

<h1 id=""prizes"">Prizes</h1>

<p>To be announced soon</p>

<h1 id=""how-to-submit-models-and-artifacts"">How to submit models and artifacts</h1>

<p>You will need to prepare a list of model and log files to prepare your submission and participate in the challenge. The complete instructions will be added for this challenge soon.</p>

<p>For questions, please check <a href=""https://discourse.aicrowd.com/c/aws-deepracer-benchmark"">AIcrowd AWS DeepRacer Discourse</a> page</p>
"
70,"
<h1 id=""introduction-to-the-problem"">Introduction to the Problem</h1>

<p><strong>TafMek</strong> has a new brand director in Spain. Since joining the team she has spent a considerable amount of time in understanding the market and meeting key customers and team members.</p>

<ul>
  <li>She is preparing for a 2020 strategy think-tank meet next week with the sales and marketing team where she wants to introduce a data driven approach for sales and marketing planning. From her past experience she knows that presenting data driven insights will help in driving the discussions well.</li>
</ul>

<p>First she sits down to jot down the key points about the brand &amp; market based on her understanding in past 2 months</p>

<p>_ _</p>

<p><strong>Brand and market:</strong></p>

<p>-
  - Taf Mek is seeing strong growth and the franchise relies on Taf Mek’s performance for overall growth and to fund new launches
  - However new competition in targeted therapy segment (like Taf Mek) is coming in with strong marketing and sales investments
  - There is strong brand equity with Drs but the penetration of Immunotherapy drugs are a key threat for the brand growth</p>

<p><strong>Execution &amp; implementation:</strong></p>

<p>-
  - Sales and marketing initiatives have been implemented per plan in 2019. New channels have also been implemented (Rep triggered e mails etc.) and the team feels they are having a positive impact
  - Doctor list revised every year.
  - Calls frequency higher than planned</p>

<p><strong>Outlook for next year</strong></p>

<p>-
  - Aggressive sales targets for next year. Team is upbeat. Looking forward to next year.
  - Budgets as always under stretch
  - Lots of new ideas in pipeline but need to prioritize to manage budgets.</p>

<p><strong>Key questions in mind for data driven insights</strong></p>

<p>-
  - How many different market archetypes are there?
  - What is the Return on Investment (RoI) for the different promotional channels being used?
    - Do they differ by different areas/accounts/customers?
  - Are we adequately spending what we have? Is there a way to maximize the return?</p>

<p>How can we empower our team members to leverage data easily through a tool for more informed decision making</p>

<h1 id=""tasks-and-evaluation-metric"">Tasks and Evaluation Metric</h1>

<table>
  <thead>
    <tr>
      <th>#</th>
      <th>Task</th>
      <th>Description</th>
      <th>Evaluation</th>
      <th>Weightage</th>
      <th>Timing</th>
      <th>Note</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>Download the data</td>
      <td>Exponentially decaying score based on the timestamp of the first submission made by the team</td>
      <td>Speed. First one gets 1, then decay in increments of 0.1</td>
      <td>10%</td>
      <td>ASAP</td>
      <td> </td>
    </tr>
    <tr>
      <td>2</td>
      <td>Primary Key</td>
      <td>Identify the primary keys in the data</td>
      <td>% of files correctly identified</td>
      <td>10%</td>
      <td>Any Time</td>
      <td> </td>
    </tr>
    <tr>
      <td>3</td>
      <td>Market Arcehtypes</td>
      <td>Identify the diffetent market archetypes using clustering</td>
      <td>Silhouette score of the market archetypes using the descriptor variables, transformed to 0 &amp; 1</td>
      <td>30%</td>
      <td>Any Time</td>
      <td>Use the template to give the final hospital ID, market archetype and the descriptor variables in the martket archetypes</td>
    </tr>
    <tr>
      <td>4</td>
      <td>Predict Sales</td>
      <td>Predict Sales for 2019 Jan - 2019 March</td>
      <td>1-MAPE, floored at 0</td>
      <td>35%</td>
      <td>Any Time</td>
      <td> </td>
    </tr>
    <tr>
      <td>5</td>
      <td>Optimization</td>
      <td>Using a given budget for 6 months (Euro 830k), how to generate maximum sales in Euros.</td>
      <td>Heuristic evaluation, post final submission with suggested strategies</td>
      <td>15%</td>
      <td>At the end of the hackathon. Optional: Please provide a write up on your strategy to achieve the recommended optimization number</td>
      <td>No promotion can change by more than 25%</td>
    </tr>
    <tr>
      <td>6</td>
      <td>Visualization</td>
      <td>Provide easy to understand but compelling visualizations of the data and predictions</td>
      <td>Jury to select the winning entries</td>
      <td>0%</td>
      <td>Submission at 16:30 on September 19th.</td>
      <td>This is a separate competition to the prediction challenge.</td>
    </tr>
  </tbody>
</table>

<h1 id=""description-of-the-data"">Description of the Data</h1>

<p>_Need to update the process of the data download</p>

<p><strong>Sales –</strong> The file contains Hospital/Account level sales(units) data for the brands Tafinlar &amp; Mekinist(TafMek) by SKU for each month starting Feb’17. You might encounter negative values for a few records; these are the returns coming from the hospitals/accounts. You will need to combine and normalize the brand sales for both the brands combined using the normal dosing regimen (for a month for a patient) information. Please see the dosage information for Taf-Mek and convert the sales Taf and Mek units into 30 day equivalent for 1 patient, in suggested dosage.</p>

<p><a href=""https://www.hcp.novartis.com/products/tafinlar-mekinist/advanced-melanoma/dosing-and-administration/"">https://www.hcp.novartis.com/products/tafinlar-mekinist/advanced-melanoma/dosing-and-administration/</a></p>

<p><strong>Promotions –</strong> The different promotional channels, which the TafMek brand team uses to reach to the customers. The promotional channels used are –</p>

<ul>
  <li>F2F (Face to Face) Calls – Sales/Medical Representative(Reps) detailing the Health Care Professionals (HCPs) about the brand. In the data provided, you will have calls data for a few more brands as well. Filter on the Detailed Products column to get only TafMek calls</li>
  <li>Emails – there are 2 broad categories of emails:
    <ul>
      <li>Rep Triggered Emails (RTE) – Emails sent by the Reps to the HCPs</li>
      <li>Mass Emails – Email campaigns are carried out during international conferences and major events. They are sent together to a group of HCPs (usually by 3rd party agencies on behalf of Novartis) rather than being sent by a specific Rep to a specific HCP (RTE)</li>
    </ul>
  </li>
  <li>Events – Events are congresses or meetings sponsored by Novartis where one or more than one HCPs are invited. Events are categorized as Local, National or International (EU or US) events</li>
</ul>

<p><strong>Targeting –</strong> Targeting files contains various segmentation information utilized by the brand team for prioritizing promotional efforts. In the files you will find,</p>

<ul>
  <li>Attitudinal Segmentation – Strategic HCP segments identified by the Brand team</li>
  <li>Hospital Potential – Segmentation of Hospitals/Accounts based on potential</li>
  <li>Potential by Account – Contains information on Hospital's Population, Incidence and Prevalence rates for Melanoma</li>
  <li>National Targeting – It is a questionnaire filled every month by a small set of HCPs/Reps with questions like total patients on TafMek, new patients on TafMek the HCP has</li>
  <li>Tiering – HCP level call plans for each Rep and HCP Tiering for 6 different call plan cycles</li>
</ul>

<p><strong>Others –</strong> These are set of files containing varied information. You will need to utilize this information at different phases from stitching together a final dataset to refining your analysis and final outputs.</p>

<ul>
  <li>Centers with Diagnostic Tools contains the names of Hospitals/Accounts, which have the facility for carrying out a special test to detect BRAF gene mutation(primary requirement to prescribe TafMek to patient)</li>
  <li>Value Transfer NVS Congress and Competitors contains the list of events which were carried out by Novartis and its competitors</li>
  <li>Centres en COMBI contains the list of HCPs along-with the Hospital/Account name and no. of patients that they have enrolled in the COMBI AD clinical trial</li>
  <li>Event to Event Type mapping file should be used to identify Local, National etc. events</li>
  <li>Hospital Name to CNH ID mapping file contains 2 versions of the mapping for Hospital Names to IDs. You will need this to integrate all the datasets to arrive at final Hospital/Account level analysis dataset</li>
</ul>

<p><strong>Cost Assumptions</strong> file contains the per unit cost of promotions and the average monthly cost of TafMek for 1 patient</p>

<h1 id=""submission-instructions"">Submission Instructions</h1>

<p><strong>NOTE</strong> : You will first need to agree to the rules of the competition by clicking on the <code class=""highlighter-rouge"">Participate</code> button on the challenge page.</p>

<ul>
  <li>Download the evaluation template from : <a href=""https://www.aicrowd.com/challenges/novartis-pharma-data-science-hackathon/dataset_files"">here</a></li>
  <li>Unzip the file to get 4 different <code class=""highlighter-rouge"">.csv</code>  files.</li>
  <li>Update the values in these <code class=""highlighter-rouge"">CSV</code> files based on your approach</li>
  <li>When you are ready to submit, zip the <code class=""highlighter-rouge"">CSV</code> files into a single ZIP file such that <strong>all the csv files are at the root level of the zip file</strong></li>
  <li>Upload the Zip file here : https://www.aicrowd.com/challenges/novartis-pharma-data-science-hackathon/submissions/new</li>
</ul>

<h1 id=""contact"">Contact</h1>
<ul>
  <li>Discussion Forums : <a href=""https://discourse.aicrowd.com/c/novartis-pharma-data-science-hackathon"">https://discourse.aicrowd.com/c/novartis-pharma-data-science-hackathon</a></li>
  <li>Emails :
    <ul>
      <li>mohanty@aicrowd.com (to be replaced by a few novartis emails)</li>
    </ul>
  </li>
</ul>
"
149,"<blockquote>
  <p><strong>Update 11/02</strong>: For urgent issues please visit <a href=""https://gitter.im/crowdAI/NIPS-Learning-To-Run-Challenge"">gitter channel</a> and <a href=""https://github.com/stanfordnmbl/osim-rl/issues/164"">the Round 2 github issue</a></p>
</blockquote>

<blockquote>
  <p><strong>Update 11/01</strong>: Google Cloud Platform will generously sponsor also the second round of the challenge. Top 50 teams from the Round 1 will be awarded 400$ cloud credits!</p>
</blockquote>

<blockquote>
  <p><strong>Update 10/31</strong>: Instructions for submission for Round-2 are available at : <a href=""https://github.com/crowdAI/nips2018-ai-for-prosthetics-round2-starter-kit"">https://github.com/crowdAI/nips2018-ai-for-prosthetics-round2-starter-kit</a></p>
</blockquote>

<blockquote>
  <p><strong>Update 08/27:</strong> We’ve updated osim-rl package to version 2.1. If you joined before 08/27/2018 please run in your conda environment:<br /><br />
<code class=""highlighter-rouge"">
pip install git+https://github.com/stanfordnmbl/osim-rl.git -U
</code></p>
</blockquote>

<blockquote>
  <p><strong>Update 07/27:</strong> Google Cloud Platform will sponsor participants of the challenge. Top 400 teams with positive (&gt;0) number of points will be awarded 250$ cloud credits!</p>
</blockquote>

<blockquote>
  <p><strong>Update 07/30:</strong> Watch our <a href=""https://www.youtube.com/watch?v=M2D5xSSxshE"">webinar</a> to learn more about biomechanics, neuroscience and reinforcement learning!</p>
</blockquote>

<p>Welcome to <strong>AI for Prosthetics challenge</strong>, one of the official challenges in the <a href=""https://nips.cc/Conferences/2018/CompetitionTrack"">NeurIPS 2018 Competition Track</a>. In this competition, you are tasked with developing a controller to enable a physiologically-based human model with a prosthetic leg to walk and run. You are provided with a human musculoskeletal model, a physics-based simulation environment <a href=""http://opensim.stanford.edu/"">OpenSim</a> where you can synthesize physically and physiologically accurate motion, and datasets of normal gait kinematics. You are scored based on how well your agent adapts to the requested velocity vector changing in real time.</p>

<p>Follow the instructions on our <a href=""https://github.com/stanfordnmbl/osim-rl"">github repo</a> to get started!</p>

<p><a href=""https://github.com/stanfordnmbl/osim-rl""><img src=""https://s3-eu-west-1.amazonaws.com/kidzinski/nips-challenge/images/ai-prosthetics.jpg"" alt=""AI for prosthetics"" /></a></p>

<p>Our objectives are to:</p>

<ul>
  <li>bring Deep Reinforcement Learning to solve problems in medicine,</li>
  <li>promote open-source tools in RL research (the physics simulator <a href=""http://opensim.stanford.edu/"">OpenSim</a>, the RL environment, and the competition platform are all open-source),</li>
  <li>encourage RL research in computationally complex environments, with stochasticity and highly-dimensional action spaces.</li>
</ul>

<p>Visit our <a href=""https://github.com/stanfordnmbl/osim-rl"">github repo</a> to get started!</p>

<h2 id=""whats-new-compared-to-nips-2017-learning-to-run"">What’s new compared to NIPS 2017: Learning to run?</h2>

<p>We took into account comments from the last challenge and there are several changes:</p>

<ul>
  <li><strong>You can use experimental data (to greatly speed up the learning process)</strong></li>
  <li>We released the 3rd dimensions <a href=""http://opensim.stanford.edu/"">OpenSim</a> model (the model can fall sideways)</li>
  <li>We added a prosthetic leg – the goal is to solve a medical challenge on modeling how walking will change after getting a prosthesis. Your work can speed up design, prototying, or tuning prosthetics!</li>
</ul>

<p>You haven’t heard of NIPS 2017: Learning to run? <a href=""https://www.youtube.com/watch?v=rhNxt0VccsE"">Watch this video!</a></p>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>
<p>Your task is to build a function f which takes the current state observation (a dictionary describing the current state) and returns the muscle excitations action (19-dimensional vector) maximizing the total reward. The objective is to follow the requested velocity vector. The trial ends either if the pelvis of the model falls below 0.6 meters or if you reach 1000 iterations (corresponding to 10 seconds in the virtual environment).</p>

<p><a href=""https://github.com/stanfordnmbl/osim-rl""><img src=""https://s3.amazonaws.com/osim-rl/videos/running.gif"" alt=""Solution from 2017"" /></a></p>

<h3 id=""round-1"">Round 1</h3>
<p>The total reward is 9 * s - p * p where s is the number of steps before reaching one of the stop criteria and p is the absolute difference between horizonal velocity and 3. You can interpret it as a request to run at a constat speed of 3 meters per second.</p>

<h3 id=""round-2"">Round 2</h3>
<p>In the second round the task is also to follow a requested velocity vector. However, in this round the vector will change in time and it will be a random process. We will provide the distribution of this process in mid-July.</p>

<h3 id=""timeline"">Timeline</h3>

<ul>
  <li>Round 1: 16.06.2018 - 28.10.2018</li>
  <li>Round 2 (test run): 1.10.2018 - 28.10.2018</li>
  <li>Round 2: 29.10.2018 - 6.11.2018</li>
</ul>

<p>Top 50 teams from <strong>Round 1</strong> qualify to <strong>Round 2</strong></p>

<h3 id=""resources"">Resources</h3>

<p>Please visit <a href=""http://osim-rl.stanford.edu"">osim-rl project’s website</a> for resources on biomechanics and reinforcement learning, solutions from the NeurIPS 2017 and other materials. 
Visit <a href=""http://opensim.stanford.edu/"">OpenSim</a> website for materials on musculoskeletal simulations.</p>

<p>Here are some interesting blog posts written by participants:</p>

<ul>
  <li><a href=""https://www.endtoend.ai/blog/ai-for-prosthetics-1"">Understanding the Challenge</a></li>
  <li><a href=""https://www.endtoend.ai/blog/ai-for-prosthetics-2"">Understanding the Action Space</a></li>
  <li><a href=""https://www.endtoend.ai/blog/ai-for-prosthetics-3"">Understanding the Observation Space</a></li>
  <li><a href=""https://www.endtoend.ai/blog/ai-for-prosthetics-5"">Understanding the Reward</a></li>
</ul>

<p>Here are some helper libraries written by participants:</p>

<ul>
  <li><a href=""https://github.com/seungjaeryanlee/osim-rl-helper"">https://github.com/seungjaeryanlee/osim-rl-helper</a></li>
</ul>

<p>Here are some articles and blog posts written by participants about NIPS 2017: Learning to Run Challenge (can very helpful for this challenge as well):</p>

<ul>
  <li><a href=""https://medium.com/mlreview/our-nips-2017-learning-to-run-approach-b80a295d3bb5"">https://medium.com/mlreview/our-nips-2017-learning-to-run-approach-b80a295d3bb5</a></li>
  <li><a href=""https://arxiv.org/abs/1711.06922"">https://arxiv.org/abs/1711.06922</a></li>
  <li><a href=""https://medium.com/@scitator/run-skeleton-run-3rd-place-solution-for-nips-2017-learning-to-run-207f9cc341f8"">https://medium.com/@scitator/run-skeleton-run-3rd-place-solution-for-nips-2017-learning-to-run-207f9cc341f8</a></li>
</ul>

<h2 id=""contact-us"">Contact Us</h2>

<p>Use one of the public channels:</p>

<ul>
  <li>Gitter Channel : <a href=""https://gitter.im/crowdAI/NIPS-Learning-To-Run-Challenge"">crowdAI/NIPS-Learning-To-Run-Challenge</a></li>
  <li>Technical issues : <a href=""https://github.com/stanfordnmbl/osim-rl/issues"" target=""_blank"">https://github.com/stanfordnmbl/osim-rl/issues </a></li>
  <li>Discussion Forum : <a href=""https://www.crowdai.org/challenges/nips-2018-ai-for-prosthetics-challenge/topics"">https://www.crowdai.org/challenges/nips-2018-ai-for-prosthetics-challenge/topics</a></li>
</ul>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organisers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

<ul>
  <li><a href=""mailto:lukasz.kidzinski@stanford.edu"" target=""_blank"">lukasz.kidzinski@stanford.edu</a></li>
  <li><a href=""mailto:sharada.mohanty@epfl.ch"" target=""_blank"">sharada.mohanty@epfl.ch</a></li>
</ul>

<h2 id=""media"">Media</h2>

<p><a href=""https://techcrunch.com/2017/08/07/dueling-ais-compete-in-learning-to-walk-secretly-manipulating-images-and-more-at-nips/""><img src=""https://seeklogo.com/images/T/techcrunch-logo-B444826970-seeklogo.com.png"" alt=""TechCrunch"" class=""img-logo"" /></a>
<a href=""http://med.stanford.edu/news/all-news/2018/07/virtual-athletes-compete-to-take-on-a-medical-challenge.html""><img src=""https://cehg.stanford.edu/sites/default/files/styles/large-scaled/public/c876e3f31ce0c5ba771fbdccdcb3c1dc.png?itok=-83R2NJW"" alt=""Stanford News"" class=""img-logo"" /></a>
<a href=""http://insights.globalspec.com/article/6167/watch-computer-generated-skeletons-run-for-cerebral-palsy""><img src=""https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/50fa0a860a431c503132b1fa0cac8377_logo%20%283%29.png"" alt=""IEEE"" class=""img-logo"" /></a></p>

<h3 id=""prizes"">Prizes</h3>

<p>We will provide the full list of prizes in mid-July.
Prizes confirmed for now include:</p>

<ul>
  <li>1st - 2x <a href=""https://www.nvidia.com/en-us/titan/titan-v/"" target=""_blank"">NVIDIA Titan V</a></li>
  <li>2nd - <a href=""https://www.nvidia.com/en-us/titan/titan-v/"" target=""_blank"">NVIDIA Titan V</a></li>
  <li>3rd - <a href=""https://www.nvidia.com/en-us/titan/titan-v/"" target=""_blank"">NVIDIA Titan V</a></li>
</ul>

<p>Additionally:</p>

<ul>
  <li>Invitation to publish articles in the NeurIPS competition book.</li>
  <li>Invitation to the 3rd <a href=""https://www.appliedmldays.org"">Applied Machine Learning Days</a> at EPFL in Switzerland on January 26 - 29, 2019, with travel and accommodation covered.</li>
  <li>Invitation to give a research talk at Stanford, with travel and accommodation covered.</li>
  <li>Reimbursement of travel and accommodation at NeurIPS 2018</li>
</ul>

<h3 id=""datasets-license"">Datasets License</h3>

"
11,something
18,"<blockquote class=""update"">
  <p>The Challenge has ended! Thank you for all the great contributions!!! The winners of the challenge will be notified and published soon.</p>
</blockquote>

<p><img src=""https://s3.eu-central-1.amazonaws.com/aicrowd-static/SBB/images/Flatland_Logo.svg"" alt=""flatland_logo"" /></p>

<p><strong>The key question we want to answer here is: How can trains learn to automatically coordinate among themselves, so that there are minimal delays in large train networks ?</strong></p>

<h1 id=""abstract"">Abstract</h1>

<p><em>The Flatland Challenge is a competition to foster progress in multi-agent reinforcement learning for any <a href=""https://en.wikipedia.org/wiki/Vehicle_rescheduling_problem"">re-scheduling problem (RSP)</a>. The challenge addresses a real-world problem faced by many transportation and logistics companies around the world (such as the Swiss Federal Railways, SBB. Different tasks related to RSP on a simplified 2D multi-agent railway simulation must be solved. Your contribution may shape the way modern traffic management systems (TMS) are implemented not only in railway but also in other areas of transportation and logistics. This will be the first of a series of challenges related to re-scheduling and complex transportation systems.</em></p>

<h1 id=""background"">Background</h1>

<p>The Swiss Federal Railways (SBB) operate the densest mixed railway traffic in the world. SBB maintain and operate the biggest railway infrastructure in Switzerland. Today, there are more than 10,000 trains running each day, being routed over 13,000 switches and controlled by more than 32,000 signals. Each day 1.2 million passengers and almost half of Switzerland’s volume of transported goods are transported on this railway network. Due to the growing demand for mobility, SBB needs to increase the transportation capacity of the network by approximately 30% in the future.</p>

<p>The increase in transport capacity can be achieved through different measures, such as <a href=""https://smartrail40.ch/index.asp?inc=&amp;lang=en"">denser train schedules, investments in new infrastructure, and/or investments in new rolling stock</a>. However, SBB currently lack suitable technologies and tools to quantitatively assess these different measures.</p>

<p>A promising solution to this dilemma is a complete railway simulation that efficiently evaluates the consequences of infrastructure changes or schedule adaptations for network stability and traffic flow. A complete railway simulation consists of a full dynamical physics simulation as well as an automated traffic management system.</p>

<p><img src=""https://s3.eu-central-1.amazonaws.com/aicrowd-static/SBB/images/Flatland_Preview.svg"" alt=""flatland_visual"" />
<em><strong>Flatland</strong>: This image illustrates an early draft of the environment visualization. The core task of this challenge is to manage and maintain railway traffic on complex scenarios in complex networks.</em></p>

<p>The research group at SBB has developed a high-performance simulator which simulates the dynamics of train traffic as well as the railway infrastructure. Different approaches for <a href=""https://on-demand-gtc.gputechconf.com/gtcnew/on-demand-gtc.php?searchByKeyword=erik%20nygren&amp;searchItems=&amp;sessionTopic=&amp;sessionEvent=&amp;sessionYear=&amp;sessionFormat=&amp;submit=&amp;select="">automated traffic management systems (TMS)</a> are currently under investigation. The role of the traffic management system is to select routes for all trains and decide on their priorities at switches in order to optimize traffic flow across the network.</p>

<p>At the core of this challenge lies the general vehicle re-scheduling problem (VRSP) proposed by Li, Mirchandani and Borenstein in 2007:</p>

<blockquote class=""update"">
  <p>The vehicle rescheduling problem (VRSP) arises when a previously assigned trip is disrupted. A traffic accident, a medical emergency, or a breakdown of a vehicle are examples of possible disruptions that demand the rescheduling of vehicle trips. The VRSP can be approached as a dynamic version of the classical vehicle scheduling problem (VSP) where assignments are generated dynamically.</p>
</blockquote>

<p>The “Flatland” Competition aims to address the vehicle rescheduling problem by providing a simplistic grid world environment and allowing for diverse solution approaches. The challenge is open to any methodological approach, e.g. from the domain of reinforcement learning or of operations research.</p>

<p>The problems are formulated as a 2D grid environment with restricted transitions between neighboring cells to represent railway networks. On the 2D grid, multiple agents with different objectives must collaborate to maximize global reward. There is a range of tasks with increasing difficulty that need to be solved as explained in the coming sections.</p>

<h1 id=""tasks"">Tasks</h1>

<p>The challenge requires your creativity and savviness. In 3 submission rounds with increasing difficulty, you can prove that you have what it takes. We invite you to enter the race with your unique solution and to win great prizes - at the same time solving one of the key challenges in the world of transportation!</p>

<p>Here is a teaser of what we expect you to do:
<img src=""https://i.imgur.com/9cNtWjs.gif"" alt=""Teaser"" />
Your overall goal is to make all agents (trains) arrive at their target destination with a minimal travel time. In other words, we want to minimize the time steps (or wait time) that it takes for each agent in the group to reach its destination.</p>

<p>Let’s say in a scenario with n-agents, the travel time is measured by the collected amount of timesteps all the agents have until the n-th agent arrives at its destination.</p>

<h2 id=""can-you-design-the-best-performing-agent"">1. Can you design the best-performing agent?</h2>

<p>Design the best-performing agent. At the more basic levels, the agents may achieve their goals using ad-hoc decisions. But as difficulty increases from round to round, the agents have to be able to plan ahead, i.e. with increasing difficulty, planning becomes more relevant!</p>

<h2 id=""can-you-design-the-best-observation"">2. Can you design the best observation?</h2>

<p>As a participant, you have the choice. You can either work with the three base observations that we prepared or better, design an improved observation yourself. If you do the latter, then share your observation and you will have chances of winning the Community Contribution Prize (see Prizes). These are the three base observation that we prepared:</p>

<ul>
  <li>
    <p>Global Observation: The whole scene is observed</p>
  </li>
  <li>
    <p>Local Grid Observation: A local grid around the agent is observed</p>
  </li>
  <li>
    <p>Tree Observation: The agent can observe its navigable path to some predefined depth.</p>
  </li>
</ul>

<p>Sounds complicated? Do not despair, the next sections will provide you with more useful information about these rounds!</p>

<h1 id=""timeline"">Timeline</h1>

<p>There will be 3 rounds in the challenge. The first one (round 0) is a beta round and serves as an introduction to get familiar with Flatland (as well as bug fixing). Rounds 1 and 2 pose the actual problems to be solved. Submissions are only accepted for Round 1 and Round 2, both rounds will contribute to the final ranking. <strong>Round 2 is currently ongoing and will close on Sunday, 5th of January 2020, 12 PM, UTC +1.</strong></p>

<h2 id=""round-0-learn-to-navigate-beta-round"">Round 0: Learn to navigate (Beta Round)</h2>

<p>A single agent has to navigate from a freely chosen starting point to a freely chosen target destination on a random infrastructure. It is, in other words, a relatively simple shortest path problem.</p>

<p>There will be no uploading possibility, no ranking, nor any prizes to be gained in this round - but the collected insights make it all worth it!</p>

<p>Check out this simple <a href=""https://gitlab.aicrowd.com/flatland/baselines/blob/master/torch_training/Getting_Started_Training.md"">introduction to training</a> to get started with your own training on Flatland.</p>

<p><strong>The beta round starts on the 1st of July 2019 and ends on the 30th of July 2019</strong></p>

<p><img src=""https://i.imgur.com/t5ULr4L.gif"" alt=""Round0"" /></p>

<h2 id=""round-1-avoid-conflicts"">Round 1: Avoid conflicts</h2>

<p>We pick-up the same problem from the previous round and turn it into a multi-agent problem. This means, multiple agents have to find their ways to their respective target destinations. In this scenario you are likely to encounter resource conflicts when two or more agents simultaneously plan to occupy the same section of infrastructure. Thus, the agents have to learn to avoid conflicts and find feasible solutions. By timely submitting your solution and adhering to the <a href=""#rules"">participation rules</a> you are automatically eligible for the <a href=""#prizes"">Contribution Prize &amp; Best Agent Prize</a>. Good luck!</p>

<p><strong>Round 1 will open on Tuesday, 30th of July and close on Sunday, 13th of October 2019, 12 PM, UTC +1.</strong>
<strong>Round 1 submissions closed early in order to start with Round 2 as early as possible.</strong>
<strong>If you still want to test your code on earlier version please get in touch with us directly.</strong></p>

<p><img src=""https://i.imgur.com/AvBHKaD.gif"" alt=""Round1"" /></p>

<p><strong>Round 2: Optimize train traffic</strong>: In reality, not all trains can go at the same speed. In round 2 we introduce additional complexity to the multi-agent-problem of round 1 by letting the trains have different speeds! Furthermore, stochastic events will occur during the episodes which mean that your controller will need to adapt to a changing environment. Key features of the updated environment are:</p>

<ul>
  <li>Agents travel at 4 different speeds.</li>
  <li>Some agents will experience malfunctions which render them immobile at times.</li>
  <li>Agents have to actively start their journey in the environment and leave the environment when they reach their target.</li>
</ul>

<p>This means that a good solution not only avoids/resolves conflicts, but also optimizes by taking into account that slower agents can slow down the faster ones. The prize is reserved for the winner who submits the solution with the minimal cumulated travel time for all agent. By submitting your solution timely and adhering to the <a href=""#rules"">participation rules</a>, you are automatically eligible for the <a href=""#prizes"">Contribution Prize &amp; Best Agent Prize</a>. Good luck!</p>

<p><img src=""https://i.imgur.com/Pc9aH4P.gif"" alt=""Round2"" /></p>

<p><strong>Round 2 is now open and will close on Sunday, 5th of January 2020, 12 PM, UTC +1.</strong></p>

<h1 id=""environment"">Environment</h1>

<p>There are a few important basic elements and notions specific to this challenge that you should be aware of before diving into the “Lets get started” section.</p>

<h2 id=""agent"">Agent</h2>

<p>Flatland is a discrete time simulation, that means that all actions performed happen with a constant time step. At each step, the agents can choose an action. The term agent is defined as an entity that can move within the grid and must solve tasks - these agents are, who would have thought, trains. A train does basically two things: wait or go into a particular direction. Depending on the train type (e.g. freight train or passenger train), they have different speeds. An agent can move in any arbitrary direction (if the environment permits it) and transition from one cell to the next. If the agent chooses a valid action, the corresponding transition will be executed and the agent’s position and orientation is updated. Each agent has its individual start and target.</p>

<p>Agent at start:</p>

<p><img src=""https://i.imgur.com/mXW7O3L.png"" alt=""starting_agent"" /></p>

<p>Target Destination:</p>

<p><img src=""https://i.imgur.com/NiSEryT.png"" alt=""destination"" /></p>

<p>The cell where the agent is located at must have enough capacity to hold the agent on (thus a “blank” or already reserved cell is impossible). Every agent reserves exact one capacity or resource and since the capacity of a cell is maximal one, it can never hold more than one agent. The different cell-types are introduced in the next section.</p>

<h2 id=""grid-world"">Grid World</h2>

<p>As you know by now, the Flatland environment consists of cells that are arranged in a simulation grid. These cells have a tile type - for a railway specific problem, 8 basic tile types can be defined. These tile types determine where the agent can be located and how the agent can move through the cell. Here is a quick overview of the tile types:</p>

<p><img src=""https://i.imgur.com/geH2KOV.png"" alt=""Basic Tiles"" /></p>

<p>Of course, there are more possibilities, as these tiles can be rotated in steps of 90° and mirrored along the North-South and East-West axis - but the principal idea remains the same. To get an intuition, let us now discuss four interesting cases in more detail.</p>

<blockquote>
  <p><strong>Railway Network Fact:</strong> Every time an agent approaches a switch, a navigation choice has to be made. In Flatland (like in reality) a maximum of two options is available. There does not exist a switch with three or more options.</p>
</blockquote>

<table>
  <thead>
    <tr>
      <th>Element</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><img src=""https://i.imgur.com/u9uzWyV.png"" alt=""straight"" /></td>
      <td><strong>Straight</strong>: This tile represents a single passage. While on the tile, the agents can’t take any navigation decisions, but only decide to either continue, i.e. passing to the next connected tile or wait.</td>
    </tr>
    <tr>
      <td><img src=""https://i.imgur.com/mOqfRhD.png"" alt=""simple switch"" /></td>
      <td><strong>Simple Switch</strong>: The switch in this tile forces agents who arrive from one direction (in this case from the West) to make a navigation choice (to turn North or go straight). Navigation in either direction is equally costly. Agents coming from any other direction do not have a navigation choice.</td>
    </tr>
    <tr>
      <td><img src=""https://i.imgur.com/NOEnlRH.png"" alt=""double slip switch"" /></td>
      <td><strong>Double Slip Switch</strong>: : In this case, we have a crossing with two switches accessible from all four directions. Thus, in every case, the agent has a navigation choice. The agents can only change directions through the switches, the crossing alone doesn’t permit direction changes.</td>
    </tr>
    <tr>
      <td><img src=""https://i.imgur.com/UVp7sJT.png"" alt=""deadend"" /></td>
      <td><strong>Dead end</strong>: : If an agent occupies this cell, only stop or backward motion is possible.</td>
    </tr>
  </tbody>
</table>

<p><strong>Important to note</strong>:</p>

<ul>
  <li>
    <p>Due to the dynamics of train traffic, each transition probability is symmetric in this environment. This means that neighboring cells will always have the same transition probability, regardless of direction of movement.</p>
  </li>
  <li>
    <p>Each cell is exclusive and can only be occupied by one agent at any given time.</p>
  </li>
</ul>

<h1 id=""getting-started"">Getting Started</h1>

<p>You should now be equiped with the needed background knowledge to get started with the challenge. Check out the <a href=""http://flatland-rl-docs.s3-website.eu-central-1.amazonaws.com/readme.html"">Starter Kit</a> for a description on the technical set up and tips on how to get started.</p>

<p><a href=""https://gitlab.aicrowd.com/flatland/flatland/tree/master"">Here</a> is the public respository containing all the code you need to participate in this challenge.</p>

<p>If any questions arise, head over to the <a href=""https://flatlandrl-docs.aicrowd.com/09_faq_toc.html"">FAQ</a> section to get answers quickly.</p>

<h1 id=""prizes"">Prizes</h1>

<p>Your problem solutions mean something to us - hence prizes with a total value of 30k CHF (approx. 30k USD) are reserved for those with the best submissions. You can excel in two categories: The best solution category and the community prize category. Within both those categories your submission is individually ranked taking into account your performance in Round 1 and Round 2. Make sure to check the participation rules before you start. Only submissions conforming to our rules have a chance of winning the prizes.</p>

<p><strong>Best Solution Prize</strong>: Won by the participants with the best performing submission on our test set. Only your ranking from the Round 2 is taken into account. Check the leader board on this site regularly for the latest information on your ranking.</p>

<p>The top three submissions in this category will be awarded the following cash prizes (in Swiss Francs):</p>

<p><strong>CHF 7’500.- for first prize</strong></p>

<p><strong>CHF 5’000.- for second prize</strong></p>

<p><strong>CHF 2’500.- for third prize</strong></p>

<p><strong>Community Contributions Prize</strong>: Awarded to the person/group who makes the biggest contribution to the community - done through generating new observations and sharing them with the community.</p>

<p>The top submission in this category will be awarded the following cash prize (in Swiss Francs): <strong>CHF 5’000.-</strong></p>

<p>In addition, we will hand-pick and award up to five (5) travel grants to the Applied Machine Learning Days 2019 in Lausanne, Switzerland. Participants with promising solutions may be invited to present their solutions at SBB in Bern, Switzerland.</p>

<blockquote>
  <p><strong>Note:</strong> It is possible for a participant to win in both categories.</p>
</blockquote>

<h1 id=""rules"">Rules</h1>

<p>The following rules apply to all participants:</p>

<ul>
  <li>Participants are allowed at most 5 submissions per day.</li>
  <li>When evaluating individual solutions directly via the REST-API, a limit of one REST-call per minute shall be observed.</li>
  <li>The results achieved by the solver must be reproducible. If there are randomized portions of your approach, be sure to include seeds to make the runs repeatable.</li>
  <li>In case of conflicts, the decision of the Organizers will be final and binding.</li>
  <li>Organizers reserve the right to make changes to the rules and timeline.</li>
  <li>Violation of the rules or other unfair activity may result in disqualification.</li>
</ul>

<p>More legal details, such as eligibility criteria are <a href=""https://www.aicrowd.com/challenges/flatland-challenge/challenge_rules/68"">here</a></p>

<h1 id=""contact"">Contact</h1>

<p>For Challenge-related questions (technical and/or content questions):</p>

<ul>
  <li>Gitter Channel : <a href=""https://gitter.im/AIcrowd-HQ/flatland-rl"">https://gitter.im/AIcrowd-HQ/flatland-rl</a></li>
  <li>Technical Issues : Please use the <a href=""https://gitlab.aicrowd.com/flatland/flatland/issues"">issue tracker</a> in the public repository</li>
  <li>Discussion Forum : <a href=""https://discourse.aicrowd.com/"">https://discourse.aicrowd.com/</a></li>
</ul>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers.
But in case look for a direct communication channel, feel free to reach out to us at:</p>

<ul>
  <li><strong>mohanty</strong> [at] <strong>aicrowd.com</strong></li>
  <li><strong>erik.nygren</strong> [at] <strong>sbb.ch</strong></li>
</ul>

<p>For press inquiries, please contact SBB Media Relations at <a href=""mailto:press@sbb.ch"">press@sbb.ch</a></p>
"
19,"<p>Sandbox Challenge for Uber ML Hackathon. Please check with Mohanty <a href=""mailto:mohanty@aicrowd.com"">mohanty@aicrowd.com</a> if you can see this, and are confused about this.</p>
"
71,"<h1 class=""mt-0"" id=""introduction"">Introduction</h1>
<p>Privacy is important to each one of us. Whether we share information about us with the Government or with commercial entities like social media websites, it is in our interest to:</p>

<ul>
  <li>Share minimal data</li>
  <li>Expect that our data is stored securely</li>
  <li>Ensure our data is used only for purposes that we have agreed to, and</li>
  <li>Ensure our data is not shared with 3rd parties without our consent.</li>
</ul>

<p>With the advent of Data Protection Regulations like <a href=""https://en.wikipedia.org/wiki/General_Data_Protection_Regulation"">GDPR</a> in the European Union, <a href=""https://en.wikipedia.org/wiki/California_Consumer_Privacy_Act"">California Privacy Protection Act</a> in the US,  and the <a href=""https://meity.gov.in/writereaddata/files/Personal_Data_Protection_Bill,2018.pdf"">Personal Data Protection Bill 2018</a> in India, there is increasing regulatory backing for our privacy, and the protection of our personal data.</p>

<h2 id=""industry"">Industry</h2>

<p>On the other end of the spectrum, protecting the personal data of customers is a huge challenge for companies. Even identifying all the personal data available with a company is non-trivial. Identifying personal data entities in customer data, protecting and anonymizing personal data, and serving customer requests related to the usage of their data, are all part of a company’s Data analytics and regulatory compliance systems.</p>

<h2 id=""governance"">Governance</h2>

<p>At the government level, this problem of protecting sensitive personal information assumes gargantuan proportions. Govt collects information about citizens for a number of reasons, like welfare, identification, and security. All these information could be linked to a unique identification numbers like the SSN (US), Aadhaar number (India), which further increases the data protection requirements.</p>

<h1 id=""what-is-the-davatar-challenge"">What is the D’Avatar Challenge?</h1>

<p>We’ll be providing a corpus of English texts which are from customer complaints to financial companies. The personal data entities in these texts have already been removed and contain placeholders like xxxx. As part of this challenge, you have to impute (create new) values for the personal data entities that have been redacted from texts.</p>

<h2 id=""why-does-this-matter"">Why does this matter</h2>

<p>While the intellectual curiosity to solve a problem is likely to be the main motivation for you to participate in this challenge, we hope this exercise will also be of use to the academic community, government and industry in India. The datasets that we produce during this challenge can be made available to researchers, to come up with better models to improve privacy and regulatory compliance. Based on the number of submissions, we might be able to produce a combined large dataset, with more diversity of personal data entities than each of the teams attempting separately.</p>

<h1 id=""problem-statement"">Problem Statement</h1>

<p>Given a dataset with unstructured text containing one or more redacted spans, where the spans  are  known  to  have  contained  Personal  Data  Entities  (PDE),  participants  have  to impute unrelated PDEs of the same types, in place of the redacted spans.</p>

<p>Consider the below example:</p>

<p>“My credit card number is xxxx and I wish to raise a compliant .”</p>

<p>In the above text, the entity masked with xxxx is the redacted span.  We might be able to guess that a 16 digit credit card number was originally present in this text.</p>

<p>The  simplest  output  we  are  looking  for  is  a  re-written  text  with  the  redacted  span replaced with a personal data entity of the expected type.  In this example, the redacted portion should be replaced with some variant of a 16 digit number.</p>

<p>“My credit card number is 1234-5678-9012-3456 and I wish to raise a compliant .”</p>

<p>However, a better output will be credit card number which is not completely random, but obeys the <a href=""https://en.wikipedia.org/wiki/Luhn_algorithm"">Luhn algorithm</a>.</p>

<h2 id=""personal-data-entity-types"">Personal Data Entity Types</h2>

<p>The personal data entities imputed by you must have one or more of the below types. You can provide other finer types, if you wish, but we’ll ignore them for the purpose of this evaluation.</p>

<ul>
  <li>/bio/cause_of_death</li>
  <li>/bio/criminal_charge</li>
  <li>/bio/nationality</li>
  <li>/contact/first_name</li>
  <li>/contact/last_name</li>
  <li>/contact/name</li>
  <li>/finance/currency</li>
  <li>/id/url</li>
  <li>/interest/ideology</li>
  <li>/interest/religion</li>
  <li>/location</li>
  <li>/location/city</li>
  <li>/location/continent</li>
  <li>/location/country</li>
  <li>/location/county</li>
  <li>/location/province</li>
  <li>/location/zipcode</li>
  <li>/other</li>
  <li>/other/datetime</li>
  <li>/other/datetime/date</li>
  <li>/other/number</li>
  <li>/other/organization</li>
  <li>/other/percent</li>
  <li>/profession/job_title</li>
</ul>

<h1 id=""unbiased-datasets"">Unbiased datasets</h1>

<p>As bonus credit, can you impute entities of the above entity types, without bias in any protected variable? For example, can you ensure the /location/country has reasonably diverse country names?</p>

<p>Refer to <a href=""http://aif360.mybluemix.net/"">AIF 360 Toolkit</a> for detecting bias in datasets.</p>

<h1 id=""probable-solutions"">Probable solutions</h1>

<p>To get you started, we are providing some pointers for your solutions. We,however, encourage you to come up with your own innovative solutions to the problem.</p>

<ol>
  <li>
    <p>Manual annotations</p>

    <p><a href=""http://brat.nlplab.org/"">BRAT</a> tool can be used to crowd source the problem and let human annotators  guess the masked entities, and optionally impute values too.  But a more feasible solution is to let human annotators provide the entity types for the masked entities, and then use some dictionary to impute values of that type.</p>
  </li>
  <li>
    <p>Rule based annotations</p>

    <p>A rule based system, which uses dictionaries (of names, places, credit card numbers etc) can be used to find patterns in sentences, and replace the masked portions. <a href=""https://researcher.watson.ibm.com/researcher/view_group.php?id=1264"">IBM’s System T</a> (or any other solution, or perhaps just regular expressions) can be used to find such patterns in sentences.</p>
  </li>
  <li>
    <p>Data Programming</p>

    <p><a href=""https://www.snorkel.org/"">Snorkel</a> is a system used for generating large amounts of noisy training data by writing labeling functions.  After generating a gold set using manual methods, this system could be used to annotate more.</p>
  </li>
  <li>
    <p>Natural Language Generation</p>

    <p>A machine learning model can also be used to generate words/numbers to replace the redacted portions in a sentence. This problem can perhaps be solved using <a href=""https://pages.github.ibm.com/natural-language-generation/web/"">Natural Language Generation</a> models which typically tend to be sequence-to-sequence models.</p>
  </li>
  <li>
    <p>Masked Language Models</p>

    <p>Language Models like BERT, Elmo, XLNet could potentially be used predict the masked entities. See this <a href=""https://www.quora.com/What-is-a-masked-language-model-and-how-is-it-related-to-BERT"">page</a>.</p>
  </li>
  <li>
    <p>Generative Adversarial Networks</p>

    <p><a href=""https://arxiv.org/pdf/1807.01514.pdf"">Avino et al 2018</a> tried using GANs to generate synthetic healthcare datasets.</p>
  </li>
</ol>
"
222,"<p><em>Note: Do not forget to read the <strong>Rules</strong> section on this page. Pressing the red <strong>Participate button</strong> leads you to a page where you have to agree with those rules. You will not be able to submit any results before agreeing with the rules.</em></p>

<p><em>Note: Before trying to submit results, read the <strong>Submission instructions</strong> section on this page.</em></p>

<h1 id=""challenge-description"">Challenge description</h1>

<p>Welcome to the 4th edition of the Tuberculosis Task!</p>

<hr />

<p><em>Tuberculosis (TB) is a bacterial infection caused by a germ called Mycobacterium tuberculosis. About 130 years after its discovery, the disease remains a persistent threat and a leading cause of death worldwide according to WHO. This bacteria usually attacks the lungs, but it can also damage other parts of the body. Generally, TB can be cured with antibiotics. However, the different types of TB require different treatments, and therefore the detection of the TB type and the evaluation of lesion characteristics are important real-world tasks.</em></p>

<hr />

<p>In this year edition, we decided to concentrate on the automated CT report generation task, since it has important outcome that can have a major impact in the real-world clinical routines. In order to make the task both more attractive for participants and practically valuable, this year report generation is lung-based rather than CT-based, which means the labels for left and right lungs will be provided independently. The set of target labels in the CT Report was updated with accordance to the opinion of medical experts. This year we provide 3 labels for each lung: <i>presence of TB lesions</i> in general, presence of <i>pleurisy</i> and <i>caverns</i> in particular. Also the dataset size was increased compared to the previous year.</p>

<h1 id=""data"">Data</h1>

<hr />
<p><em>As soon as the data is released it will be available under the “Resources” tab.</em></p>

<hr />

<p>In this edition, a dataset containing chest CT scans of 403 (283 for train and 120 for test) TB patients is used. Since the labels are provided on lung-wise scale rather than CT-wise scale, the total number of cases is virtually increased twice.</p>

<p>Provided data includes sets of train and test CT images, lungs masks, CT report for train data.</p>

<p><strong><em>CT Images</em></strong></p>

<p>We provide 3D CT image which are stored in NIFTI file format with .nii.gz file extension (g-zipped .nii files). This file format stores raw voxel intensities in Hounsfield units (HU) as well the corresponding image metadata such as image dimensions, voxel size in physical units, slice thickness, etc. A freely-available tool called <a href=""https://www.creatis.insa-lyon.fr/rio/vv"">“VV”</a> can be used for viewing image files. Currently, there are various tools available for reading and writing NIFTI files. Among them there are <a href=""https://de.mathworks.com/matlabcentral/fileexchange/8797-tools-for-nifti-and-analyze-image"">load_nii and save_nii</a> functions for Matlab; <a href=""http://niftilib.sourceforge.net/"">Niftilib</a> library for C, Java, Matlab and Python and <a href=""https://nipy.org/nibabel/"">NiBabel</a> package for Python.</p>

<p><strong><em>Masks</em></strong></p>

<p>We provide two versions of automatically extracted masks of the lungs which are stored in the same file format as CTs.</p>

<p>The first version of segmentation was retrieved using the same technique as previous year. The details of this segmentation can be found <a href=""http://publications.hevs.ch/index.php/publications/show/1871"">here</a>.</p>

<p>The second version of segmentation was retrieved using non-rigid image registration scheme. The details of this segmentation and open-source implementation can be found <a href=""https://github.com/skliff13/CT_RegSegm"">here</a>.</p>

<p>The first version of segmentation provides more accurate masks, but it tends to miss large abnormal regions of lungs in the most severe TB cases. The second segmentation on the contrary provides more rough bounds, but behaves more stable in terms of including lesion areas.</p>

<p><em>Please note, that only first version of segmentation allows extracting mask for left and right individually (voxel values differs per lung), while second version of segmentation needs custom post-processing.</em></p>

<p>In case the participants use the provided masks in their experiments, please refer to the section “Citations” at the end of this page to find the appropriate citation for this lung segmentation technique.</p>

<p><strong><em>CT report for train CT images</em></strong></p>

<p>We provide labels for training set as a simple .csv file, containing following columns (headed included):</p>

<p><em>Filename</em> - train file name</p>

<p><em>LeftLungAffected</em> - binary label for presence of any TB lesions in the left lung</p>

<p><em>RightLungAffected</em> - binary label for presence of any TB lesions in the right lung</p>

<p><em>CavernsLeft</em> - binary label for presence of caverns in the left lung</p>

<p><em>CavernsRight</em> - binary label for presence of caverns in the right lung</p>

<p><em>PleurisyLeft</em> - binary label for presence of pleurisy in the left lung</p>

<p><em>PleurisyRight</em> - binary label for presence of pleurisy in the right lung</p>

<p>Plese note, that “presence of any TB lesions” means <strong>any</strong> TB lesions, not limited to caverns or pleurisy. So rows like “1,1,0,0,0,0” are correct.</p>

<p><strong><em>Test data</em></strong></p>

<p>Test data release planned for mid-March (tentative).</p>

<h1 id=""submission-instructions"">Submission instructions</h1>

<hr />
<p><em>As soon as the submission is open, you will find a “Create Submission” button on this page (next to the tabs).</em></p>

<hr />
<p><em>Before being allowed to submit your results, you have to first press the red participate button, which leads you to a page where you have to accept the challenges rules.</em></p>

<hr />

<p>Submit a plain text file named with the prefix <strong>CTR</strong> (e.g. CTRfree-text.txt) with the following format:</p>

<p><em>&lt;Filename&gt;,&lt;Probability of “left lung affected”&gt;,&lt;Probability of “right lung affected”&gt;,&lt;Probability of “presence of caverns in the left lung”&gt;,&lt;Probability of “presence of caverns in the right lung”&gt;,&lt;Probability of “pleurisy in the left lung”&gt;,&lt;Probability of “pleurisy in the right lung”&gt;</em></p>

<p>e.g.:</p>

<p><em>CTR_TST_001.nii.gz,0.89,0.1,0.84,0.05,0.9,0.2</em>
<em>CTR_TST_002.nii.gz,0.1,0.6,0.222,0.333,0.444,0.55</em>
<em>CTR_TST_003.nii.gz,0.1,0.7,0.0,0.2,0.1,0.46</em>
<em>CTR_TST_004.nii.gz,0.88,0.78,0.59,0.65,0.8,0.4</em></p>

<p>You need to respect the following constraints:</p>

<p><em>Filenames must be same as original test file names</em></p>

<p><em>All filenames must be present in the runfiles</em></p>

<p><em>Only use numbers between 0 and 1 for the probabilities. Use the dot (.) as a decimal point (no commas accepted)</em></p>

<h1 id=""evaluation-criteria"">Evaluation criteria</h1>

<p>This task is considered as a multi-binary classification problem.</p>

<p>The ranking of this task will be done first by average AUC and then by min AUC over the 3 target labels.</p>

<p>The AUC values will be evaluated in a lung-wise manner.</p>

<h1 id=""rules"">Rules</h1>

<p><em>Note: In order to participate in this challenge you have to sign an End User Agreement (EUA). You will find more information on the ‘Resources’ tab.</em></p>

<p>ImageCLEF lab is part of the Conference and Labs of the Evaluation Forum: CLEF 2020. CLEF 2020 consists of independent peer-reviewed workshops on a broad range of challenges in the fields of multilingual and multimodal information access evaluation, and a set of benchmarking activities carried in various labs designed to test different aspects of mono and cross-language Information retrieval systems. More details about the conference can be found <a href=""http://clef2020.clef-initiative.eu/"" target=""_blank"">here</a> .</p>

<p>Submitting a working note with the full description of the methods used in each run is mandatory. Any run that could not be reproduced thanks to its description in the working notes might be removed from the official publication of the results. Working notes are published within CEUR-WS proceedings, resulting in an assignment of an individual DOI (URN) and an indexing by many bibliography systems including DBLP. According to the CEUR-WS policies, a light review of the working notes will be conducted by ImageCLEF organizing committee to ensure quality. As an illustration, ImageCLEF 2019 working notes (task overviews and participant working notes) can be found within <a href=""http://ceur-ws.org/Vol-2380/"" target=""_blank"">CLEF 2019 CEUR-WS</a> proceedings.</p>

<h2 id=""important"">Important</h2>

<p>Participants of this challenge will automatically be registered at CLEF 2020. In order to be compliant with the CLEF registration requirements, please edit your profile by providing the following additional information:</p>

<ul>
  <li>
    <p>First name</p>
  </li>
  <li>
    <p>Last name</p>
  </li>
  <li>
    <p>Affiliation</p>
  </li>
  <li>
    <p>Address</p>
  </li>
  <li>
    <p>City</p>
  </li>
  <li>
    <p>Country</p>
  </li>
  <li>
    <p><em>Regarding the username, please choose a name that represents your team.</em></p>
  </li>
</ul>

<p><em>This information will not be publicly visible and will be exclusively used to contact you and to send the registration data to CLEF, which is the main organizer of all CLEF labs</em></p>

<h2 id=""participating-as-an-individual-non-affiliated-researcher"">Participating as an individual (non affiliated) researcher</h2>

<p>We welcome individual researchers, i.e. not affiliated to any institution, to participate. We kindly ask you to provide us with a motivation letter containing the following information:</p>

<ul>
  <li>
    <p>the presentation of your most relevant research activities related to the task/tasks</p>
  </li>
  <li>
    <p>your motivation for participating in the task/tasks and how you want to exploit the results</p>
  </li>
  <li>
    <p>a list of the most relevant 5 publications (if applicable)</p>
  </li>
  <li>
    <p>the link to your personal webpage</p>
  </li>
</ul>

<p>The motivation letter should be directly concatenated to the End User Agreement document or sent as a PDF file to bionescu at imag dot pub dot ro. The request will be analyzed by the ImageCLEF organizing committee. We reserve the right to refuse any applicants whose experience in the field is too narrow, and would therefore most likely prevent them from being able to finish the task/tasks.</p>

<h1 id=""citations"">Citations</h1>

<p>Information will be posted after the challenge ends.</p>

<h1 id=""prizes"">Prizes</h1>

<h2 id=""publication"">Publication</h2>

<p>ImageCLEF 2020 is an evaluation campaign that is being organized as part of the <a href=""http://clef2020.clef-initiative.eu/"" target=""_blank"">CLEF initiative</a> labs. The campaign offers several research tasks that welcome participation from teams around the world. The results of the campaign appear in the working notes proceedings, published by CEUR Workshop Proceedings (CEUR-WS.org). Selected contributions among the participants, will be invited for publication in the following year in the Springer Lecture Notes in Computer Science (LNCS) together with the annual lab overviews.</p>

<h1 id=""resources"">Resources</h1>

<h2 id=""contact-us"">Contact us</h2>

<p><em>Discussion Forum</em></p>

<ul>
  <li>You can ask questions related to this challenge on the Discussion Forum. Before asking a new question please make sure that question has not been asked before.</li>
  <li>Click on Discussion tab above or direct link: <a href=""https://discourse.aicrowd.com/c/imageclef-2020-tuberculosis-ct-report"" target=""_blank"">https://discourse.aicrowd.com/c/imageclef-2020-tuberculosis-ct-report</a></li>
</ul>

<p><em>Alternative channels</em></p>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

<ul>
  <li>kozlovski[dot]serge[at]gmail[dot]com</li>
  <li>vitali[dot]liauchuk[at]gmail[dot]com</li>
  <li>yashindc[at]gmail[dot]com</li>
  <li>vassili[dot]kovalev[at]gmail[dot]com</li>
  <li>henning[dot]mueller[at]hevs[dot]ch</li>
</ul>

<h2 id=""more-information"">More information</h2>

<p>You can find additional information on the challenge here:
<a href=""https://www.imageclef.org/2020/medical/tuberculosis"" target=""_blank"">https://www.imageclef.org/2020/medical/tuberculosis</a></p>
"
13,"<p>Trajectory forecasting in crowded scenes has become an important topic in recent times because of the increasing demands of emerging applications of artificial intelligence like autonomous cars and service bots. One important challenge in trajectory forecasting is to effectively model the interaction among agents. In the past few years, several novel methods have been proposed to tackle agent-agent interactions. However, these methods have been evaluated on different subsets of the available data without proper indexing of trajectories making it difficult to objectively compare the forecasting techniques.</p>

<p>We introduce TrajNet++, a new, large scale trajectory-based benchmark. Researchers have to study how their method performs in <strong>explicit agent-agent</strong> scenarios. Our challenge provides not only proper indexing of trajectories but also a unified extensive evaluation system to test the gathered methods for a fair comparison.</p>

<h2 id=""what-do-we-provide"">What do we provide?</h2>
<p>We present a framework for the fair evaluation of trajectory forecasting algorithms, explicitly in agent-agent scenarios. We provide:</p>

<ul>
  <li>A large collection of agent-agent centric datasets</li>
  <li>A defined categorisation of trajectories</li>
  <li>A common evaluation tool providing several performance measures</li>
  <li>An easy way to compare the performance of state-of-the-art methods.</li>
</ul>

<h2 id=""data-description"">Data Description</h2>

<p>The dataset files contain two different data representations:</p>

<p>1.Scene</p>

<p>{“scene”: {“id”: 266, “p”: 254, “s”: 10238, “e”: 10358, “fps”: 2.5, “tag”: 2}}</p>

<ul>
  <li><strong>id:</strong> scene id</li>
  <li><strong>p:</strong> pedestrian ID</li>
  <li><strong>s, e:</strong>  starting and ending frames id of pedestrian “p”</li>
  <li><strong>fps:</strong>  frame rate.</li>
  <li><strong>tag:</strong>  trajectory type. Discussed in detail below.</li>
</ul>

<p>Note: Corresponding to each scene, there exists a primary pedestrian denoted by the pedestrian ID of the scene. The scene is categorised (tag) with respect to this primary pedestrian.</p>

<p>2.Track</p>

<p>{“track”: {“f”: 10238, “p”: 248, “x”: 13.2, “y”: 5.85, “pred_number”: 0, “scene_id”: 123}}</p>

<ul>
  <li><strong>f:</strong> frame id</li>
  <li><strong>p:</strong> pedestrian ID</li>
  <li><strong>x, y:</strong> x and y coordinates in meters of pedestrian “p” in frame “f”.</li>
  <li><strong>pred_number:</strong> prediction number. This is useful when you are providing multiple predictions as opposed to a single prediction. Max 3 predictions allowed</li>
  <li><strong>scene_id:</strong> This is useful when you are providing predictions of other agents in the scene as opposed to only primary pedestrian prediction.</li>
</ul>

<p>For a more detailed description, we provide the following helper code: <a href=""https://github.com/vita-epfl/trajnetplusplustools"" target=""_blank"">Tools for Trajnet++</a></p>

<h2 id=""trajectory-categorization"">Trajectory Categorization</h2>

<p>We explicitly categorise the primary pedestrian trajectory of the scene into different types. The definition of each type is provided below:</p>

<ul>
  <li>
    <p><strong>Static (Type I)</strong>: If the euclidean displacement of the primary pedestrian in the scene is less than 1 meter</p>
  </li>
  <li>
    <p><strong>Linear (Type II)</strong>: If the trajectory of the primary pedestrian can be <em>correctly predicted</em> with the help of an Extended Kalman Filter (EKF). A trajectory is said to be correctly predicted by EKF if the FDE between the ground truth trajectory and predicted trajectory is less than 0.5 meter.</p>
  </li>
  <li>
    <p><strong>Non-Linear</strong>: The rest of the scenes are classified as ‘Non-Linear’. We further divide non-linear scenes into <strong>Interacting (Type III)</strong> and <strong>Non-Interacting (Type IV)</strong>.</p>
  </li>
</ul>

<p>We further sub-categorize the <strong>Interacting (Type III)</strong> trajectories as follows:</p>

<ul>
  <li>
    <p><strong>Leader Follower</strong>: Leader follower phenomenon refers to the tendency to follow pedestrians going in relatively the same direction. The follower tends to regulate his/her speed and direction according to the leader. If the primary pedestrian is a follower, we categorize the scene as Leader Follower.</p>
  </li>
  <li>
    <p><strong>Collision Avoidance</strong>: Collision avoidance phenomenon refers to the tendency to avoid pedestrians coming from the opposite direction. We categorize the scene as Collision avoidance if primary pedestrian to be involved in collision avoidance.</p>
  </li>
  <li>
    <p><strong>Group</strong>: The primary pedestrian is said to be a part of a group if he/she maintains a close and roughly constant distance with atleast one neighbour on his/her side during prediction.</p>
  </li>
  <li>
    <p><strong>Others</strong>: Trajectories where the primary pedestrian undergoes social interactions other than Leader Follower, Collision Avoidance and Group. We define <em>social interaction}</em> as follows: We look at an angular region in front of the primary pedestrian. If any neighbouring pedestrian is present in the defined region at any time-instant during prediction, the scene is classified as having a presence of social interactions.</p>
  </li>
</ul>

<p>If a trajectory of primary pedestrian is non-linear and undergoes no social interactions during prediction,  the trajectory is classified as <strong>Non-Interacting (Type 4)</strong>.</p>

<p><img src=""https://drive.google.com/uc?export=view&amp;id=1nopmFmBwBoAl_ZOxH8PcqWlc5Wo7kAhs"" alt=""Figure 1"" /></p>

<p>During evaluation, we provide the evaluation of the submitted model with respect to each of above categories to provide insight into the model performance in different scenarios.</p>

<p>We rely on the spirit of crowdsourcing, and encourage researchers to submit their sequences to our benchmark, so the quality of trajectory forecasting models can keep increasing in tackling more challenging scenarios.</p>

<h2 id=""metrics"">Metrics</h2>

<p>A good benchmark requires not only a standard dataset but also important evaluation metrics to provides insights regarding the model performance through different perspectives. We describe the evaluation metrics for this challenge:</p>

<h3 id=""unimodal-metrics-single-prediction"">Unimodal Metrics: Single Prediction</h3>

<ul>
  <li>
    <p><strong>Average Displacement Error (ADE)</strong>: Average L2 distance between the ground truth and prediction of the primary pedestrian over all predicted time steps. Lower is better.</p>
  </li>
  <li>
    <p><strong>Final Displacement Error (FDE):</strong> The L2 distance between the final ground truth coordinates and the final prediction coordinates of the primary pedestrian. Lower is better</p>
  </li>
  <li>
    <p><strong>Ground Truth Collision (Col I)</strong>: Calculates the percentage of collisions of primary pedestrian with neighbouring pedestrians in the scene. The ground truth of neighbouring pedestrians is used to check the occurrence of collisions. Lower is better.</p>
  </li>
  <li>
    <p><strong>Prediction truth Collision (Col II)</strong>: Calculates the percentage of collisions of primary pedestrian with neighbouring pedestrians in the scene. The <em>model prediction</em> of neighbouring pedestrians is used to check the occurrence of collisions. Lower is better.</p>
  </li>
</ul>

<h3 id=""multimodal-metrics-multiple-prediction"">Multimodal Metrics: Multiple Prediction</h3>

<ul>
  <li>
    <p><strong>Topk Average Displacement Error (Topk_ADE)</strong>: Given k output predictions for an observed scene, the metric calculates the ADE of the prediction which is closest to the groundtruth trajectory in terms of ADE. Lower is better. In this challenge, k=3</p>
  </li>
  <li>
    <p><strong>Topk Final Displacement Error (Topk_FDE):</strong> Given k output predictions for an observed scene, the metric calculate the FDE of the prediction which is closest to the groundtruth trajectory in terms of ADE. Lower is better. In this challenge, k=3</p>
  </li>
  <li>
    <p><strong>Average NLL (NLL):</strong> Given n output predictions for an observed scene, the metric calculates the average negative log-likelihood of groundtruth trajectory over the prediction horizon. Higher is better. In this challenge, n=100.</p>
  </li>
</ul>

<h1 id=""data"">Data</h1>

<p>The training and test datasets can be found <a href=""https://github.com/vita-epfl/trajnetplusplusdata"">here</a></p>

<h1 id=""submission"">Submission</h1>

<p>We strongly encourage all participants to use only the sequences from the training set for finding parameters and report results on the provided test scenarios to enable a meaningful comparison of forecasting methods.</p>

<h2 id=""file-format"">File Format</h2>

<p>To have your predictions evaluated, you need to submit a single .zip file containing <strong>the exact same directory structure and file names</strong>  as the test file. Specifically, you will be given a single .zip with folders ‘real_data’ and ‘synth_data’ within the parent folder ‘test’. Each of these folders will contain one or more .ndjson files.  In every file, corresponding to each “scene” (length = 21 frames), you are supposed to predict the coordinates of the primary pedestrian and the corresponding neighbours in the last 12 frames (Tpred = 12), given the observations for first 9 frames (Tobs = 9) only.</p>

<p><strong>Please note:</strong> You are supposed to <em>append</em> your predicted tracks to the test scenes and tracks. The observed test tracks do not have the pred_number and scene_id attributes (set to None). The predicted tracks (last 12 frames) MUST have the pred_number (numbering starts from 0) and scene_id (corresponding to the id of scene being predicted) attributes, even when outputting a single prediction corresponding to each scene.</p>

<p>Your submitted file may contain multiple predictions corresponding to each scene. For unimodal metrics evaluation, the first prediction (prediction_number=0) will be considered. For top3_ADE and top3_FDE metrics, the first 3 predictions (prediction_number=0, 1 and 2) will be considered. Likewise, for NLL, the first 100 predictions will be considered.</p>

<p>An example of input test file and output prediction file is provided below.</p>

<p><a href=""https://drive.google.com/file/d/1PlPhnlIUbrMxtLICwSWG0gBLkaOMfAUB/view?usp=sharing"" target=""_blank"">Input Test</a></p>

<p><a href=""https://drive.google.com/file/d/1yEEQPWMqQ8JPRb4wlmXXSVQ-n_GUwTco/view?usp=sharing"" target=""_blank"">Output Predictions</a></p>

<p>As mentioned above, please submit a single .zip file that matches exactly the format given to you for testing.</p>

<h2 id=""evaluation"">Evaluation</h2>

<p>Once your files are correctly submitted, they will be graded with multiple criteria. The primary and secondary grades correspond to the final displacement error in the real test dataset and synthetic test dataset respectively. A figure comparing the submitted model to baseline Vanilla LSTM model and a table containing a detailed model evaluation will also be provided.</p>

<p>The result of the baseline: Vanilla LSTM
<a href=""https://github.com/vita-epfl/trajnetplusplusdata/blob/master/Baseline.png"" target=""_blank"">Baseline Score</a></p>

<h2 id=""resources"">Resources</h2>

<p>In this section, participants can find useful resources for the Trajnet++ challenge.</p>

<h3 id=""visualisations"">Visualisations</h3>
<p>We provide visualisations for the datasets provided in order to better understand the data. The visualisations capture attributes of human motion as well as nature of interactions in the different datasets.</p>

<p><a href=""https://github.com/vita-epfl/trajnetplusplustools"" target=""_blank"">Tools for TrajNet++ </a></p>

<h3 id=""baselines"">Baselines</h3>
<p>We provide baseline codes of important papers in trajectory prediction.</p>

<p><a href=""https://github.com/vita-epfl/trajnetplusplusbaselines"" target=""_blank"">Baseline algorithms for TrajNet++</a></p>
"
32,"<h1 id=""Submission""><em style=""color: red"">IMPORTANT: How to win the challenge!</em></h1>
<ul>
  <li>
    <p>Challenge Participants are required to submit a 4 or 4+ page document using the ICCV 2019 <a href=""http://iccv2019.thecvf.com/submission/main_conference/author_guidelines"">paper template</a> to describe data processing steps, network architectures, other implementation details such as hyper-parameters of the network training and the results.</p>
  </li>
  <li>
    <p>Please submit the document through the CMT system at &nbsp;<a rel=""noreferrer noopener"" href=""https://cmt3.research.microsoft.com/ADW2019"" target=""_blank"">https://cmt3.research.microsoft.com/ADW2019</a> by 20-10-2019 [11:59 p.m. Pacific Standard Time] in order to win the challenge.</p>
  </li>
  <li>
    <p>It is allowed to include the names of the authors and the affiliations.</p>
  </li>
  <li>Please be aware that the novelty of the method is also evaluated. The winners will be invited to present their work in the workshop.</li>
</ul>

<h1 class=""mt-0"" id=""news"">News</h1>
<ul>
  <li>
    <p>The top-performing teams will be required to include a four+ page write-up describing their method and code to reproduce their results to the claim victory. The detailed procedure for releasing the code is to be determined.</p>
  </li>
  <li>Username and password to download the csv and image data are located in the Resources-&gt;Drive360 Credentials text file.</li>
  <li>Added zipped starter-kit to resources tab.</li>
</ul>

<h1 id=""Motivation"">Why Learn-to-Drive?</h1>

<div class=""embed-responsive embed-responsive-16by9"">
        <iframe class=""embed-responsive-item"" src=""https://www.youtube.com/embed/mnnSf2KwTS4?rel=0"" allowfullscreen=""""></iframe>
</div>

<p>Autonomous driving has seen a surge in popularity not only in the academic community but also from industry, with millions and millions of industry dollars poured into the development of level 5 autonomous vehicles.</p>

<p>While the traditional autonomous driving stack based on explicit perception, path planning and control systems has been continuously developed and is widely deployed in level 3 autonomous cars nowadays, new angles of approach, for solving the level 5 problem have, nonetheless, emerged.</p>

<p>One such approach is end-to-end driving via imitation learning. The model learns to imitate and drive like the human driver. This approach aims to compliment the traditional autonomous driving stack with a fully learned driving model that implicitly handles perception, path planning and control.</p>

<p>Such a two-system architecture (traditional stack and learned driving model)  allows for better redundancy due systemic errors present in one system not propagating to the other as both systems are inherently different, one being model and the other being data driven.</p>

<p>In essence an end-to-end driving model is a neural network that takes some subset of the available sensor data from the vehicle and predicts future control output.</p>

<h1 id=""Introduction"">Learning-to-Drive Challenge</h1>

<p>Welcome to the <em>ICCV 2019: Learning-to-Drive Challenge</em> part of our  <a href=""https://adworkshop.org"">Autonomous Driving</a> workshop hosted at ICCV 2019.</p>

<p>The goal of this challenge is to develop state of the art driving models that can predict the future steering wheel angle and vehicle speed given a large set of sensor inputs.</p>

<h2 id=""sensor-data"">Sensor Data</h2>
<p>We supply the Drive360 dataset consisting of the following sensors:</p>

<ul>
  <li>4xGoPro Hero5 cameras (front, rear, right, left)</li>
  <li>Visual map from HERE Technologies</li>
  <li>Visual map from TomTom (may not be perfectly synced)</li>
  <li>Semantic map from HERE Technologies</li>
</ul>

<p>The Drive360 dataset is split into a train, validation and test partition.</p>

<h2 id=""task"">Task</h2>
<p>Challenge participants are tasked to design, develop and train a driving model that is capable of predicting the steering wheel angle and vehicle speed obtained from the vehicles CAN bus 1 second in the future.</p>

<p>Challenge participants can use any combination of camera images, visual map images and semantic map information as input to their models. It is also allowed to use past sensor information in addition to the present observations, however it is NOT allowed to use future sensor information.</p>

<p>A detailed summary of the dataset is given in the Drive360 section.</p>

<h2 id=""evaluation"">Evaluation</h2>
<p>Driving Models will be evaluated on the test partition using the mean-squared-error (MSE) performance for both steering wheel angle (‘canSteering’) and vehicle speed (‘canSpeed’) predictions with the human ground truth as a metric. Thus best performing driving models will drive identical to the human driver in these situations.</p>

<h1 id=""Drive360"">Drive360 Dataset</h1>

<p>Our Drive360 dataset contains around 55 hours of recorded driving around Switzerland. We recorded camera images, routing/navigation information and the human driving manoeuvrers (steering wheel angle and vehicle speed) via the CAN bus.</p>

<h2 id=""structure"">Structure</h2>

<p>The dataset is structured into runs that typically last 1-4 hours of continuous driving, these usually have place names such as Appenzell, Bern, Zurich, etc.</p>

<p>We have further split each run into 5 minute chapters, thus the run Aargau which lasted for around 50 minutes will have 10 chapters, while the run Appenzell which lasted for around 3 hour will have 37 chapters.</p>

<p>In total we have 682 chapters for our 27 routes. Out of this total we then <strong>randomly sample</strong> 548 chapters for training, 36 for validation and 98 for testing, giving us our dataset splits.</p>

<h2 id=""data-types"">Data Types</h2>
<p>We supply a specific csv file for each of the three phases (training, validation and testing) along with a zip file of our camera and map images, adhering to our run and chapter structure.</p>

<p>The columns of the csv file specify the data that is available, while each row is a time synchronized collection of the data from the available sensors.</p>

<p>IMPORTANT: We have already projected the targets (canSteering and canSpeed) 1s into the future, thus simply read a single row and predict the targets specified on that row. Everything is sampled at 10Hz.</p>

<h2 id=""starter-kit"">Starter-kit</h2>
<p>Check out our GettingStarted.ipynb Jupyter book in the <a href=""https://gitlab.aicrowd.com/shecker/learning_to_drive_starter_kit"">starter-kit</a> for an example on how to use the dataset to train a model.</p>

<p>This <a href=""https://gitlab.aicrowd.com/shecker/learning_to_drive_starter_kit"">starter-kit</a> also contains a <strong>dataset.py</strong> file with a <em>Drive360</em> python class that handles all dataset filtering and scaling (optional but strongly recommended).</p>

<p>The <em>Drive360</em> class also appropriately handles chapter boundaries when using temporal sequences of historic data. This is particularly important when generating a submission file, as we truncate the beginning of each chapter in the test partition by 10 seconds to allow more flexibility to the challenge participants when choosing the length of historic data they would like to use.</p>

<h2 id=""here-semantic-maps"">HERE Semantic Maps</h2>
<p>We give a short overview of the available semantic map information generously provided by HERE Technologies.</p>

<p>To obtain this data, we use a Hidden-Markov-Model path matcher to snap our noisy GPS trace, that we recorded during our drive, onto the underlying HERE road network. We then use the map matched positions to query the HERE database.</p>

<p><img src=""http://people.ee.ethz.ch/~heckers/Drive360/map002.png"" alt=""alt text"" title=""HERE Semantic Maps"" class=""md-image"" /></p>

<p>Group 1:</p>

<ul>
  <li><em>hereSignal</em>: road-distance to next traffic signal. (m)</li>
  <li><em>hereYield [1d]</em>:  road-distance to next yield instance. (m)</li>
  <li><em>herePedestrian[1c]</em>: road-distance to next pedestrian crossing. (m)</li>
  <li><em>hereIntersection</em>: road-distance to next intersection. (m)</li>
  <li><em>hereMmIntersection</em>: road-distance to next intersection but using map matched localization instead of recorded GPS coordinates. (m)</li>
</ul>

<p>Group 2:</p>

<ul>
  <li><em>hereSpeedLimit[2a]</em>: speed limit from ADAS Map. (km/h)</li>
  <li><em>hereSpeedLimit_2</em>: speed limit from Navigation Map. (km/h)</li>
  <li><em>hereFreeFlowSpeed[2b]</em>: average driving speed based on underlying road geometry. Measured by HERE. (km/h)</li>
</ul>

<p>Group 3:</p>

<ul>
  <li><em>hereCurvature</em>: inverse radius of the approximated road geometry. (1/m)</li>
</ul>

<p>Group 4:</p>

<ul>
  <li><em>hereTurnNumber</em>: index of road at next intersection to travel (counter-clockwise).</li>
</ul>

<p>Group 5:</p>

<ul>
  <li><em>hereSegmentExitHeading</em>: heading of the current road our car is on at next intersection. (degrees)</li>
  <li><em>hereSegmentEntryHeading[5a]</em>: heading of the road that our car will take at next intersection. (degrees)</li>
  <li><em>hereSegmentOthersHeading[5b]</em>: heading of all other roads at next intersection. (degrees)</li>
</ul>

<p>Group 6:</p>

<ul>
  <li><em>hereCurrentHeading</em>: current heading. (degrees)</li>
  <li><em>here1mHeading</em>: relative heading of map matched GPS coordinate in 1 meter. (degrees)</li>
  <li><em>here5mHeading</em>:  … in 5 meters. (degrees)</li>
  <li><em>here10mHeading[6c]</em>:  … in 10 meters. (degrees)</li>
  <li><em>here20mHeading[6d]</em>: … in 20 meters. (degrees)</li>
  <li><em>here50mHeading</em>: … in 50 meters. (degrees)</li>
</ul>

<h1 id=""getting-started"">Getting Started</h1>

<p>Please sign up for the challenge, download the dataset and then check out our starter kit which should get you going quite quickly.</p>

<p>If you have any questions please don’t hesitate to contact us, we are happy to help.</p>

<h1 id=""rounds"">Rounds</h1>

<ul>
  <li>Round 1:  July 29th ~ October 20th, 2019
    <ul>
      <li>Top placing participants will be announced at our workshop.</li>
      <li>The top-performing teams will be required to include a four+ page write-up describing their method and code to reproduce their results to the claim victory. The detailed procedure for releasing the code is to be determined.</li>
    </ul>
  </li>
  <li>Round 2: October 20th, 2019 ~ Open End</li>
</ul>

<h1 id=""rules"">Rules</h1>

<p>In addition to the Challenge rules, outlined when you participate, don’t forget to cite our work if you use the Drive360 dataset for your work, thanks!</p>

<p>[1] Hecker, Simon, Dengxin Dai, and Luc Van Gool. “End-to-end learning of driving models with surround-view cameras and route planners.” <em>Proceedings of the European Conference on Computer Vision (ECCV).</em> 2018.</p>

<p>[2] Hecker, Simon, Dengxin Dai, and Luc Van Gool. “Learning Accurate, Comfortable and Human-like Driving.” <em>arXiv preprint arXiv:1903.10995</em> (2019).</p>

<h1 id=""contact"">Contact</h1>

<ul>
  <li>Simon Hecker, heckers@vision.ee.ethz.ch</li>
  <li>Denxgin Dai, daid@vision.ee.ethz.ch</li>
</ul>
"
258,"<p><em>This challenge has been submitted to MICCAI 2020 and will take place subject to acceptance.</em></p>

<h1 class=""mt-0"" id=""introduction"">Introduction</h1>

<p style=""text-align: justify;"">This challenge will be presented at the 23rd International Conference on Medical Image Computing and Computer Assisted Intervention, October 4th to 8th, 2020 in Lima, Peru. The task is the automatic segmentation of Head and Neck (H&amp;N) tumors and metastatic lymph nodes in FDG-PET and CT images. It will offer an opportunity for participants working on 3D segmentation algorithms to develop automatic bi-modal approaches for the segmentation of H&amp;N tumors in PET-CT scans, focusing on oropharyngeal cancers. Various approaches must be explored and compared to extract and merge information from the two modalities, including early or late fusion, full volume or patch-based approaches, 2-, 2.5- or 3-D approach.</p>

<h1 id=""timeline"">Timeline</h1>
<ul>
  <li>the release date of the training cases: 01/06/2020</li>
  <li>the release date of the test cases: 01/08/2020</li>
  <li>the submission date(s): opens 1/09/2020 closes 10/09/2020</li>
  <li>associated workshop days: 4/10/2020 or 8/10/2020</li>
  <li>the release date of the results: 15/09/2020</li>
</ul>

<h1 style=""text-align: justify;"" id=""motivation"">Motivation</h1>

<p>Radiomics, the prediction of disease characteristics using quantitative image biomarkers from medical images has shown tremendous potential to optimize patient care, particularly in the context of H&amp;N tumors (Vallières et al. 2017). However, it relies on an expensive and error-prone manual annotation process of Regions of Interest (ROI) to focus the analysis. The automatic segmentation of H&amp;N tumors and nodal metastases from FDG-PET and CT images will enable the validation of radiomics models on very large cohorts and with optimal reproducibility. By focusing on metabolic and morphological tissue properties respectively, FDG-PET and CT modalities include complementary and synergistic information for cancerous lesion segmentation. This challenge will allow identifying the best methods to leverage the rich bi-modal information in the context of H&amp;N tumor and metastatic lymph node segmentation. This precious knowledge will be transferable to other cancer types and radiomics studies. In previous work, automated PET-CT analysis has been proposed for different tasks, including lung cancer segmentation in (Kumar et al. 2019, Li et al. 2019, Zhao et al. 2018, Zhong et al. 2018) and bone lesion detection in (Xu et al. 2018). In (Moe et al. 2019), a PET-CT segmentation was proposed for a task similar to the one presented in this challenge, i.e. H&amp;N Gross Tumour Volume (GTV) delineation of the primary tumor as well as metastatic lymph nodes using a 2D U-Net architecture. This challenge will build upon this work by comparing, on a publicly available dataset, 2D and 3D recent segmentation architectures (V-Net) as well as the complementarity of the two modalities with quantitative and qualitative analyses. Finally, we evaluate the generalization of the trained algorithms to new centers in distinct geographic locations.</p>

<p>(Vallières et al. 2017) Vallières, Martin et al. “Radiomics strategies for risk assessment of tumour failure in head-and-neck cancer.” Scientific reports, 7(1):10117, 2017</p>

<p>(Kumar et al. 2019) Kumar, Ashnil, et al. “Co-learning feature fusion maps from PET-CT images of lung cancer.” IEEE Transactions on Medical Imaging 39.1 (2019): 204-217.</p>

<p>(Li et al. 2019) Li, Laquan, et al. “Deep learning for variational multimodality tumor segmentation in PET/CT.” Neurocomputing (2019).</p>

<p>(Zhao et al. 2018) Zhao, Xiangming, et al. “Tumor co-segmentation in PET/CT using multi-modality fully convolutional neural network.” Physics in Medicine &amp; Biology 64.1 (2018): 015011.</p>

<p>(Zhong et al. 2018) Zhong, Zisha, et al. “3D fully convolutional networks for co-segmentation of tumors on PET-CT images.” 2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018). IEEE, 2018.</p>

<p>(Xu et al. 2018) Xu, Lina, et al. “Automated whole-body bone lesion detection for multiple myeloma on 68Ga-Pentixafor PET/CT imaging using deep learning methods.” Contrast media &amp; molecular imaging 2018 (2018).</p>

<p>(Moe et al. 2019) Moe, Yngve Mardal, et al. “Deep learning for automatic tumour segmentation in PET/CT images of patients with head and neck cancers.” arXiv preprint arXiv:1908.00841 (2019).</p>

<h1 style=""text-align: justify;"" id=""evaluation-criteria"">Evaluation Criteria</h1>

<p>The Dice Similarity Coefficient (DSC) will be computed from the 3D volumes to assess the performance of segmentation algorithms by comparing the automatic segmentation and the annotated ground truth. Participant’s runs will be ranked based on the average DSC across all test cases. The method with the highest average DSC will be best. DSC measures volumetric overlap between segmentation results and annotations. It is an appropriate measure of segmentation for imbalanced segmentation problems, i.e. when the region to segment is small as compared to the image size. DSC is commonly used in the evaluation of segmentation algorithms and particularly tumor segmentation tasks (Gudi et al. 2017), (Song et al. 2013), (Blanc-Durand et al. 2018), (Moe et al. 2019), (Menze et al. 2015). One aim of the developed algorithms is to further perform radiomics studies to predict clinical outcomes. DSC mostly evaluates the segmentation inside the ground truth volume (similar to intersection over union) and less the segmentation precision at the boundary. Therefore, DSC is particularly relevant for radiomics where first and second-order statistics are most relevant and less sensitive to small changes of the contour boundaries (Depeursinge et al. 2015). When compared to e.g. lung cancer, shape features are less useful in H&amp;N because the oropharyngeal tumors are not spiculated and constrained by the anatomy of the throat.</p>

<p>(Gudi et al. 2017) Gudi, Shivakumar, et al. “Interobserver variability in the delineation of gross tumour volume and specified organs-at-risk during IMRT for head and neck cancers and the impact of FDG-PET/CT on such variability at the primary site.” Journal of medical imaging and radiation sciences 48.2 (2017): 184-192.</p>

<p>(Song et al. 2013) Song, Qi, et al. “Optimal co-segmentation of tumor in PET-CT images with context information.” IEEE transactions on medical imaging 32.9 (2013): 1685-1697.</p>

<p>(Blanc-Durand et al. 2018) Blanc-Durand, Paul, et al. “Automatic lesion detection and segmentation of 18F-FET PET in gliomas: a full 3D U-Net convolutional neural network study.” PLoS One 13.4 (2018).</p>

<p>(Moe et al. 2019) Moe, Yngve Mardal, et al. “Deep learning for automatic tumour segmentation in PET/CT images of patients with head and neck cancers.” arXiv preprint arXiv:1908.00841 (2019).</p>

<p>(Menze et al. 2014) Menze, Bjoern H., et al. “The multimodal brain tumor image segmentation benchmark (BRATS).” IEEE transactions on medical imaging 34.10 (2014): 1993-2024.</p>

<p>(Depeursinge et al. 2015) Depeursinge, Adrien, et al. “Predicting adenocarcinoma recurrence using computational texture models of nodule components in lung CT.” Medical physics 42.4 (2015): 2054-2063.</p>

<h1 style=""text-align: justify;"" id=""prizes"">Prizes</h1>

<p style=""text-align: justify;"">We will offer a 500 euros winner prize. We will organize presentations and results at a half-day event at MICCAI 2020.</p>

<h1 id=""organiser-info"">Organiser Info</h1>

<ul>
  <li>Vincent Andrearczyk: Vincent Andrearczyk completed his PhD degree on deep learning for texture and dynamic texture analysis at Dublin City University in 2017. He is currently a post-doctoral researcher at the University of Applied Sciences and Arts Western Switzerland with a research focus on deep learning for texture analysis and medical imaging. Vincent co-organized ImageCLEF 2018 Caption detection and prediction challenge and his team at HES-SO Valais has extensive experience in organizing challenges (various tasks in ImageCLEF every year since 2012)</li>
  <li>Valentin Oreiller: Valentin Oreiller received his M.Sc. degree in bioengineering from the Swiss Federal Institute of Technology (EPFL), Lausanne, Switzerland with a specialization in bioimaging. He is currently a PhD candidate at the University of Applied Sciences and Arts Western Switzerland with a research focus on radiomics.</li>
  <li>Martin Vallières: Martin Vallières is a newly appointed Assistant Professor in the Department of Computer Science of Université de Sherbrooke (April 2020). He received a PhD in Medical Physics from McGill University in 2017, and completed post-doctoral training in France and USA in 2018 and 2019. The overarching goal of Martin Vallières’ research is centered on the development of clinically-actionable models to better personalize cancer treatments and care (“precision oncology”). He is an expert in the field of radiomics (i.e. the high-throughput and quantitative analysis of medical images) and machine learning in oncology. Over the course of his career, he has developed multiple prediction models for different types of cancers. His main research interest is now focused on the graph-based integration of heterogeneous medical data types for improved precision oncology. He has shared various datasets on The Cancer Imaging Archive (TCIA), including Soft-tissue sarcoma: FDG-PET/CT and MR imaging data of 51 patients, with tumors contours (RTstruct) and clinical data, Low-grade gliomas: Tumour contours for MR images of 108 patients of the TCGA-LGG dataset in MATLAB format, and Head-and-neck: FDG-PET/CT imaging data of 300 patients, with RT plans (RTstruct, RTdose, RTplan) and clinical data. Moreover, he has co-organized the PET radiomics challenge: A MICCAI 2018 CPM Grand Challenge. He participated in the organization of the data online. He also contributed to the challenge data pool via the Head-and-neck TCIA collection.</li>
  <li>Joel Castelli: Dr Joël Castelli is an oncologist-radiation therapist at the radiation department in Centre Eugène Marquis, Rennes, France. He completed his PhD at the University of Rennes 1, France in 2017 on adaptive radiotherapy of head and neck cancers.</li>
  <li>Hesham Elhalawani: Hesham Elhalawani, MD, MSc is a radiation oncology clinical fellow at Cleveland Clinic. He completed a 3-year quantitative imaging biomarker research fellowship at MD Anderson Cancer Center. His deep-rooted research focus is leveraging artificial intelligence, radiomics, and imaging informatics to personalize cancer patients care. He published more than 50 peer-reviewed articles and served as a reviewer for journals and conferences, including Radiotherapy &amp; Oncology, Red Journal, European Radiology, and AMIA conferences. He is among the editorial board of Radiology: Artificial intelligence, an RSNA publication. He has been an advocate for FAIR principles of data management via contributing to the mission and goals of NCI Cancer Imaging Program. Collaboratively with The Cancer Imaging Archive (TCIA), they publicly shared two large curated head and neck cancer datasets that included matched clinical and multi-modal imaging data. Moreover, he served on the organizing committee for the 2016 and 2018 MICCAI radiomics challenges that were hosted on Kaggle in Class to fuel the growing trend in mass crowdsource innovation.</li>
  <li>Sarah Boughdad: Dr. Boughdad is currently a Fellow at the Service of Nuclear Medicine and Molecular Imaging at Lausanne University Hospital, Switzerland. In 2014, she graduated from the Medical Faculty of Paris-Sud, Paris-Saclay. She obtained her PhD in medical physics in 2018 from EOBE, Orsay University. She is an active researcher in the field of Radiomics.</li>
  <li>Mario Jreige: Mario Jreige, MD, is a nuclear medicine resident at Lausanne University Hospital, Switzerland. He has previously completed a specialization in radiology at the Saint-Joseph University, Beirut. He is a junior member of the Swiss Society of Nuclear Medicine.</li>
  <li>John O. Prior: John O. Prior, PhD MD, FEBNM has been Professor and Head of Nuclear Medicine and Molecular Imaging at Lausanne University Hospital, Switzerland since 2010. After graduating with a MSEE degree from ETH Zurich, he received a PhD in Biomedical Engineering from The University of Texas Southwestern Medical Center at Dallas and a MD from the University of Lausanne. He underwent thereafter specialization training in nuclear medicine in Lausanne and a visiting associate professorship at the University of California at Los Angeles (UCLA). Prof. Prior is currently President of the Swiss Society of Nuclear Medicine, Member of the European Association of Nuclear Medicine, the Society of Nuclear Medicine and Molecular Imaging, as well as IEEE Senior Member.</li>
  <li>Adrien Depeursinge: Adrien Depeursinge received the M.Sc. degree in electrical engineering from the Swiss Federal Institute of Technology (EPFL), Lausanne, Switzerland with a specialization in signal processing. From 2006 to 2010, he performed his Ph.D. thesis on medical image analysis at the University Hospitals of Geneva (HUG). He then spent two years as a Postdoctoral Fellow at the Department of Radiology of the School of Medicine at Stanford University. He has currently a joint position as an Associate Professor at the Institute of Information Systems, University of Applied Sciences Western Switzerland (HES-SO), and as a Senior Research Scientist at the Lausanne University Hospital (CHUV). A large experience in challenge organization (e.g. ImageCLEF, VISCERAL) exists in his group jointly led with Prof. Müller (MedGIFT). He also prepared a dataset of Interstitial Lung Disease (ILD) for comparison of algos open access dataset. The library contains 128 patients affected with ILDs, 108 image series with more than 41 liters of annotated lung tissue patterns as well as a comprehensive set of 99 clinical parameters related to ILDs. This dataset has become a reference for research on ILDs and the associated paper has &gt;100 citations.</li>
</ul>
"
20,"<blockquote class=""update"">
  <p>Submissions are <a href=""https://discourse.aicrowd.com/t/real-competition-submissions-open/1763"">open</a>!<br />
Deadlines have been extended, see <a href=""https://discourse.aicrowd.com/t/real-competition-round-1-extended/1982"">here</a>.</p>
</blockquote>

<h1 class=""mt-0"" id=""introduction"">Introduction</h1>

<p>Robots that learn to interact with the environment autonomously.</p>

<h1 id=""abstract"">Abstract</h1>
<p>Open-ended learning, also named ‘life-long learning’, ‘autonomous
curriculum learning’, ‘no-task learning’) aims to build learning
machines and robots that are able to acquire skills and knowledge
in an incremental fashion. The REAL competition, which is part of
NeurIPS 2019 competition track, addresses open-ended learning with
a focus on ‘Robot open-Ended Autonomous Learning’ (REAL), that is
on systems that: (a) acquire sensorimotor competence that allows
them to interact with objects and physical environments; (b) learn
in a fully autonomous way, i.e. with no human intervention, on the
basis of mechanisms such as curiosity, intrinsic motivations,
task-free reinforcement learning, self-generated goals, and any
other mechanism that might support autonomous learning. The
competition will have a two-phase structure where during a first
‘intrinsic phase’ the system will have a certain time to freely
explore and learn in the environment, and then during an
`extrinsic phase’ the quality of the autonomously acquired
knowledge will be measured with tasks unknown at design time. The
objective of REAL is to: (a) track the state-of-the-art in robot
open-ended autonomous learning; (b) foster research and the
proposal of new solutions to the many problems posed by open-ended
learning; (c) favour the development of benchmarks in the field.</p>

<h1 id=""challenge"">Challenge</h1>
<p>In this challenge, you will have to develop an algorithm to
control a multi-link arm robot interacting with a table, a shelf
and a few objects. The robot is supposed to interact with the
environment and learn in autonomous manner, i.e. no reward is
provided from the environment to direct its learning. The robot
has access to the state of its joint angle and to the output of a
fixed camera seeing the table from above. By interacting with the
environment, the robot should learn how to achieve different
states of the environment: e.g. how to push objects around, how to
bring them on top of the shelf and how to place them one on top of
the other.</p>

<p><img src=""https://github.com/GOAL-Robots/REALCompetitionStartingKit/raw/master/docs/figs/demo0.gif"" alt=""REAL environment"" width=""200"" />
<img src=""https://github.com/GOAL-Robots/REALCompetitionStartingKit/raw/master/docs/figs/demo1.gif"" alt=""REAL environment"" width=""200"" />
<img src=""https://github.com/GOAL-Robots/REALCompetitionStartingKit/raw/master/docs/figs/demo2.gif"" alt=""REAL environment"" width=""200"" /></p>

<h1 id=""evaluation"">Evaluation</h1>
<p>The evaluation of the algorithm is split in two phases: the
intrinsic phase and the extrinsic phase.
- In the first phase, the algorithm will be able to interact
with the environment, without being provided any reward. In
this intrinsic phase, the algorithm is supposed to learn the
dynamics of the environment and how to interact with it.
- In the second phase, a goal will be given to the algorithm
that it needs to achieve within a strict time limit. The goal
will be provided to the robot as an image of the state of the
environment it has to reach. This goal might require, for
example, to push an object in a certain position or move one
object on top of another.</p>

<h1 id=""how-to-do-it"">How to do it</h1>
<p>While the robot is given no reward for the environment, it is
perfectly reasonable (and expected) that the algorithm controlling
the robot will use some kind of “intrinsic” motivation derived
from its interaction with the environment. Below, we provide some
of the approach to this problem found in the current literature.
On the other hand, it would be “easy” for a human knowing the
environment (and the final tasks) as described in this page to
develop a reward function tailored to this challenge so that the
robot specifically learns to grasp objects and move them around.
This last approach is discouraged and it is not eligible to win the
competition (see the rules below). The spirit of the challenge is
that the robot initially does not know anything about the
environment and what it will be asked to do. So the approach
should be as general as possible.</p>

<p>The necessary software to participate in the
competition is available on GitHub at the following links:<br />
<a href=""https://github.com/AIcrowd/real_robots"">REALRobot gym environment</a><br />
<a href=""https://github.com/AIcrowd/neurips_goal_real_robots_starter_kit"">Submission Starting Kit</a>.</p>

<p><strong>Literature:</strong></p>

<ul>
  <li>Carlos Florensa, David Held, Xinyang Geng, Pieter Abbeel <a href=""https://arxiv.org/abs/1705.06366"">Automatic Goal Generation for Reinforcement Learning Agents</a></li>
  <li>Tianhe Yu, Gleb Shevchuk, Dorsa Sadigh, Chelsea Finn <a href=""https://arxiv.org/abs/1902.05542"">Unsupervised Visuomotor Control through Distributional Planning Networks</a></li>
  <li>Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, Trevor Darrell <a href=""https://arxiv.org/abs/1705.05363"">Curiosity-driven Exploration by Self-supervised Prediction</a></li>
  <li>Ashvin Nair, Vitchyr Pong, Murtaza Dalal, Shikhar Bahl, Steven Lin, Sergey Levine <a href=""https://arxiv.org/abs/1807.04742"">Visual Reinforcement Learning with Imagined Goals</a></li>
</ul>

<p><em>more to follow</em></p>

<h1 id=""rules"">Rules</h1>
<p><em>Note: rules might be updated while the competition is
run “on Beta”</em></p>

<p>The rules of the competition will be as follows:</p>

<ul>
  <li><strong>Overview</strong> The competition focuses on autonomous
open-ended learning with a simulated robot. The setup features
a simulated robot that in a first intrinsic phase interacts
autonomously with a partially unknown environment and learns
how to interact with it, and then in a second extrinsic phase
has to solve a number of tasks on the basis of the knowledge
acquired in the first phase. Importantly, in the intrinsic
phase the system does not know the tasks it will have to solve
in the extrinsic phase.</li>
  <li><strong>Simulator</strong> To this purpose, the competitors will be
given a software kit with which they will be able to install
the simulator of the robot and environment on their machines
(see below).</li>
  <li><strong>Robot</strong> The robot will be formed by: an arm; a
gripper; one camera.</li>
  <li><strong>Environment</strong> The environment used will be a
simplified kitchen-like scenario formed by: a table with; a
shelf; some kitchen objects.</li>
  <li><strong>Training and testing phases</strong> During the development
of the system, and during its evaluation (performance
scoring), the competitor systems will have to undergo two
phases: an intrinsic phase of training; an extrinsic phase of
testing.</li>
  <li><strong>Intrinsic phase</strong> During the intrinsic phase, the
robot will have to autonomously interact with an environment
for a certain period of time during which it should acquire as
much knowledge and skills as possible, to best solve the tasks
in the extrinsic phase. Importantly, during the intrinsic
phase the robot will not be aware of the tasks it will have to
solve in the extrinsic phase.</li>
  <li>
    <p><strong>Extrinsic phase</strong> During the extrinsic phase the
system will be tested for the quality of the knowledge
acquired during the intrinsic phase. <br />
During the extrinsic phase, the robot will undergo 3
challenges (see below) to be solved on the basis of the
knowledge acquired during the intrinsic phase. <br />
For every task the robot is given during these challenges, the
environment will be put in a different starting state and the
robot will be given a camera image of how the environment has
to look like when he has achieved the goal of the task.<br /></p>
  </li>
  <li>
    <p><strong>Learning time budget</strong> The time available for
learning in the intrinsic phase is limited to <strong>D<sub>int</sub></strong>
minutes of simulated time. Learning in the extrinsic phase
will be possible but its utility will be strongly limited by
the short time available to solve each task, consisting in <strong>D<sub>ext</sub></strong>
seconds of simulated time for solving each task.</p>
  </li>
  <li><strong>Three challenges</strong> During the extrinsic phase, there will be three kind of challenges.
The three challenges involve tasks drawn from the following classes of possible problems defined on the basis of the nature of the goal to accomplish.
For each task, the agent is given an image of the configuration of the objects it has to reach. Each time the agent is given a task, the objects are placed in a different starting position.
The challenges are as follows:
    <ul>
      <li>2D challenge: goal defined in terms of the configuration
of 3 objects on the table plane; objects will not be placed on the shelf.</li>
      <li>2.5D challenge: goal defined in terms of the configuration
of 3 objects on the table plane and on the shelf; one or more objects will have to be moved from the table to the shelf and vice-versa.<br />
For both 2D and 2.5D challenges, the objects will start in different positions for each task but they will have a fixed orientation, both in the initial positions and in the final configuration they are required to reach.</li>
      <li>3D challenge: goal defined in terms of a configuration of 3 objects with no restrictions (objects can assume any orientation and be in any part of the table and on the shelf).</li>
    </ul>
  </li>
  <li>
    <p><strong>Repetitions of the challenges</strong> Each challenge will
be repeated multiple times with different goals..<br /></p>
  </li>
  <li>
    <p><strong>Knowledge transfer</strong> The only regularities
(`structure’) that are shared between the intrinsic and the
extrinsic phase are related to the environment and objects; in
particular in the intrinsic phase the robot has no knowledge
about which tasks it will be called to solve in the extrinsic
phase. Therefore, in the intrinsic phase the robot should
undergo an autonomous open-ended learning process that should
lead it to acquire, in the available time, as much knowledge
and as many skills as possible to be ready to best face the
unknown tasks of the following extrinsic phase.</p>
  </li>
  <li><strong>Competition structure</strong> The competition will be divided into two rounds.
    <ul>
      <li><strong>Round 1:</strong> During the first round, submissions will be evaluated by running only the extrinsic phase. Participants will have to pre-train their robot controllers on their machines before submission. Top 20 ranked participants whose submissions follow the spirit of the rules will be able to participate to Round 2 (see also Spirit of the Rules and Code inspection below).</li>
      <li><strong>Round 2:</strong> during the second round, submissions will be evaluated by running both the intrinsic and extrinsic phase. All final submissions will be checked for coherence with the spirit of the rules.</li>
    </ul>
  </li>
  <li>
    <p><strong>Spirit of the rules</strong> As also explained above, the spirit of the rules is that during the intrinsic phase the robot is not explicitly given any task to learn and it does not know of the future extrinsic tasks, but it rather learns in a fully autonomous way. <br />
As such, the <strong>Golden Rule</strong> is that <u>it is explicitly forbidden to use the scoring function of the extrinsic phase or variants of it as a reward function to train the agent</u>. Participants should give as little information as possible to the robot, rather the system should learn from scratch to interact with the objects using curiosity, intrinsic motivations, self-generated goals, etc. <br />
However, given the difficulty of the competition and the many challenges that it contains and to encourage a wide participation, in Round 1 it will be possible to violate in part the aspects of the spirit of the competition, except the <strong>Golden Rule</strong> above. For example, it will be possible to use hardwired or pre-trained models for recognising the identity of objects and their position in space.<br />
All submissions, except those violating the Golden Rule, will be considered valid and ranked for Round 1. However, only submissions fully complying with the spirit of the rules will access Round 2 and take part to the final ranking.</p>
  </li>
  <li>
    <p><strong>Code inspection</strong> To be eligible for ranking, participants are required to open the source code of their submissions to the competition monitoring check. Submitted systems will be sampled for checking their compliance with the competition rules and spirit during the competition. Top ranked submission of Round 1 will be checked for admission to Round 2. All final submissions of Round 2 will be checked before announcing the final ranking and winners.</p>
  </li>
  <li><strong>Eligibility</strong> Participants belonging to the GOAL-Robots project, AIcrowd, or other parts of the Organization Team might participate to the competition to provide baselines for other participants but are ineligible for the final Round 1 and Round 2 competition ranking.</li>
</ul>

<h1 id=""important-dates"">Important dates</h1>
<ul>
  <li>3rd June, 2019 - Competition start as “Beta” - Environment
and Rules may be updated.</li>
  <li>~~8th July, 2019~~ <em>extended to 31st July 2019</em> - End of Beta. Leadeboards are reset. Rules
and environment finalized.</li>
  <li>~~30th September~~ <em>extended to 25th October 2019</em> - End of Round 1</li>
  <li>
    <p>~~31th October, 2019~~ <em>extended to 25th November 2019</em> - End of Round 2.</p>
  </li>
  <li>6th December, 2019 - Competition results are announced.</li>
  <li>8-14th December - Competition results are presented on NeurIPS.</li>
</ul>

<h1 id=""team"">Team</h1>
<p>This competition is organized by the GOAL-Robots Consortium: <a href=""http://www.goal-robots.eu/"">www.goal-robots.eu</a>.</p>

<h1 id=""sponsor"">Sponsor</h1>
<p>Computational resources for online evaluations are offered by Google through the GCP research credits program.</p>

<h1 id=""contact"">Contact</h1>
<p>If you have any questions, please feel free to contact us:</p>

<ul>
  <li>
    <p>Emilio Cartoni <a href=""mailto:emilio.cartoni@yahoo.it"">emilio.cartoni@yahoo.it</a></p>
  </li>
  <li>
    <p>Sharada Mohanty <a href=""mailto:mohanty@aicrowd.com"">mohanty@aicrowd.com</a></p>
  </li>
</ul>

"
23,"
<blockquote class=""update"">

  <p>Download the <a href=""https://github.com/AIcrowd/neurips2019_disentanglement_challenge_starter_kit"">starter kit</a> to get started immediately. Following the instructions there will download all the necessary datasets and code. The starter kit itself can already be used to make a valid submission, hence providing an easy entry-point.</p>
</blockquote>

<blockquote class=""update"">

  <p>To stay up to date with further announcements you may also follow us on <a href=""https://twitter.com/disen_challenge"">twitter</a>.</p>
</blockquote>

<h1 class=""mt-0"" id=""introduction"">NeurIPS2019 Disentanglement Challenge - Bringing Disentanglement to the Real-World!</h1>

<p><img src=""https://i.imgur.com/kcFtBa4.png"" alt=""mpi"" /></p>

<table class=""tg"">
  <tr>
    <th class=""tg-lboi""><center><img src=""https://i.imgur.com/FOiH5WR.png"" width=""50"" /></center></th>
    <th class=""tg-lboi"">Notice: Submit the report of Stage 2: <a href=""https://openreview.net/group?id=NeurIPS.cc/2019/Workshop/DC_S2"">Here</a></th>
  </tr>
</table>
<table class=""tg"">
  <tr>
    <th class=""tg-lboi""><center><img src=""https://i.imgur.com/FOiH5WR.png"" width=""50"" /></center></th>
    <th class=""tg-lboi"">LateX template: <a href=""https://storage.googleapis.com/disentanglement_report_template/Stage1/Archive.zip"">Here</a>(May need JMLR package <a href=""https://ctan.org/tex-archive/macros/latex/contrib/jmlr"">Here</a>)</th>
  </tr>
</table>

<p>The success of machine learning algorithms depends heavily on the representation of the data. It is widely believed that good representations are <em>distributed</em>, <em>invariant</em> and <em>disentangled</em> [1]. This challenge focuses on disentangled representations where explanatory factors of the data tend to change independently of each other. Independent codes have been proven to be useful in different areas of machine learning such as causal inference [2], reinforcement learning [3], efficient coding [4], and neuroscience [5].</p>

<p>Since real-world data is notoriously costly to collect, many recent state-of-the-art
disentanglement models have heavily relied on synthetic toy data-sets [6, 7].</p>

<p>While synthetic datasets are cheap, easy to generate and independent generative factors can be controlled,  the performance of state-of-the-art unsupervised disentanglement learning on real-world data is unknown.</p>

<p>Given the growing importance of the field and the potential societal impact in the medical domain or fair decision making [e.g. 8, 9, 10], it is high time to bring disentanglement to the <em>real-world</em>:</p>

<p>Stage 1: Sim-to-real transfer learning - design representation learning algorithms on simulated data and transfer them to the real world.</p>

<p>Stage 2: Advancing disentangled representation learning to complicated physical objects.</p>

<p>More details for each stage are provided below.</p>

<h1 id=""summary"">Summary</h1>

<p>Contestants can participate by implementing a trainable disentanglement algorithm and submitting it to the evaluation server.  Participants will have to submit their code to AIcrowd which will be evaluated via the AIcrowd evaluators to come up with their score (as described below).</p>

<p>The submitted method will access a dataset on the evaluation server. The challenge objective is to let the method automatically determine the dataset’s independent factors of variation in an unsupervised fashion.</p>

<p>In order to prevent overfitting, the dataset used to compute the final scores is kept completely hidden from participants until the respective challenge stage is terminated. Participants are encouraged to find robust disentanglement methods that work well without the need for manual adjustments.</p>

<p>Additionally, participants are required to submit a three-page report on their method to OpenReview. Detailed requirements for the report are given below.</p>

<p>The final score used to rank the participants and determine winners is a mixture of several disentanglement metrics. Details about the evaluation procedure can be found below.</p>

<p>The library disentanglement_lib [<a href=""https://github.com/google-research/disentanglement_lib"">Link</a>] provides several datasets, disentanglement methods and evaluation metrics, which gives participants an easy way to get started.</p>

<p>A public leaderboard shows rankings of the methods submitted until then (but not taking into account the quality of the report). The final ranking can differ substantially from the one on the public leaderboard because a different dataset will be used to determine the score.</p>

<h1 id=""stages"">Stages</h1>

<p>The challenge is split into two stages. In each stage there are three different datasets [6] (all of which include labels):</p>

<ul>
  <li>(1) a dataset which is publicly available,</li>
  <li>(2) a dataset which is used for the public leaderboard,</li>
  <li>(3) a dataset which is used for the private leaderboard.</li>
</ul>

<p>The Github repository [<a href=""https://github.com/rr-learning/disentanglement_dataset"">Link</a>] contains the necessary information to use dataset (1).</p>

<p>The participants may use dataset (1) to develop their methods. Each method which is submitted will be retrained and evaluated on dataset (2) as well as dataset (3).</p>

<p>In the first stage, the goal is to transfer from simulated to real images. Dataset (1) will correspond to simplistic simulated images, (2) to more realistically simulated images and (3) will consist of real images of the same setup.</p>
<center>Simple simulated data</center>
<center><img src=""https://i.imgur.com/fSudgLx.gif"" width=""500"" /></center>

<p>In the second stage, the goal is to transfer to unseen objects. (1) will consist of all datasets used in the first phase, (2) will consist of realistically simulated images of objects which are not included in (1) and (3) will consist of real images of those unseen objects.</p>

<p>Note: while the publicly released dataset is ordered according to the factors of variation, the private datasets will be randomly permuted prior to training. This means that at training time the images will be effectively unlabeled.</p>

<h1 id=""timeline"">Timeline</h1>

<ul>
  <li><strong>June 28th, 2019</strong>: Stage 1 starts</li>
  <li><strong>August 6th, 2019</strong>, 11:59pm AoE Submission deadline for methods, Stage 1</li>
  <li>~~<strong>August 16th, 2019</strong>, 11:59pm AoE~~ <strong>September 23rd. 2019</strong> 11:59 AM GMT: Submission deadline for reports, Stage 1</li>
  <li><strong>September 2nd, 2019</strong>: Stage 2 starts.</li>
  <li><strong>October 1st, 2019</strong>, 11:59pm AoE Submission deadline for methods, Stage 2</li>
  <li>~~<strong>October 23rd, 2019</strong>, 11:59pm GMT~~ <strong>November 13th, 2019</strong>, 11:59pm AoE : Submission deadline for reports, Stage 2</li>
</ul>

<h1 id=""writing-a-report"">Writing a Report</h1>

<p>Contestants have to provide a three-page document (with additionally up to 10 pages of references and supplementary material) providing all the necessary details of their approach on the challenge venue on <a href=""https://openreview.net/group?id=NeurIPS.cc/2019/Workshop/DC_S1"">Openreview</a>. This guarantees the reproducibility of the results and the transparency needed to advance the state of the art for learning disentangled representations. The report has to be submitted according to the deadlines provided above.</p>

<p>Participants are required to use a LaTeX template we provide to prepare the reports, changing formatting is not allowed. The template will be released before Stage 1 ends.</p>

<p>The report has a maximum length of three pages with an appendix of ten pages. However, reviewers are not required to consider the appendix and all essential information must be contained in the main body of the report. Submissions must fulfill some essential requirements in terms of clarity and precision: The information contained in the report should be sufficient for an experienced member of the community to reimplement the proposed method (including hyperparameter optimization) and reviewers may check coherence between the report and the submitted code.</p>

<p>Reports which do not satisfy those requirements will be disqualified along with the corresponding methods.</p>

<p>We encourage all participants to actively engage in discussions on OpenReview. While not a formal challenge requirement, every participant who submits a report should commit to review or comment on at least three submissions which are not their own.</p>

<h1 id=""evaluation-criteria"">Evaluation Criteria</h1>

<p>Methods are scored as follows:</p>

<p>The model is evaluated on the full dataset using each of the following metrics (as implemented in disentanglement_lib [<a href=""https://github.com/google-research/disentanglement_lib/tree/master/disentanglement_lib/evaluation/metrics"">Link</a>]):</p>

<ul>
  <li>IRS</li>
  <li>DCI</li>
  <li>Factor-VAE</li>
  <li>MIG</li>
  <li>SAP-Score</li>
</ul>

<p>The final score for a method is determined as follows:</p>

<ul>
  <li>All participants’ methods are ranked independently according to each of the five evaluation metrics</li>
  <li>Those 5 <em>ranks</em> are summed up for each method to give the method’s <strong>final score</strong> (higher is better)</li>
</ul>

<p>Teams whose reports do not satisfy basic requirements of clarity and thoroughness (as detailed above) will be disqualified.</p>

<p>Furthermore, the goal of this challenge is to advance the state-of-the-art in representation learning, hence we reserve the right to disqualify methods which are overly tailored to the type of data used in this challenge. By overly tailored methods we mean methods which will by design not work on slightly different problems, e.g. a slightly different mechanical setup for moving the objects.</p>

<p>The organizers may decide to change the computation of the scores for Stage 2. If so, this will be announced at the end of Stage 1.</p>

<p>Prizes are awarded to the participants in the order of their methods’ final scores (the lowest score wins), excluding participants who are not eligible for prizes. Prizes are awarded independently in each of the two challenge stages.</p>

<h1 id=""prizes"">Prizes</h1>

<p>In each of the two stages, the following prizes are awarded to the participants with the best scores.</p>

<h2 id=""stage-1"">Stage 1</h2>

<ul>
  <li><strong>Winner:</strong> 3,000 EUR</li>
  <li><strong>Runner-up:</strong> 1,500 EUR</li>
  <li><strong>Third-place:</strong> 1,000 EUR</li>
  <li><strong>Best Paper:</strong> 3,000 EUR</li>
  <li><strong>Runner-up best paper:</strong> 1,500 EUR</li>
</ul>

<h2 id=""stage-2"">Stage 2</h2>

<ul>
  <li><strong>Winner:</strong> 3,000 EUR</li>
  <li><strong>Runner-up:</strong> 1,500 EUR</li>
  <li><strong>Third-place:</strong> 1,000 EUR</li>
  <li><strong>Best Paper:</strong> 3,000 EUR</li>
  <li><strong>Runner-up best paper:</strong> 1,500 EUR</li>
</ul>

<p>The reports of the best-performing teams will be published in JMLR proceedings.</p>

<p>The winners are determined independently in each of the rounds. Winners of Stage 1 are <em>not</em> excluded from winning prizes in Stage 2.</p>

<p>We award <em>two Brilliancy prizes for each stage</em> to the best paper that were determined by the jury to be the most innovative and best described.</p>

<p>Cash prizes will be paid out to an account specified by the organizer of each team. It is the responsibility of the team organizer to distribute the prize money according to their team-internal agreements.</p>

<h1 id=""eligibility"">Eligibility</h1>

<p>The organizers will not be able to transfer the prize money to accounts of any of the following countries or regions. (Please note that residents of these countries or regions are still allowed to participate in the challenge.) The same applies to candidates which are stated on the EU sanction list.</p>

<ul>
  <li>The Crimea region of Ukraine</li>
  <li>Cuba</li>
  <li>Iran</li>
  <li>North Korea</li>
  <li>Sudan</li>
  <li>Syria</li>
  <li>Quebec, Canada</li>
  <li>Brazil</li>
  <li>Italy</li>
</ul>

<p>Members/employees of the following institutions may participate in the challenge, but are excluded from winning any prizes:</p>

<ul>
  <li>Max Planck Institute for Intelligent Systems</li>
  <li>ETH Zurich (Computer Science Department)</li>
  <li>Google AI Zurich</li>
  <li>AIcrowd</li>
</ul>

<p>Reviewers of the paper “On the Role of Inductive Bias From Simulation and the Transfer to the Real World: a New Disentanglement Dataset” may participate in the challenge, but are excluded from winning any prizes</p>

<h1 id=""further-rules"">Further Rules</h1>

<ul>
  <li>Participants may participate alone or in a team of up to 6 people in one or both stages of the challenge.</li>
  <li>Individuals are not allowed to enter the challenge using multiple accounts. Each individual can only be part of one team.</li>
  <li>To be eligible to win prizes, participants agree to release their code under an OSI approved license.</li>
  <li>The organizers reserve the right to change the rules if doing so is absolutely necessary to resolve unforeseen problems.</li>
  <li>The organizers reserve the right to disqualify participants who are violating the rules or engage in scientific misconduct.</li>
  <li>The organizers reserve the right to disqualify any participant engaging in unscientific behavior or which harm a successful organization of the challenge in any way.</li>
</ul>

<h1 id=""organizing-team"">Organizing Team</h1>

<p>This challenge is jointly organized by members from the <a href=""https://www.is.mpg.de/en"">Max Planck Institute for Intelligent Systems</a>, <a href=""https://www.ethz.ch/de.html"">ETH Zürich</a>, <a href=""https://mila.quebec/"">Mila</a>, and <a href=""https://research.google.com/teams/brain/"">Google Brain</a>.</p>

<p>The Team consists of:</p>

<ul>
  <li>Stefan Bauer (MPI)</li>
  <li>Manuel Wüthrich (MPI)</li>
  <li>Francesco Locatello (MPI, ETH)</li>
  <li>Alexander Neitz (MPI)</li>
  <li>Arash Mehrjou (MPI)</li>
  <li>Djordje Miladinovic (ETH)</li>
  <li>Waleed Gondal (MPI)</li>
  <li>Olivier Bachem (Google Research, Brain Team)</li>
  <li>Sharada Mohanty (AIcrowd)</li>
  <li>Martin Breidt (MPI)</li>
  <li>Nasim Rahaman (Mila)</li>
  <li>Valentin Volchkov (MPI)</li>
  <li>Joel Bessekon Akpo (MPI)</li>
  <li>Yoshua Bengio (Mila)</li>
  <li>Karin Bierig (MPI)</li>
  <li>Bernhard Schölkopf (MPI)</li>
</ul>

<h1 id=""logos"">Logos</h1>

<p><img src=""https://i.ibb.co/ykN0R28/mpi.png"" alt=""mpi"" /></p>

<p><br /><br />
<img src=""https://i.ibb.co/b1XB6K1/eth.png"" alt=""drawing"" width=""300"" />
<br /><br /></p>

<p><img src=""https://i.ibb.co/bsnvQKq/mila.png"" alt=""mila"" />
<img src=""https://i.ibb.co/5GtR81J/google.png"" alt=""google"" />
<br /><br />
<img src=""https://i.ibb.co/zxCX5sY/539f3ffbecad044276726c01-750-273.jpg"" alt=""drawing"" width=""340"" />
<br /><br />
# Sponsors</p>

<ul>
  <li>Max Planck Institute for Intelligent Systems</li>
  <li>ETH Zürich</li>
  <li>Montreal Institute for Learning Algorithms</li>
  <li>Google AI</li>
  <li>Amazon</li>
</ul>

<h1 id=""references"">References</h1>

<p>[1] Bengio, Y., Courville, A. and Vincent, P. <em>Representation learning: A review and new perspectives</em>. IEEE transactions on pattern analysis and machine intelligence, 35(8), pp.1798-1828. 2013.</p>

<p>[2] Suter, R., Miladinovic, D., Schölkopf, B., Bauer, S. <em>Robustly Disentangled Causal Mechanisms: Validating Deep Representations for Interventional Robustness.</em> International Conference on Machine Learning. 2019.</p>

<p>[3] Lesort, T., Díaz-Rodríguez, N., Goudou, J. F., Filliat, D. <em>Disentangled State representation learning for control: An overview</em>. Neural Networks. 2018.</p>

<p>[4] Barlow, H. B. <em>Possible principles underlying the transformation of sensory messages.</em> Sensory communication 1. 217-234. 1961.</p>

<p>[5] Olshausen, B. A., Field, D. J. <em>Sparse coding of sensory inputs.</em> Current opinion in neurobiology 14.4. 481-487. 2004.</p>

<p>[6] Gondal, M. W., Wüthrich, M., Miladinović, Đ., Locatello, F., Breidt, M, Volchkov, V., Akpo, J., Bachem, O., Schölkopf, B., Bauer, S. <em>On the Transfer of Inductive Bias from Simulation to the Real World: a New Disentanglement Dataset</em>. arXiv preprint arXiv:1906.03292. 2019.</p>

<p>[7] Locatello, F., Bauer, S., Lucic, M., Gelly, S., Schölkopf, B., Bachem, O. <em>Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations.</em> International Conference on Machine Learning. 2019.</p>

<p>[8] Locatello, F., Abbati, G., Rainforth, T., Bauer, S., Schölkopf, B. and Bachem, O. <em>On the Fairness of Disentangled Representations</em>. arXiv preprint arXiv:1905.13662. 2019.</p>

<p>[9] Creager, E., Madras, D., Jacobsen, J.H., Weis, M.A., Swersky, K., Pitassi, T. and Zemel, R. <em>Flexibly Fair Representation Learning by Disentanglement</em>. International Conference on Machine Learning. 2019.</p>

<p>[10] Chartsias, A., Joyce, T., Papanastasiou, G., Semple, S., Williams, M., Newby, D., Dharmakumar, R. and Tsaftaris, S.A. <em>Factorised spatial representation learning: application in semi-supervised myocardial segmentation</em>. International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 490–498. Springer. 2018.</p>

<h2 id=""contact-us"">Contact Us</h2>

<p>Use one of the public channels:</p>

<ul>
  <li>Gitter Channel : <a href=""https://gitter.im/AIcrowd-HQ/disentanglement_challenge"">AIcrowd-HQ/disentanglement_challenge</a></li>
  <li><a href=""https://discourse.aicrowd.com/c/neurips-2019-disentanglement-challenge"">Discussion Forum</a></li>
</ul>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

<ul>
  <li>Organizers <a href=""mailto:disentanglement@tuebingen.mpg.de"">disentanglement@tuebingen.mpg.de</a></li>
  <li><a href=""mailto:mohanty@aicrowd.com"" target=""_blank"">mohanty@aicrowd.com</a></li>
</ul>
"
164,"<p><em>Note: Do not forget to read the <strong>Rules</strong> section on this page.</em></p>

<h3 id=""motivation"">Motivation</h3>

<p>With the increasing interest in artificial intelligence (AI) to support clinical decision making and improve patient engagement, opportunities to generate and leverage algorithms for automated medical image interpretation are currently being explored. Since patients may now access structured and unstructured data related to their health via patient portals, such access also motivates the need to help them better understand their conditions regarding their available data, including medical images.</p>

<p>The clinicians’ confidence in interpreting complex medical images can be significantly enhanced by a “second opinion” provided by an automated system. In addition, patients may be interested in the morphology/physiology and disease-status of anatomical structures around a lesion that has been well characterized by their healthcare providers – and they may not necessarily be willing to pay significant amounts for a separate office- or hospital visit just to address such questions. Although patients often turn to search engines (e.g. Google) to disambiguate complex terms or obtain answers to confusing aspects of a medical image, results from search engines may be nonspecific, erroneous and misleading, or overwhelming in terms of the volume of information.</p>

<h3 id=""challenge-description"">Challenge description</h3>

<p>Visual Question Answering is an exciting problem that combines natural language processing and computer vision techniques. Inspired by the recent success of visual question answering in the general domain, we conducted a pilot task in ImageCLEF 2018 to focus on visual question answering in the medical domain (<a href=""https://www.imageclef.org/2018/VQA-Med"" target=""_blank"">VQA-Med 2018</a>). Based on the success of the inaugural edition and the huge interest from both computer vision and medical informatics communities, we will continue the task this year with enhanced focus on a nicely curated enlarged dataset. Same as last year, given a medical image accompanied with a clinically relevant question, participating systems are tasked with answering the question based on the visual image content.</p>

<h3 id=""data"">Data</h3>

<p>The datasets include a training set of 3,200 medical images with 12,792 Question-Answer (QA) pairs, a validation set of 500 medical images with 2,000 QA pairs, and a test set of 500 medical images with 500 questions. In order to generate a more-focused set of questions for a meaningful task evaluation, we considered generating 4 categories of questions based on: Modality, Plane, Organ System, and Abnormality. Please see the readme file of the crowdAI dataset section for more detailed information.</p>

<h3 id=""submission-instructions"">Submission instructions</h3>

<hr />
<p><em>As soon as the submission is open, you will find a “Create Submission” button on this page (just next to the tabs)</em></p>

<hr />
<p>Further instructions on the submission format will be published soon.</p>

<h3 id=""citations"">Citations</h3>

<p>Information will be posted after the challenge ends.</p>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>
<p>More on the evaluation criteria will be published soon.</p>

<h3 id=""resources"">Resources</h3>

<h3 id=""contact-us"">Contact us</h3>
<ul>
  <li>
    <p>Discussion Forum: 
<a href=""https://www.crowdai.org/challenges/imageclef-2019-vqa-med/topics"" target=""_blank"">https://www.crowdai.org/challenges/imageclef-2019-vqa-med/topics </a></p>
  </li>
  <li>
    <p>Join our mailing list: 
<a href=""https://groups.google.com/d/forum/imageclef-vqa-med"" target=""_blank"">https://groups.google.com/d/forum/imageclef-vqa-med</a></p>
  </li>
</ul>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at:</p>

<ul>
  <li>Asma Ben Abacha &lt;asma.benabacha(at)nih.gov&gt;</li>
  <li>Sadid A. Hasan &lt;sadid.hasan(at)philips.com&gt;</li>
  <li>Vivek Datla &lt;vivek.datla(at)philips.com&gt;</li>
  <li>Joey Liu &lt;joey.liu(at)philips.com&gt;</li>
  <li>Dina Demner-Fushman &lt;ddemner(at)mail.nih.gov&gt;</li>
  <li>Henning Müller &lt;henning.mueller(at)hevs.ch&gt;</li>
</ul>

<h3 id=""more-information"">More information</h3>
<ul>
  <li>You can find additional information on the challenge here: <a href=""https://www.imageclef.org/2019/medical/vqa"" target=""_blank"">https://www.imageclef.org/2019/medical/vqa </a></li>
</ul>

<h3 id=""prizes"">Prizes</h3>

<p>ImageCLEF 2019 is an evaluation campaign that is being organized as part of the <a href=""http://clef2019.clef-initiative.eu/"" target=""_blank""> CLEF initiative </a> labs. The campaign offers several research tasks that welcome participation from teams around the world. The results of the campaign appear in the working notes proceedings, published by CEUR Workshop Proceedings (CEUR-WS.org). Selected contributions among the participants, will be invited for publication in the following year in the Springer Lecture Notes in Computer Science (LNCS) together with the annual lab overviews.</p>

<h3 id=""datasets-license"">Datasets License</h3>

"
211,"<p><em>Note: LifeCLEF Bird 2020 is divided into 2 subtasks (challenges). This challenge is about bird sound recognition in <strong>Monophone</strong> recordings. For information on the <strong>Stereo</strong> challenge click <a href=""/challenges/lifeclef-2020-bird-stereo"" target=""_blank"">here</a>. Both challenges share the same dataset, so registering for one of these challenges will automatically give you access to the other one.</em></p>

<p><em>Note: Do not forget to read the <strong>Rules</strong> section on this page. Pressing the red <strong>Participate button</strong> leads you to a page where you have to agree with those rules. You will not be able to submit any results before agreeing with the rules.</em></p>

<p><em>Note: Before trying to submit results, read the <strong>Submission instructions</strong> section on this page.</em></p>

<h1 id=""challenge-description"">Challenge description</h1>

<p>Automated recognition of bird sounds in continuous soundscapes can be a transformative tool for ornithologists, conservation biologists and citizen scientist alike. Long-term monitoring of ecosystem health relies on robust detection of indicator species – and birds are particularly well suited for that use case. However, designing a reliable detection system is not a trivial task and the shift in acoustic domains between high-quality example recordings (typically used for training a classifier) and low-quality soundscapes poses a significant challenge. The 2020 LifeCLEF bird recognition task for monophone recordings focuses on this use case. The goal is to design, train and apply an automated detection system can reliably recognize bird sounds in diverse soundscape recordings.</p>

<h1 id=""data"">Data</h1>
<hr />
<p><em>Training and validation data will be released in early February. Test data will follow in March.</em></p>

<hr />
<p><em>As soon as the data is released it will be available under the “Resources” tab.</em></p>

<hr />

<p>The training data will consist of example recordings for bird species from South and North America and Europe. The Xeno-canto community contributes this data and provides hundreds of high-quality recordings to this year’s challenge. Each recording will be accompanied by metadata containing information on recording location, date and other high-level descriptions provided by the recordists.</p>

<p>The test data will consist of soundscapes recorded in Colombia, the USA, and Germany. Each soundscape will be of ten-minute duration and will contain high quantities of (overlapping) bird vocalizations.</p>

<p>A representative validation dataset will be provided to locally test the system performance before submitting. The validation data will also include the official evaluation script used to assess submissions.</p>

<h1 id=""submission-instructions"">Submission instructions</h1>

<hr />
<p><em>As soon as the submission is open, you will find a “Create Submission” button on this page (next to the tabs).</em></p>

<hr />
<p><em>Before being allowed to submit your results, you have to first press the red participate button, which leads you to a page where you have to accept the challenges rules.</em></p>

<hr />

<p>The goal of this challenge is to submit a ranked list of species predictions for each 5-second interval of each soundscape. These lists are limited to max. 10 species per interval and have to include timestamps and confidence values. Each participating group will be allowed to submit 10 runs.</p>

<h1 id=""evaluation-criteria"">Evaluation criteria</h1>

<p>For the sake of consistency and comparability, the metrics will be the same as in previous edition. Since participants are required to submit ranked species lists, two popular ranking metrics will be employed: Sample-wise mean average precision and class-wise mean average precision. We will assess the overall system performance across all recording locations and individual performance for each site.</p>

<h1 id=""rules"">Rules</h1>

<p>The 2020 LifeCLEF bird recognition challenge for monophone recordings will feature a set of rules that is somewhat different from previous editions:</p>

<ul>
  <li>No other than the provided training data (audio and metadata) will be allowed to train a recognition system (this also excludes pre-trained models)</li>
  <li>The validation data can be used to locally test the system but is not allowed for training (this also applies for the test data)</li>
  <li>Only single-model performance will be assed and ensemble approaches are not allowed</li>
  <li>Participants are required to submit working notes that describe the approach</li>
</ul>

<p>LifeCLEF lab is part of the Conference and Labs of the Evaluation Forum: CLEF 2020. CLEF 2020 consists of independent peer-reviewed workshops on a broad range of challenges in the fields of multilingual and multimodal information access evaluation, and a set of benchmarking activities carried in various labs designed to test different aspects of mono and cross-language Information retrieval systems. More details about the conference can be found <a href=""http://clef2020.clef-initiative.eu/"" target=""_blank"">here</a>.</p>

<p>Submitting a working note with the full description of the methods used in each run is mandatory. Any run that could not be reproduced thanks to its description in the working notes might be removed from the official publication of the results. Working notes are published within CEUR-WS proceedings, resulting in an assignment of an individual DOI (URN) and an indexing by many bibliography systems including DBLP. According to the CEUR-WS policies, a light review of the working notes will be conducted by LifeCLEF organizing committee to ensure quality. As an illustration, LifeCLEF 2019 working notes (task overviews and participant working notes) can be found within <a href=""http://ceur-ws.org/Vol-2380/"" target=""_blank"">CLEF 2019 CEUR-WS</a> proceedings.</p>

<h2 id=""important"">Important</h2>

<p>Participants of this challenge will automatically be registered at CLEF 2020. In order to be compliant with the CLEF registration requirements, please edit your profile by providing the following additional information:</p>

<ul>
  <li>
    <p>First name</p>
  </li>
  <li>
    <p>Last name</p>
  </li>
  <li>
    <p>Affiliation</p>
  </li>
  <li>
    <p>Address</p>
  </li>
  <li>
    <p>City</p>
  </li>
  <li>
    <p>Country</p>
  </li>
  <li>
    <p><em>Regarding the username, please choose a name that represents your team.</em></p>
  </li>
</ul>

<p><em>This information will not be publicly visible and will be exclusively used to contact you and to send the registration data to CLEF, which is the main organizer of all CLEF labs</em></p>

<h1 id=""citations"">Citations</h1>

<p>Information will be posted after the challenge ends.</p>

<h1 id=""prizes"">Prizes</h1>

<h2 id=""cloud-credit"">Cloud credit</h2>

<p>The winner of each of the challenge will be offered a cloud credit grant of 5k USD as part of Microsoft’s AI for earth program.</p>

<h2 id=""publication"">Publication</h2>

<p>LifeCLEF 2020 is an evaluation campaign that is being organized as part of the <a href=""http://clef2020.clef-initiative.eu/"" target=""_blank"">CLEF initiative</a> labs. The campaign offers several research tasks that welcome participation from teams around the world. The results of the campaign appear in the working notes proceedings, published by CEUR Workshop Proceedings (CEUR-WS.org). Selected contributions among the participants, will be invited for publication in the following year in the Springer Lecture Notes in Computer Science (LNCS) together with the annual lab overviews.</p>

<h1 id=""resources"">Resources</h1>

<h2 id=""contact-us"">Contact us</h2>

<p><em>Discussion Forum</em></p>

<ul>
  <li>You can ask questions related to this challenge on the Discussion Forum. Before asking a new question please make sure that question has not been asked before.</li>
  <li>Click on Discussion tab above or direct link: <a href=""https://discourse.aicrowd.com/c/lifeclef-2020-bird-monophone"" target=""_blank"">https://discourse.aicrowd.com/c/lifeclef-2020-bird-monophone</a></li>
</ul>

<p><em>Alternative channels</em></p>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

<ul>
  <li>stefan[dot]kahl[at]informatik[dot]tu-chemnitz[dot]de</li>
  <li>wp[at]xeno-canto[dot]org</li>
  <li>glotin[at]univ-tln[dot]fr</li>
  <li>herve[dot]goeau[at]cirad[dot]fr</li>
  <li>fabian-robert[dot]stoter[at]inria[dot]fr</li>
</ul>

<h2 id=""more-information"">More information</h2>

<p>You can find additional information on the challenge here:
<a href=""https://www.imageclef.org/BirdCLEF2020"" target=""_blank"">https://www.imageclef.org/BirdCLEF2020</a></p>

"
225,"<h2 id=""dronerl-workshop"">DroneRL Workshop</h2>

<p><strong>Timeline</strong></p>

<ul>
  <li>
    <p>What is reinforcement learning? <a href=""https://docs.google.com/presentation/d/1juMEUcWgdg3IHfk33HvWxjVuwnNh4U-KurhBFtGTl6k/edit?usp=sharing"">slides</a></p>
  </li>
  <li>
    <p>The <code class=""highlighter-rouge"">DeliveryDrones</code> environment <a href=""https://docs.google.com/presentation/d/1k1wuReVWI04bYMPAQKyrPiajEZ2eVH9n4KdMIcy_Ntg/edit?usp=sharing"">slides</a> / <a href=""https://colab.research.google.com/drive/1gtZ1zZD1O1aT6v577Y3z9HfiXFo5u1zD"">notebook</a></p>
  </li>
  <li>
    <p>Q-learning and DQN <a href=""https://docs.google.com/presentation/d/1aWn3qE9_sJFMJeTyB6H4sPTQaB3R-2zz7DnWOXLiO9I/edit?usp=sharing"">slides</a> / <a href=""https://colab.research.google.com/drive/1ubKreDv8Cdd5UtB7BL-prnZWqd6dSKFs"">notebook</a></p>
  </li>
  <li>
    <p>DQN Tips &amp; Ticks <a href=""https://colab.research.google.com/drive/1OBlDdEiiZVRJ91P9DwR6PVGHDE21LoNn"">notebook</a></p>
  </li>
</ul>

<p><strong>Troubleshooting</strong></p>

<p>When running the notebook on your machine in Jupyter Lab, you will need to activate the <code class=""highlighter-rouge"">ipywidgets</code> plugin by running this command in the Conda environment</p>

<p><code class=""highlighter-rouge"">bash
conda env create -f environment.yml
jupyter labextension install @jupyter-widgets/jupyterlab-manager
</code></p>

<p><strong>Credits</strong></p>

<p>Authors: @pacm, @MasterScrat, @metataro</p>

<p>Contributors: @spMohanty</p>

<p>Part of this work was supported by the <a href=""http://exts.epfl.ch/"">EPFL Extension School</a> and <a href=""http://aicrowd.com/"">AIcrowd</a>.</p>

<p><strong>Licence</strong></p>

<ul>
  <li><a href=""https://opengameart.org/content/1616-ship-collection"">16ShipCollection</a> by master484 under Public Domain / CC0</li>
  <li><a href=""https://fonts.google.com/specimen/Inconsolata"">Inconsolata-Bold</a> by Raph Levien under <a href=""https://scripts.sil.org/cms/scripts/page.php?site_id=nrsi&amp;id=OFL_web"">Open Font License</a></li>
</ul>
"
69,"<p>For this choice of project task, you are supposed to predict good recommendations, e.g. of movies to users. We have acquired ratings of 10000 users for 1000 different items (think of movies). All ratings are integer values between 1 and 5 stars. No additional information is available on the movies or users.</p>

<p>All information of the task and some baselines are provided in <a href=""https://github.com/epfml/ML_course/blob/344456134d9b217798c4050e62c3be4d3de96c1c/labs/ex10/exercise10.pdf"">Exercise 10</a></p>

<p>Please see also detailed instructions on the course github.</p>

<h1 id=""file-descriptions"">File descriptions</h1>

<ul>
  <li>data_train.csv - the training set. Each entry consists of an ID of the form r3_c6 (meaning row 3 column 6) and the value between 1-5 stars, given by the user for this user/movie combination</li>
  <li>sampleSubmission.csv - a sample submission file in the correct format. You have to predict the star ratings of the matrix entries specified in this file. In this dummy submission example, a rating of 3 is predicted for all positions in question.</li>
</ul>

<h1 id=""evaluation-criteria"">Evaluation Criteria</h1>

<p>Your collaborative filtering algorithm is evaluated according to the prediction error, measured by root-mean-squared error (RMSE).</p>

<h1 id=""rules"">Rules</h1>

<p>Each participant is allowed to make <strong>5 submissions per day</strong> (i.e. up to 15 submissions per team per day). Failed submissions (e.g. wrong submission file format) do not count.</p>
"
14,"<p><strong>NEWS:</strong> Deadlines <a href=""http://www.cs.ox.ac.uk/isg/challenges/sem-tab/"">updated</a>. Please join our <a href=""https://groups.google.com/d/forum/sem-tab-challenge"">discussion group</a>.</p>

<p>This is a task of <a href=""https://iswc2019.semanticweb.org/challenges/"">ISWC 2019</a> “Semantic Web Challenge on Tabular Data to Knowledge Graph Matching”. The task is to annotate each of the specified table cells (entity mentions) of a give table set with a DBPedia entity. <a href=""http://www.cs.ox.ac.uk/isg/challenges/sem-tab/"">Click here</a> for the official challenge website.</p>

<h1 id=""task-description"">Task Description</h1>

<p>The task is to annotate a cell with an entity of DBpedia with the prefix of http://dbpedia.org/resource/.</p>

<p>Each submission should contain the annotation of the target cell. One cell can be annotated by one entity. Any of the wiki page redirected entities of the ground truth entity (defined by dbo:wikiPageRedirects) are regarded as correct. Case is NOT sensitive.</p>

<p>The submission file should be in CSV format. Each line should contain the annotation of one cell which is identified by a table id, a column id and a row id. Namely one line should have four fields: “Table ID”, “Column ID”, “Row ID” and “DBpedia entity”. The headers should be excluded from the submission file. Here is an example:
“9206866_1_8114610355671172497”,”0”,”121”,”http://dbpedia.org/resource/Norway”</p>

<p>Notes:</p>

<p>1) Table ID does not include filename extension; make sure you remove the .csv extension from the filename.</p>

<p>2) Column ID is the position of the column in the table file, starting from 0, i.e., first column’s ID is 0.</p>

<p>3) Row ID is the position of the row in the table file, starting from 0, i.e., first row’s ID is 0.</p>

<p>4) At most one entity should be annotated for one cell.</p>

<p>5) One submission file should have NO duplicate lines for  one cell.</p>

<p>6) Annotations for cells out of the target cells are ignored.</p>

<h1 id=""datasets"">Datasets</h1>

<p>Table set for Round #1: <a href=""https://www.cs.ox.ac.uk/isg/challenges/sem-tab/data/CEA_Round1.tar.gz"">Tables</a>, <a href=""https://www.cs.ox.ac.uk/isg/challenges/sem-tab/data/CEA_Round1_Targets.csv"">Target Cells</a></p>

<p>Table set for Round #2: <a href=""https://www.cs.ox.ac.uk/isg/challenges/sem-tab/data/Tables_Round2.tar.gz"">Tables</a>, <a href=""https://www.cs.ox.ac.uk/isg/challenges/sem-tab/data/CEA_Round2_Targets.csv"">Target Cells</a></p>

<p>Table set for Round #3: <a href=""https://www.cs.ox.ac.uk/isg/challenges/sem-tab/data/Tables_Round3.tar.gz"">Tables</a>, <a href=""https://www.cs.ox.ac.uk/isg/challenges/sem-tab/data/CEA_Round3_Targets.csv"">Target Cells</a></p>

<p>Data Description: One table is stored in one CSV file. Each line corresponds to a table row. Note that the first row may either be the table header or content. The cells for annotation are saved in a CSV file.</p>

<h1 id=""evaluation-criteria"">Evaluation Criteria</h1>

<p>Precision, Recall and F1 Score are calculated:</p>

<p>Precision = (# correctly annotated cells) / (# annotated cells)</p>

<p>Recall = (# correctly annotated cells) / (# target cells)</p>

<p>F1 Score = (2 * Precision * Recall) / (Precision + Recall)</p>

<p>Notes:</p>

<p>1) # denotes the number.</p>

<p>2) F1 Score is used as the primary score; Precision is used as the secondary score.</p>

<p>3) An empty annotation of a cell will lead to an annotated cell; we suggest to exclude the cell with empty annotation in the submission file.</p>

<h1 id=""prizes"">Prizes</h1>

<p>SIRIUS and IBM Research sponsor the prizes for the best systems.</p>

<h1 id=""rules"">Rules</h1>

<ol>
  <li>
    <p>Selected systems with the best results in Round 1 and 2 will be invited to present their results during the <a href=""https://iswc2019.semanticweb.org/"">ISWC conference</a> and the <a href=""http://om2019.ontologymatching.org/"">Ontology Matching workshop</a>.</p>
  </li>
  <li>
    <p>The prize winners will be announced during the <a href=""https://iswc2019.semanticweb.org/"">ISWC conference</a> (on October 30, 2019). We will take into account all evaluation rounds specially the ones running till the conference dates.</p>
  </li>
  <li>
    <p>Participants are encouraged to submit a system paper describing their tool and the obtained results. Papers will be published online as a volume of <a href=""http://ceur-ws.org/"">CEUR-WS</a> as well as indexed on <a href=""https://dblp.uni-trier.de/"">DBLP</a>. By submitting a paper, the authors accept the CEUR-WS and DBLP publishing rules.</p>
  </li>
  <li>
    <p>Please see additional information at our <a href=""http://www.cs.ox.ac.uk/isg/challenges/sem-tab/"">official website</a></p>
  </li>
</ol>
"
25,"<blockquote class=""update"">
  <p><strong>Update 08 Jan 2019</strong>: The submission system is now live on <a href=""https://easychair.org/conferences/?conf=wsdmcup2019ssspc"" target=""_blank""> EasyChair </a>. We hope to see many of you submit reports on your work for this challenge.</p>
</blockquote>

<blockquote class=""update"">
  <p><strong>Update 07 Jan 2019</strong>: The final results are now available. We will be contacting the teams to confirm that their code is open sourced and can be verified, but a provisional congratulations to the winning teams, and thank you all for participating in this challenge. We look forward to reading and hearing about your insights.</p>
</blockquote>

<blockquote class=""update"">
  <p><strong>Update 04 Jan 2019</strong>: Good luck to all contestants in the final hours of the challenge! Once the submission period has concluded we will begin the final leaderboard evaluations. Additionally, we are in the process of finalizing the paper submission system for the WSDM Cup workshop, the deadline will be January 11, 2019. In the meantime, see the Call for Papers section here on the challenge overview page for information. Please note that submitting a paper is mandatory in order to be considered for the winning leaderboard positions, and in order to be eligible for the prizes.</p>
</blockquote>

<blockquote class=""update"">
  <p><strong>Update 12 Dec 2018</strong>: We would like to make several announcements: (1) We are happy to share that Google have kindly offered to sponsor coupons for google cloud compute resources for participants of this challenge. Please see the ‘Google Sponsored Computational Resources’ section of the overview page for further details. (2) We have released the call for papers for the WSDM Cup Workshop day. Please see the ‘Rules’ and ‘Call for Papers’ sections of the overview page for further details. (3) We are now providing the training set split into 10 files to make it easier for participants with slow connections to download the training set. Please see the Training_Set_Split_Download.txt file under the Dataset tab for the download links. (4) There was some ambiguity in the description of the challenge metric which has now been clarified, see the ‘Evaluation’ section of the overview page for further details. Please note that the metric is unchanged we have simply clarified the terminology.</p>
</blockquote>

<blockquote class=""update"">
  <p><strong>Update 20 Nov 2018</strong>: Unfortunately we have had to make some changes to the challenge dataset. More specifically, we have had to remove some features from the track features table (the updated Dataset Description file outlines the new track features schema). Please note that the other parts of the dataset all remain unchanged, except the track features table in the mini version of the dataset which was changed correspondingly. We apologize for any inconvenience caused by this change. If your work on the challenge is affected, we would appreciate if you email us at <a href=""wsdm-cup-2019@spotify.com"" target=""_blank""> wsdm-cup-2019@spotify.com </a> so that we can better understand any potential impact on participants</p>
</blockquote>

<p>Spotify is an online music streaming service with over 190 million active users interacting with a library of over 40 million tracks. A central challenge for Spotify is to recommend the right music to each user. While there is a large related body of work on recommender systems, there is very little work, or data, describing how users sequentially interact with the streamed content they are presented with. In particular within music, the question of if, and when, a user skips a track is an important implicit feedback signal.</p>

<p>We release this dataset and challenge in the hope of spurring research on this important and understudied problem in streaming. Our challenge focuses on the task of session-based sequential skip prediction,  i.e. predicting whether users will skip tracks, given their immediately preceding interactions in their listening session.</p>

<p>The organization of this challenge is a joint effort of <a href=""https://www.spotify.com"" target=""_blank""> Spotify </a>, <a href=""http://www.wsdm-conference.org/2019/"" target=""_blank""> WSDM </a>, and <a href=""https://www.crowdai.org/"" target=""_blank""> CrowdAI </a>.</p>

<h2 id=""dataset"">Dataset</h2>

<p>The public part of the dataset consists of roughly 130 million listening sessions with associated user interactions on the Spotify service. In addition to the public part of the dataset, approximately 30 million listening sessions are used for the challenge leaderboard. For these leaderboard sessions the participant is provided all the user interaction features for the first half of the session, but only the track id’s for the second half. In total, users interacted with almost 4 million tracks during these sessions, and the dataset includes acoustic features and metadata for all of these tracks.</p>

<p>If you use this dataset in an academic publication, please cite the following paper:</p>

<p>@inproceedings{brost2019music,
  title={The Music Streaming Sessions Dataset},
  author={Brost, Brian and Mehrotra, Rishabh and Jehan, Tristan},
  booktitle={Proceedings of the 2019 Web Conference},
  year={2019},
  organization={ACM}
}</p>

<h2 id=""challenge"">Challenge</h2>

<p>The task is to predict whether individual tracks encountered in a listening session will be skipped by a particular user. In order to do this, complete information about the first half of a user’s listening session is provided, while the prediction is to be carried out on the second half. Participants have access to metadata, as well as acoustic descriptors, for all the tracks encountered in listening sessions.</p>

<p>The output of a prediction is a binary variable for each track in the second half of the session indicating if it was skipped or not, with a 1 indicating that the track skipped, and a 0 indicating that the track was not skipped. For this challenge we use the skip_2 field of the session logs as our ground truth.</p>

<p>There will be a workshop at WSDM where selected or top performing teams will be invited to present their work on this challenge. The paper submission deadline will be January 11, 2019, and the workshop will be held on February 15, 2019, as part of WSDM in Melbourne, Australia</p>

<h2 id=""how-to-generate-submissions"">How to generate submissions</h2>

<p>The test set sessions are always split between two files. Each session is partly contained in a prehistory file, and a corresponding input file. The full interaction feature set  for the first half of the session is contained in the prehistory file, and the track id’s for which you need to make a prediction are contained in the input file. For each test set session a row of 1’s and 0’s of the same length as the input part of the session must then be generated. Sample submissions are contained in the Sample_Submissions.tar.gz file under the Dataset tab, and code for generating a random submission is contained in the <a href=""https://github.com/crowdAI/skip-prediction-challenge-starter-kit"" target=""_blank""> Starter Kit </a>.</p>

<p>Accurate skip prediction can enable us to avoid recommending a potential track to the user, based on the user’s immediately preceding interactions. At a given moment in time, it is therefore most important to predict if the next immediate track is going to be skipped, but it would also be useful to predict if the tracks further into the session will be skipped. This motivates our use of Mean Average Accuracy as the primary metric for the challenge, with the average accuracy defined by</p>

<script type=""math/tex; mode=display"">\begin{align*}
AA = \frac{\sum_{i=1}^T{A(i)L(i)}}{T}
\end{align*}</script>

<p>where :</p>

<ul>
  <li><script type=""math/tex"">T</script> is the number of tracks to be predicted for the given session</li>
  <li><script type=""math/tex"">A(i)</script> is the accuracy at position <script type=""math/tex"">i</script> of the sequence</li>
  <li><script type=""math/tex"">L(i)</script> is the boolean indicator for if the <script type=""math/tex"">i</script>‘th prediction was correct.</li>
</ul>

<p>We will use the accuracy at predicting the first interaction in the second half of the session as a tie breaking secondary metric.</p>

<p>The competition rules are given below, the dataset terms of use and challenge terms and conditions can be found on the Dataset page.</p>

<ol>
  <li>Participants must agree to the dataset terms of use and challenge terms and conditions in order to participate in this challenge.</li>
  <li>Each [participant] account shall represent only one (1) team.</li>
  <li>Team mergers are allowed at any time prior to the two (2) week period before the competition end date and must be communicated in writing to the challenge organizers at <a href=""wsdm-cup-2019@spotify.com"" target=""_blank""> wsdm-cup-2019@spotify.com </a>.</li>
  <li>A merged team forms one (1) teams for the remainder of the competition, and such team must designate a single account for subsequent submissions in the competition. The other accounts associated with team members in a merged team shall not be used for further submissions.</li>
  <li>Each team may only make five (5) submissions per day. The last submission day is January 4, 2019.</li>
  <li>At the end of the challenge, each team is required to open source the source code that was used to generate their final challenge solution under the Apache license 2.0. To be eligible for the leaderboard or prizes, teams are also required to submit papers describing their method to the WSDM Cup Workshop. See the ‘Call for Papers’ section below for further details.</li>
  <li>Teams may use external data and pretrained models, provided these are public, and linked with the source code provided for the final submission.</li>
  <li>Teams may use the Spotify API to augment the dataset, provided this use complies with the terms of service for the Spotify API, and that the augmentation is included with the source code provided for the final submission.</li>
  <li>Spotify employees and their in-home relatives are prohibited from participating in the contest.</li>
  <li>Ask us: If you are not sure whether something is allowed or not, do not hesitate to contact us at <a href=""wsdm-cup-2019@spotify.com"" target=""_blank""> wsdm-cup-2019@spotify.com </a>! We can quickly give you the information you need.</li>
</ol>

<p>The prizes will be administered as part of the 2019 WSDM Cup. The winning team will be awarded 
<strong>AUD2000</strong>, the second placed team will be awarded <strong>AUD750</strong>, and the third placed team will be awarded <strong>AUD250</strong>. All prizes are in Australian Dollars.</p>

<h2 id=""call-for-papers"">Call for Papers</h2>

<p>Submissions must be in English, in PDF format, and should not exceed four pages in the current ACM two-column conference format (including references and figures). Suitable LaTeX and Word templates are available from the ACM Website. The papers can represent reports of original research, preliminary research results, or proposals for new work. The review process is ​​single-blind. ​Please mention the team name in the title or abstract, and provide a link to the repository for the open sourced code in your paper. Papers will be evaluated according to their significance, originality, technical content, style, clarity, and likelihood of generating discussion. The submission deadline is January 11, 2019 (AOE timezone).</p>

<p>Papers should be submitted on <a href=""https://easychair.org/conferences/?conf=wsdmcup2019ssspc"" target=""_blank""> EasyChair </a></p>

<p>A starter kit for participants to familiarize themselves with the dataset and challenge mechanics is provided at: <a href=""https://github.com/crowdAI/skip-prediction-challenge-starter-kit"" target=""_blank""> Starter Kit </a></p>

<p>Information about the Spotify API is provided at: <a href=""https://developer.spotify.com/documentation/web-api/"" target=""_blank""> Spotify API </a></p>

<p>For an introduction to some of the factors that affect user skip behaviour, see the following blog entry from Paul Lamere: <a href=""https://musicmachinery.com/2014/05/02/the-skip/"" target=""_blank""> MusicMachinery - Entry on skips </a></p>

<h2 id=""google-sponsored-computational-resources"">Google Sponsored Computational Resources</h2>

<p>We are very grateful to Google, who have kindly offered to sponsor 100 USD coupons for Google cloud compute resources for participants of this challenge. Teams that have made a valid submission are invited to send an email to <a href=""wsdm-cup-2019@spotify.com"" target=""_blank""> wsdm-cup-2019@spotify.com </a> to request a coupon. This email should have the title ‘Coupon’ and should provide the team name, and should be sent from the email associated with the account which made the valid submission. Every week a team makes an improved submission on the leaderboard, they will be eligible to request a further 100 USD coupon, for as long as coupons remain. Thus, if a team has already received a coupon, but makes an improved submission in the subsequent week starting Monday, they will be eligible for another request.</p>

<h2 id=""contact-us"">Contact Us</h2>

<p>Use one of the public channels:</p>

<ul>
  <li><strong>Gitter Channel</strong> : <a href=""https://gitter.im/crowdAI/spotify-sequential-skip-prediction-challenge"" target=""_blank""> https://gitter.im/crowdAI/spotify-sequential-skip-prediction-challenge </a></li>
  <li><strong>Technical issues</strong> : <a href=""https://github.com/crowdAI/skip-prediction-challenge-starter-kit/issues"" target=""_blank""> https://github.com/crowdAI/skip-prediction-challenge-starter-kit/issues </a></li>
  <li><strong>Discussion Forum</strong> : <a href=""https://www.crowdai.org/challenges/spotify-sequential-skip-prediction-challenge/topics"" target=""_blank""> https://www.crowdai.org/challenges/spotify-sequential-skip-prediction-challenge/topics </a></li>
</ul>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organisers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at:</p>

<ul>
  <li><a href=""mailto:wsdm-cup-2019@spotify.com"" target=""_blank""> wsdm-cup-2019@spotify.com </a></li>
  <li><a href=""mailto: sharada.mohanty@epfl.ch"" target=""_blank"">sharada.mohanty@epfl.ch</a></li>
  <li><a href=""mailto: brianbrost@spotify.com"" target=""_blank"">brianbrost@spotify.com</a></li>
</ul>
"
154,"<hr />
<p><strong>Important note:</strong></p>

<p><em>The <strong>ImageCLEF Caption - Caption Prediction challenge has officially ended</strong> and we would like to thank everybody for their participation. You can find the official results at <a href=""http://imageclef.org/2018/caption"">http://imageclef.org/2018/caption</a>.</em></p>

<p><em>Post-challenge submissions and the leaderboard will remain enabled  for a few weeks so you will still be able to submit result files and have them continuously evaluated during a limited period. 
Please consider that in order to see the version of the leaderboard with the post-challenge submissions integrated, you have to turn on the switch <strong>Show post-challenge submission</strong> right below the leaderboard.</em></p>

<p><em>At the same time we’d like to encourage you to submit a <a href=""http://clef2018.clef-initiative.eu/index.php?page=Pages/InstructionsforCLEF2018WorkingNotes.html"">CLEF Working notes paper</a> until the end of May.</em></p>

<p><em>Please also note that participants registering from now on will not be
automatically registered with CLEF anymore.</em></p>

<hr />

<p><em>Note: ImageCLEF Caption 2018 is divided into 2 subtasks (challenges). This challenge is about <strong>Caption Prediction</strong>. For information on the <strong>Concept Detection</strong> challenge click <a href=""/challenges/imageclef-2018-caption-concept-detection"" target=""_blank""> here </a>. Both challenges share the same dataset, so registering for one of these challenges will automatically give you access to the other one.</em></p>

<p><em>Note: Do not forget to read the <strong>Rules</strong> section on this page</em></p>

<h3 id=""motivation"">Motivation</h3>

<p>Interpreting and summarizing the insights gained from medical images such as radiology output is a time-consuming task that involves highly trained experts and often represents a bottleneck in clinical diagnosis pipelines.</p>

<p>Consequently, there is a considerable need for automatic methods that can approximate this mapping from visual information to condensed textual descriptions. In this task, we cast the problem of image understanding as a cross-modality matching scenario in which visual content and textual descriptors need to be aligned and concise textual interpretations of medical images are generated. We work on the basis of a large-scale collection of figures from open access biomedical journal articles (PubMed Central). Each image is accompanied by its original caption, constituting a natural testbed for this image captioning task.</p>

<p><em>Lessons learned</em>: In the first edition of this task, held at CLEF 2017, participants noted a broad topical variability among training images. This year, we will further group training data into image types (e.g., radiology vs. biopsy) and task participants will building either cross category models or category-specific ones. An additional source of uncertainty was noted in the use of external material. In this second edition of the task, we will clearly separate systems using exclusively the official training data from those that incorporate additional sources of evidence.</p>

<h3 id=""challenge-description"">Challenge description</h3>

<p>On the basis of the concept vocabulary detected in the first subtask as well as the visual information of their interaction in the image, participating systems are tasked with composing coherent captions for the entirety of an image. In this step, rather than the mere coverage of visual concepts, detecting the interplay of visible elements is crucial for strong performance. Evaluation of this second step is based on metrics such as BLEU that have been designed to be robust to variability in style and wording.</p>

<h3 id=""data"">Data</h3>

<p>The collection comprises a total of 4 million image-caption pairs that could potentially all be used for training with a small subset being removed for testing. To focus on useful radiology/clinical images and non-compound figures is likely good for this task to reduce the number of image-caption pairs to around 400,000, so significantly larger that in 2017.</p>

<h3 id=""submission-instructions"">Submission instructions</h3>

<hr />
<p><em>As soon as the submission is open, you will find a “Create Submission” button on this page (just next to the tabs)</em></p>

<hr />

<p>For the submission we expect the following format:</p>

<p>[Figure-ID] [TAB] [description]</p>

<p>e.g.:</p>

<div class=""highlighter-rouge""><div class=""highlight""><pre class=""highlight""><code>1743-422X-4-12-1-4   description of the first image in one single line
1743-422X-4-12-1-3   description of the second image....
1743-422X-4-12-1-2   descrition of the third image...
</code></pre></div></div>

<p>You need to respect the following constraints:</p>

<ul>
  <li>The separator between the figure ID and the description has to be a tabular whitespace</li>
  <li>Each figure ID of the testset must be included in the runfile exactly once</li>
  <li>You should not include special characters in the description.</li>
</ul>

<h3 id=""acknowledgements"">Acknowledgements</h3>

<p><a href=""http://www.ncbi.nlm.nih.gov/pmc/"" target=""_blank""> PubMed Central </a></p>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>
<p>Evaluation is based on <strong>BLEU scores</strong>, using the following methodology and parameters:</p>

<ul>
  <li>
    <p>The default implementation of the Python <a href=""http://nltk.org/"" target=""_blank""> NLTK </a> (v3.2.2) (Natural Language ToolKit) BLEU scoring method is used. It is documented here and based on the original article describing the <a href=""http://www.aclweb.org/anthology/P02-1040.pdf"" target=""_blank""> BLEU evaluation method </a></p>
  </li>
  <li>
    <p>A Python (3.6) script loads the candidate run file, as well as the ground truth (GT) file, and processes each candidate-GT caption pair</p>
  </li>
  <li>
    <p>Each caption is pre-processed in the following way:</p>

    <ul>
      <li>
        <p>The caption is converted to lower-case</p>
      </li>
      <li>
        <p>All punctuation is removed an the caption is <a href=""http://www.nltk.org/_modules/nltk/tokenize/punkt.html#PunktLanguageVars.word_tokenize"" target=""_blank""> tokenized </a> into its individual words</p>
      </li>
      <li>
        <p>Stopwords are removed using NLTK’s “english” stopword list</p>
      </li>
      <li>
        <p>Stemming is applied using NLTK’s <a href=""http://www.nltk.org/_modules/nltk/stem/snowball.html"" target=""_blank""> Snowball stemmer </a></p>
      </li>
    </ul>
  </li>
  <li>
    <p>The BLEU score is then calculated. Note that the caption is always considered as a single sentence, even if it actually contains several sentences. No smoothing function is used.</p>
  </li>
  <li>
    <p>All BLEU scores are summed and averaged over the number of captions (10’000), giving the final score.</p>
  </li>
</ul>

<p><strong>NOTE</strong> : The source code of the evaluation tool is available <a href=""http://fast.hevs.ch/temp/ImageCLEF-CaptionPrediction-Evaluation.zip"" target=""_blank""> here </a>. It <strong>must</strong> be executed using Python <strong>3.6.x</strong>, on a system where the NLTK (<strong>v3.2.2</strong>) Python library is installed. The script should be run like this:</p>

<p><code class=""highlighter-rouge"">
/path/to/python3.6 evaluate-bleu.py /path/to/candidate/file /path/to/ground-truth/file 
</code></p>

<hr />
<p><em>The leaderboard will be visible from 01.05.2018 (official deadline) on. The submission system will remain open few more days. Results submitted after deadline will not be part of the official results.</em></p>

<hr />

<h3 id=""resources"">Resources</h3>

<h3 id=""contact-us"">Contact us</h3>

<ul>
  <li>Technical issues : <a href=""https://gitter.im/crowdAI/imageclef-2018-caption-prediction"" target=""_blank""> https://gitter.im/crowdAI/imageclef-2018-caption-prediction </a></li>
  <li>Discussion Forum : <a href=""https://www.crowdai.org/challenges/imageclef-2018-caption-caption-prediction/topics"" target=""_blank""> https://www.crowdai.org/challenges/imageclef-2018-caption-caption-prediction/topics </a></li>
</ul>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

<ul>
  <li>Sharada Prasanna Mohanty: sharada.mohanty@epfl.ch</li>
  <li>Alba Garcia Seco de Herrera: alba[DOT]garcia[AT]essex[DOT]ac[DOT]uk</li>
  <li>Henning Müller: henning[DOT]mueller[AT]hevs[DOT]ch</li>
  <li>Vincent Adrearczyk: vincent[DOT]andrearczyk[AT]hevs[DOT]ch</li>
  <li>Ivan Eggel: ivan[DOT]eggel[AT]hevs[DOT]ch</li>
</ul>

<h3 id=""more-information"">More information</h3>

<p>You can find additional information on the challenge here:
<a href=""http://imageclef.org/2018/caption"" target=""_blank""> http://imageclef.org/2018/caption </a></p>

<h3 id=""prizes"">Prizes</h3>

<p>ImageCLEF 2018 is an evaluation campaign that is being organized as part of the <a href=""http://clef2018.clef-initiative.eu/"" target=""_blank""> CLEF initiative </a> labs. The campaign offers several research tasks that welcome participation from teams around the world. The results of the campaign appear in the working notes proceedings, published by CEUR Workshop Proceedings (CEUR-WS.org). Selected contributions among the participants, will be invited for publication in the following year in the Springer Lecture Notes in Computer Science (LNCS) together with the annual lab overviews.</p>

<h3 id=""datasets-license"">Datasets License</h3>

"
219,"<p><em>Note: Do not forget to read the <strong>Rules</strong> section on this page. Pressing the red <strong>Participate button</strong> leads you to a page where you have to agree with those rules. You will not be able to submit any results before agreeing with the rules.</em></p>

<p><em>Note: Before trying to submit results, read the <strong>Submission instructions</strong> section on this page.</em></p>

<h1 id=""challenge-description"">Challenge description</h1>

<p>Building websites requires a very specific set of skills. Currently, the two main ways to achieve this is either by using a visual website builder or by programming. Both approaches have a steep learning curve. Enabling people to create websites by drawing them on a whiteboard or on a piece of paper would make the webpage building process more accessible.</p>

<p>A first step in capturing the intent expressed by a user through a wireframe is to correctly detect a set of atomic user interface elements (UI) in their drawings. The bounding boxes and labels resulted from this detection step can then be used to accurately generate a website layout using various heuristics.</p>

<p>In this context, the detection and recognition of hand drawn website UIs task addresses the problem of automatically recognizing the hand drawn objects representing website UIs, which are further used to be translated automatically into website code.</p>

<p>Given a set of images of hand drawn UIs, participants are required to develop machine learning techniques that are able to predict the exact position and type of UI elements.</p>

<h1 id=""data"">Data</h1>

<hr />
<p><em>As soon as the data is released it will be available under the “Resources” tab.</em></p>

<hr />

<p>The provided data set consists of 2,950 hand drawn images inspired from mobile application screenshots and actual web pages containing about 1,000 different templates. Each image comes with the manual labeling of the positions of the bounding boxes corresponding to each UI element and its type. To avoid any ambiguity, a predefined shape dictionary with 21 classes is used, <em>e.g.</em>, paragraph, label, header. The development set contains 2,363 images while the test set contains 587 images.</p>

<h2 id=""classes"">Classes</h2>

<p>The 21 classes are the following:</p>

<div class=""language-py highlighter-rouge""><div class=""highlight""><pre class=""highlight""><code><span class=""p"">[</span>
    <span class=""s"">'paragraph'</span><span class=""p"">,</span>
    <span class=""s"">'dropdown'</span><span class=""p"">,</span>
    <span class=""s"">'checkbox'</span><span class=""p"">,</span>
    <span class=""s"">'radiobutton'</span><span class=""p"">,</span>
    <span class=""s"">'rating'</span><span class=""p"">,</span>
    <span class=""s"">'toggle'</span><span class=""p"">,</span>
    <span class=""s"">'textarea'</span><span class=""p"">,</span>
    <span class=""s"">'datepicker'</span><span class=""p"">,</span>
    <span class=""s"">'stepperinput'</span><span class=""p"">,</span>
    <span class=""s"">'slider'</span><span class=""p"">,</span>
    <span class=""s"">'video'</span><span class=""p"">,</span>
    <span class=""s"">'label'</span><span class=""p"">,</span>
    <span class=""s"">'table'</span><span class=""p"">,</span>
    <span class=""s"">'list'</span><span class=""p"">,</span>
    <span class=""s"">'header'</span><span class=""p"">,</span>
    <span class=""s"">'button'</span><span class=""p"">,</span>
    <span class=""s"">'image'</span><span class=""p"">,</span>
    <span class=""s"">'linebreak'</span><span class=""p"">,</span>
    <span class=""s"">'container'</span><span class=""p"">,</span>
    <span class=""s"">'link'</span><span class=""p"">,</span>
    <span class=""s"">'textinput'</span>
<span class=""p"">}</span>
</code></pre></div></div>

<p>The following image was given as a guideline for the people who drew the wireframes:</p>

<p><img src=""https://i.imgur.com/OTe2Xzt.jpg"" alt=""Example picture"" /></p>

<h2 id=""example"">Example</h2>

<p><img src=""https://i.imgur.com/xcP1nF7.jpg"" alt=""Example picture"" />
<em>Image 1c3e1163fa864f9c.jpg from the train set</em></p>

<p>The annotation format for the development set is a single CSV file with one row per image, following the format below:</p>

<p><code class=""highlighter-rouge"">
[image_ID];[class1] [[confidence1,1]:][width1,1]x[height1,1]+[xmin1,1]+[ymin1,1],[[confidence1,2]:][width1,2]x[height1,2]+[xmin1,2]+[ymin1,2],...;[class2]...;[classN];
</code></p>

<p>Here are the annotations while in development:</p>

<table>
  <thead>
    <tr>
      <th>Class</th>
      <th style=""text-align: center"">Score</th>
      <th style=""text-align: center"">Width</th>
      <th style=""text-align: center"">Height</th>
      <th style=""text-align: center"">xMin</th>
      <th style=""text-align: center"">yMin</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>container</td>
      <td style=""text-align: center"">1</td>
      <td style=""text-align: center"">614</td>
      <td style=""text-align: center"">925</td>
      <td style=""text-align: center"">179</td>
      <td style=""text-align: center"">95</td>
    </tr>
    <tr>
      <td>image</td>
      <td style=""text-align: center"">1</td>
      <td style=""text-align: center"">259</td>
      <td style=""text-align: center"">135</td>
      <td style=""text-align: center"">379</td>
      <td style=""text-align: center"">305</td>
    </tr>
    <tr>
      <td>paragraph</td>
      <td style=""text-align: center"">1</td>
      <td style=""text-align: center"">190</td>
      <td style=""text-align: center"">135</td>
      <td style=""text-align: center"">410</td>
      <td style=""text-align: center"">474</td>
    </tr>
    <tr>
      <td>container</td>
      <td style=""text-align: center"">1</td>
      <td style=""text-align: center"">549</td>
      <td style=""text-align: center"">229</td>
      <td style=""text-align: center"">219</td>
      <td style=""text-align: center"">689</td>
    </tr>
    <tr>
      <td>button</td>
      <td style=""text-align: center"">1</td>
      <td style=""text-align: center"">99</td>
      <td style=""text-align: center"">60</td>
      <td style=""text-align: center"">265</td>
      <td style=""text-align: center"">745</td>
    </tr>
    <tr>
      <td>button</td>
      <td style=""text-align: center"">1</td>
      <td style=""text-align: center"">85</td>
      <td style=""text-align: center"">50</td>
      <td style=""text-align: center"">434</td>
      <td style=""text-align: center"">810</td>
    </tr>
    <tr>
      <td>button</td>
      <td style=""text-align: center"">1</td>
      <td style=""text-align: center"">89</td>
      <td style=""text-align: center"">50</td>
      <td style=""text-align: center"">614</td>
      <td style=""text-align: center"">739</td>
    </tr>
  </tbody>
</table>

<p>And here is the corresponding row from the CSV file:</p>

<p><code class=""highlighter-rouge"">
1c3e1163fa864f9c.jpg;paragraph 1:190x135+410+474;button 1:99x60+265+745,1:85x50+434+819,1:89x50+614+739;image 1:259x135+379+305;container 1:614x925+179+95,1:549x229+219+689;;;;;;;;
</code></p>

<h1 id=""submission-instructions"">Submission instructions</h1>

<hr />
<p><em>As soon as the submission is open, you will find a “Create Submission” button on this page (next to the tabs).</em></p>

<hr />
<p><em>Before being allowed to submit your results, you have to first press the red participate button, which leads you to a page where you have to accept the challenges rules.</em></p>

<hr />

<p>More information regarding the submission instructions will be released soon.</p>

<p>The expected release date for the test set is the 16 march.</p>

<h1 id=""evaluation-criteria"">Evaluation criteria</h1>

<p>The performance of the algorithms will be evaluated using the standard mean Average Precision over IoU .5, commonly used in object detection.</p>

<h1 id=""rules"">Rules</h1>

<p><em>Note: In order to participate in this challenge you have to sign an End User Agreement (EUA). You will find more information on the ‘Resources’ tab.</em></p>

<p>ImageCLEF lab is part of the Conference and Labs of the Evaluation Forum: CLEF 2020. CLEF 2020 consists of independent peer-reviewed workshops on a broad range of challenges in the fields of multilingual and multimodal information access evaluation, and a set of benchmarking activities carried in various labs designed to test different aspects of mono and cross-language Information retrieval systems. More details about the conference can be found <a href=""http://clef2020.clef-initiative.eu/"" target=""_blank"">here</a> .</p>

<p>Submitting a working note with the full description of the methods used in each run is mandatory. Any run that could not be reproduced thanks to its description in the working notes might be removed from the official publication of the results. Working notes are published within CEUR-WS proceedings, resulting in an assignment of an individual DOI (URN) and an indexing by many bibliography systems including DBLP. According to the CEUR-WS policies, a light review of the working notes will be conducted by ImageCLEF organizing committee to ensure quality. As an illustration, ImageCLEF 2019 working notes (task overviews and participant working notes) can be found within <a href=""http://ceur-ws.org/Vol-2380/"" target=""_blank"">CLEF 2019 CEUR-WS</a> proceedings.</p>

<h2 id=""important"">Important</h2>

<p>Participants of this challenge will automatically be registered at CLEF 2020. In order to be compliant with the CLEF registration requirements, please edit your profile by providing the following additional information:</p>

<ul>
  <li>
    <p>First name</p>
  </li>
  <li>
    <p>Last name</p>
  </li>
  <li>
    <p>Affiliation</p>
  </li>
  <li>
    <p>Address</p>
  </li>
  <li>
    <p>City</p>
  </li>
  <li>
    <p>Country</p>
  </li>
  <li>
    <p><em>Regarding the username, please choose a name that represents your team.</em></p>
  </li>
</ul>

<p><em>This information will not be publicly visible and will be exclusively used to contact you and to send the registration data to CLEF, which is the main organizer of all CLEF labs</em></p>

<h2 id=""participating-as-an-individual-non-affiliated-researcher"">Participating as an individual (non affiliated) researcher</h2>

<p>We welcome individual researchers, i.e. not affiliated to any institution, to participate. We kindly ask you to provide us with a motivation letter containing the following information:</p>

<ul>
  <li>
    <p>the presentation of your most relevant research activities related to the task/tasks</p>
  </li>
  <li>
    <p>your motivation for participating in the task/tasks and how you want to exploit the results</p>
  </li>
  <li>
    <p>a list of the most relevant 5 publications (if applicable)</p>
  </li>
  <li>
    <p>the link to your personal webpage</p>
  </li>
</ul>

<p>The motivation letter should be directly concatenated to the End User Agreement document or sent as a PDF file to bionescu at imag dot pub dot ro. The request will be analyzed by the ImageCLEF organizing committee. We reserve the right to refuse any applicants whose experience in the field is too narrow, and would therefore most likely prevent them from being able to finish the task/tasks.</p>

<h1 id=""citations"">Citations</h1>

<p>Information will be posted after the challenge ends.</p>

<h1 id=""prizes"">Prizes</h1>

<h2 id=""publication"">Publication</h2>

<p>ImageCLEF 2020 is an evaluation campaign that is being organized as part of the <a href=""http://clef2020.clef-initiative.eu/"" target=""_blank"">CLEF initiative</a> labs. The campaign offers several research tasks that welcome participation from teams around the world. The results of the campaign appear in the working notes proceedings, published by CEUR Workshop Proceedings (CEUR-WS.org). Selected contributions among the participants, will be invited for publication in the following year in the Springer Lecture Notes in Computer Science (LNCS) together with the annual lab overviews.</p>

<h1 id=""resources"">Resources</h1>

<h2 id=""contact-us"">Contact us</h2>

<p><em>Discussion Forum</em></p>

<ul>
  <li>You can ask questions related to this challenge on the Discussion Forum. Before asking a new question please make sure that question has not been asked before.</li>
  <li>Click on Discussion tab above or direct link: <a href=""https://discourse.aicrowd.com/c/imageclef-2020-drawnui"" target=""_blank"">https://discourse.aicrowd.com/c/imageclef-2020-drawnui</a></li>
</ul>

<p><em>Alternative channels</em></p>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

<ul>
  <li>paul[dot]brie[at]teleporthq[dot]io</li>
  <li>dimitri[dot]fichou[at]teleporthq[dot]io</li>
  <li>stefan[dot]liviu[dot]daniel[at]gmail[dot]com</li>
  <li>cmihaigabriel[at]gmail[dot]com</li>
  <li>dogariu[dot]mihai8[at]gmail[dot]com</li>
  <li>bogdanlapi[at]gmail[dot]com</li>
</ul>

<h2 id=""more-information"">More information</h2>

<p>You can find additional information on the challenge here:
<a href=""https://www.imageclef.org/2020/drawnui"" target=""_blank"">https://www.imageclef.org/2020/drawnui</a></p>

"
221,"<p><em>Note: Do not forget to read the <strong>Rules</strong> section on this page. Pressing the red <strong>Participate button</strong> leads you to a page where you have to agree with those rules. You will not be able to submit any results before agreeing with the rules.</em></p>

<p><em>Note: Before trying to submit results, read the <strong>Submission instructions</strong> section on this page.</em></p>

<h1 id=""challenge-description"">Challenge description</h1>

<p>Interpreting and summarizing the insights gained from medical images such as radiology output is a time-consuming task that involves highly trained experts and often represents a bottleneck in clinical diagnosis pipelines.</p>

<p>Consequently, there is a considerable need for automatic methods that can approximate this mapping from visual information to condensed textual descriptions. The more image characteristics are known, the more structured are the radiology scans and hence, the more efficient are the radiologists regarding interpretation. We work on the basis of a large-scale collection of figures from open access biomedical journal articles (PubMed Central). All images in the training data are accompanied by UMLS concepts extracted from the original image caption.</p>

<p>Lessons learned:</p>

<ul>
  <li>
    <p>In the first and second editions of this task, held at ImageCLEF 2017 and ImageCLEF 2018, participants noted a broad variety of content and situation among training images. In 2019, the training data was reduced solely to radiology images</p>
  </li>
  <li>
    <p>The focus of the ImageCLEF 2020 is on radiology images, with additional imaging modality information, for pre-processing purposes and multi-modal approaches</p>
  </li>
  <li>
    <p>A large number of concepts were used in previous years. This year, the captions are first processed before concept extraction, hence leading to a reduced number of concepts.</p>
  </li>
  <li>
    <p>Concepts with less occurrence will be removed
As uncertainty regarding additional source was noted, we will clearly separate systems using exclusively the official training data from those that incorporate additional sources of evidence</p>
  </li>
</ul>

<h1 id=""data"">Data</h1>

<hr />
<p><em>As soon as the data is released it will be available under the “Resources” tab.</em></p>

<hr />

<ul>
  <li>Development data will be released on 31.01.2020</li>
  <li>Test data will be released on 27.03.2020</li>
</ul>

<h1 id=""submission-instructions"">Submission instructions</h1>

<hr />
<p><em>As soon as the submission is open, you will find a “Create Submission” button on this page (next to the tabs).</em></p>

<hr />
<p><em>Before being allowed to submit your results, you have to first press the red participate button, which leads you to a page where you have to accept the rules of the challenge.</em></p>

<hr />

<p>Please note that each group is allowed a maximum of 10 runs per subtask.</p>

<p>For the submission of the concept detection task we expect the following format:
- ROCO_CLEF_41341 C0033785;C0035561
- ROCO_CLEF_07563 C0043299;C1306645;C1548003;C1962945.</p>

<hr />

<p>You need to respect the following constraints:</p>

<ul>
  <li>
    <p>The separator between the figure ID and the concepts has to be a tabular whitespace</p>
  </li>
  <li>
    <p>The separator between the UMLS concepts has to be a semicolon (;)</p>
  </li>
  <li>
    <p>Each figure ID of the test set must be included in the submitted file exactly once (even if there are not concepts)</p>
  </li>
  <li>
    <p>The same concept cannot be specified more than once for a given figure ID</p>
  </li>
  <li>
    <p>The maximum number of concepts per image is 100</p>
  </li>
</ul>

<h1 id=""evaluation-criteria"">Evaluation criteria</h1>

<p>Evaluation is conducted in terms of F1 scores between system predicted and ground truth concepts, using the following methodology and parameters:</p>

<ul>
  <li>
    <p>The default implementation of the Python scikit-learn (v0.17.1-2) F1 scoring method is used. It is documented here.</p>
  </li>
  <li>
    <p>A Python (3.x) script loads the candidate run file, as well as the ground truth (GT) file, and processes each candidate-GT concept sets</p>
  </li>
  <li>
    <p>For each candidate-GT concept set, the y_pred and y_true arrays are generated. They are binary arrays indicating for each concept contained in both candidate and GT set if it is present (1) or not (0).</p>
  </li>
  <li>
    <p>The F1 score is then calculated. The default ‘binary’ averaging method is used.</p>
  </li>
  <li>
    <p>All F1 scores are summed and averaged over the number of elements in the test set (10’000), giving the final score.</p>
  </li>
</ul>

<p>The ground truth for the test set was generated based on the UMLS Full Release 2019AB.</p>

<p>NOTE: The source code of the evaluation tool is available here. It must be executed using Python 3.x, on a system where the scikit-learn (&gt;= v0.17.1-2) Python library is installed. The script should be run like this:</p>

<p>/path/to/python3 evaluate-f1.py /path/to/candidate/file /path/to/ground-truth/file</p>

<h1 id=""rules"">Rules</h1>

<p><em>Note: In order to participate in this challenge you have to sign an End User Agreement (EUA). You will find more information on the ‘Resources’ tab.</em></p>

<p>ImageCLEF lab is part of the Conference and Labs of the Evaluation Forum: CLEF 2020. CLEF 2020 consists of independent peer-reviewed workshops on a broad range of challenges in the fields of multilingual and multimodal information access evaluation, and a set of benchmarking activities carried in various labs designed to test different aspects of mono and cross-language Information retrieval systems. More details about the conference can be found <a href=""http://clef2020.clef-initiative.eu/"" target=""_blank"">here</a> .</p>

<p>Submitting a working note with the full description of the methods used in each run is mandatory. Any run that could not be reproduced thanks to its description in the working notes might be removed from the official publication of the results. Working notes are published within CEUR-WS proceedings, resulting in an assignment of an individual DOI (URN) and an indexing by many bibliography systems including DBLP. According to the CEUR-WS policies, a light review of the working notes will be conducted by ImageCLEF organizing committee to ensure quality. As an illustration, ImageCLEF 2019 working notes (task overviews and participant working notes) can be found within <a href=""http://ceur-ws.org/Vol-2380/"" target=""_blank"">CLEF 2019 CEUR-WS</a> proceedings.</p>

<h2 id=""important"">Important</h2>

<p>Participants of this challenge will automatically be registered at CLEF 2020. In order to be compliant with the CLEF registration requirements, please edit your profile by providing the following additional information:</p>

<ul>
  <li>
    <p>First name</p>
  </li>
  <li>
    <p>Last name</p>
  </li>
  <li>
    <p>Affiliation</p>
  </li>
  <li>
    <p>Address</p>
  </li>
  <li>
    <p>City</p>
  </li>
  <li>
    <p>Country</p>
  </li>
  <li>
    <p><em>Regarding the username, please choose a name that represents your team.</em></p>
  </li>
</ul>

<p><em>This information will not be publicly visible and will be exclusively used to contact you and to send the registration data to CLEF, which is the main organizer of all CLEF labs</em></p>

<h2 id=""participating-as-an-individual-non-affiliated-researcher"">Participating as an individual (non affiliated) researcher</h2>

<p>We welcome individual researchers, i.e. not affiliated to any institution, to participate. We kindly ask you to provide us with a motivation letter containing the following information:</p>

<ul>
  <li>
    <p>the presentation of your most relevant research activities related to the task/tasks</p>
  </li>
  <li>
    <p>your motivation for participating in the task/tasks and how you want to exploit the results</p>
  </li>
  <li>
    <p>a list of the most relevant 5 publications (if applicable)</p>
  </li>
  <li>
    <p>the link to your personal webpage</p>
  </li>
</ul>

<p>The motivation letter should be directly concatenated to the End User Agreement document or sent as a PDF file to bionescu at imag dot pub dot ro. The request will be analyzed by the ImageCLEF organizing committee. We reserve the right to refuse any applicants whose experience in the field is too narrow, and would therefore most likely prevent them from being able to finish the task/tasks.</p>

<h1 id=""citations"">Citations</h1>

<p>Information will be posted after the challenge ends.</p>

<h1 id=""prizes"">Prizes</h1>

<h2 id=""publication"">Publication</h2>

<p>ImageCLEF 2020 is an evaluation campaign that is being organized as part of the <a href=""http://clef2020.clef-initiative.eu/"" target=""_blank"">CLEF initiative</a> labs. The campaign offers several research tasks that welcome participation from teams around the world. The results of the campaign appear in the working notes proceedings, published by CEUR Workshop Proceedings (CEUR-WS.org). Selected contributions among the participants, will be invited for publication in the following year in the Springer Lecture Notes in Computer Science (LNCS) together with the annual lab overviews.</p>

<h1 id=""resources"">Resources</h1>

<h2 id=""contact-us"">Contact us</h2>

<p><em>Discussion Forum</em></p>

<ul>
  <li>You can ask questions related to this challenge on the Discussion Forum. Before asking a new question please make sure that question has not been asked before.</li>
  <li>Click on Discussion tab above or direct link: <a href=""https://discourse.aicrowd.com/c/imageclef-2020-caption-concept-detection"" target=""_blank"">https://discourse.aicrowd.com/c/imageclef-2020-caption-concept-detection</a></li>
</ul>

<p><em>Alternative channels</em></p>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

<ul>
  <li>obioma[dot]pelka[at]fh-dortmund[dot]de</li>
  <li>christoph[dot]friedrich[at]fh-dortmund[dot]de</li>
  <li>alba[dot]garcia[at]essex[dot]ac[dot]uk</li>
  <li>henning[dot]mueller[at]hevs[dot]ch</li>
</ul>

<h2 id=""more-information"">More information</h2>

<p>You can find additional information on the challenge here:
<a href=""https://www.imageclef.org/2020/medical/caption"" target=""_blank"">https://www.imageclef.org/2020/medical/caption</a></p>

"
160,"<hr />
<p><strong>Important note:</strong></p>

<p><em>The <strong>ImageCLEF Tuberculosis - TBT classification challenge has officially ended</strong> and we would like to thank everybody for their participation. You can find the official results at <a href=""http://imageclef.org/2018/tuberculosis"">http://imageclef.org/2018/tuberculosis </a>.</em></p>

<p><em>Post-challenge submissions and the leaderboard will remain enabled  for a few weeks so you will still be able to submit result files and have them continuously evaluated during a limited period. 
Please consider that in order to see the version of the leaderboard with the post-challenge submissions integrated, you have to turn on the switch <strong>Show post-challenge submission</strong> right below the leaderboard.</em></p>

<p><em>At the same time we’d like to encourage you to submit a <a href=""http://clef2018.clef-initiative.eu/index.php?page=Pages/InstructionsforCLEF2018WorkingNotes.html"">CLEF Working notes paper</a> until the end of May.</em></p>

<p><em>Please also note that participants registering from now on will not be
automatically registered with CLEF anymore.</em></p>

<hr />

<p><em>Note: ImageCLEF Tuberculosis 2018 is divided into 3 subtasks (challenges). This challenge is about <strong>TBT (tuberculosis type) Classfication</strong>. For information on the <strong>MDR (multi-drug-resistance) Detection</strong> challenge click <a href=""/challenges/imageclef-2018-tuberculosis-mdr-detection"" target=""_blank""> here </a>. For information on the <strong>Severity Scoring</strong> challenge click <a href=""/challenges/imageclef-2018-tuberculosis-severity-scoring"" target=""_blank""> here </a>. All of these challenges share the same dataset, so registering for one of these challenges will automatically give you access to the other ones.</em></p>

<p><em>Note: Do not forget to read the <strong>Rules</strong> section on this page</em></p>

<h3 id=""motivation"">Motivation</h3>

<p>About 130 years after the discovery of Mycobacterium tuberculosis, the disease remains a persistent threat and a leading cause of death worldwide.</p>

<p>The greatest disaster that can happen to a patient with tuberculosis (TB) is that the organisms become resistant to two or more of the standard drugs. In contrast to drug sensitive (DS) tuberculosis, its multi-drug resistant (MDR) form is much more difficult and expensive to recover from. Thus, early detection of the drug resistance (DR) status is of great importance for effective treatment. The most commonly used methods of DR detection are either expensive or take too much time (up to several month). Therefore there is a need for quick and at the same time cheap methods of DR detection. One of the possible approaches for this task is based on Computed Tomography (CT) image analysis. Another challenging task is automatic detection of TB types (TBT) using CT volumes.</p>

<p><em>Differences compared to 2017</em>: Scoring the severity of TB cases based on chest CT images is another task compared to both tuberculosis-related subtasks considered in 2017. There are no direct links between them. Note only that original CT image datasets used in 2017 and in 2018 may slightly overlap.</p>

<h3 id=""challenge-description"">Challenge description</h3>

<p>The goal of this subtask is to automatically categorize each TB case into one of the following five types: Infiltrative, Focal, Tuberculoma, Miliary, Fibro-cavernous.</p>

<h3 id=""data"">Data</h3>

<p>The dataset used in this task includes chest CT scans of TB patients along with the TB type. Some patients include more than one scan. All scans belonging to the same patient present the same TB type.</p>

<table>
  <thead>
    <tr>
      <th>Num. Patients (Num. CTs)</th>
      <th style=""text-align: center"">Train</th>
      <th style=""text-align: center"">Test</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Type 1</td>
      <td style=""text-align: center"">228 (376)</td>
      <td style=""text-align: center"">89 (176)</td>
    </tr>
    <tr>
      <td>Type 2</td>
      <td style=""text-align: center"">210 (273)</td>
      <td style=""text-align: center"">80 (115)</td>
    </tr>
    <tr>
      <td>Type 3</td>
      <td style=""text-align: center"">100 (154)</td>
      <td style=""text-align: center"">60 (86)</td>
    </tr>
    <tr>
      <td>Type 4</td>
      <td style=""text-align: center"">79 (106)</td>
      <td style=""text-align: center"">50 (71)</td>
    </tr>
    <tr>
      <td>Type 5</td>
      <td style=""text-align: center"">60 (99)</td>
      <td style=""text-align: center"">38 (57)</td>
    </tr>
    <tr>
      <td><strong>Total patients (CTs)</strong></td>
      <td style=""text-align: center""><strong>677 (1008)</strong></td>
      <td style=""text-align: center""><strong>317 (505)</strong></td>
    </tr>
  </tbody>
</table>

<p>We provide 3D CT images with slice size of 512*512 pixels and number of slices varying from about 50 to 400. All the CT images are stored in NIFTI file format with .nii.gz file extension (g-zipped .nii files). This file format stores raw voxel intensities in Hounsfield units (HU) as well the corresponding image metadata such as image dimensions, voxel size in physical units, slice thickness, etc. A freely-available tool called <a href=""https://www.creatis.insa-lyon.fr/rio/vv"" target=""_blank""> “VV” </a> can be used for viewing image files. Currently, there are various tools available for reading and writing NIFTI files. Among them there are <a href=""https://de.mathworks.com/matlabcentral/fileexchange/8797-tools-for-nifti-and-analyze-image/content/load_nii.m"" target=""_blank""> load_nii </a> and <a href=""https://de.mathworks.com/matlabcentral/fileexchange/8797-tools-for-nifti-and-analyze-image/content/save_nii.m"" target=""_blank""> save_nii </a> functions for Matlab and <a href=""http://niftilib.sourceforge.net/"" target=""_blank""> Niftilib </a> library for C, Java, Matlab and Python.</p>

<p>We also provide automatic extracted masks of the lungs. This material can be downloaded together with the patients CT images. The details of this segmentation can be found <a href=""http://publications.hevs.ch/index.php/publications/show/1871"" target=""_blank""> here </a>.
In case the participants use these masks in their experiments, please refer to the section “Citations” to find the appropriate citation for this lung segmentation technique.</p>

<h3 id=""submission-instructions"">Submission instructions</h3>

<hr />
<p><em>As soon as the submission is open, you will find a “Create Submission” button on this page (just next to the tabs)</em></p>

<hr />

<p>Submit a plain text file named with the prefix <strong>TBT</strong> (e.g. TBTfree-text.txt) with the following format:</p>

<p>&lt;Patient-ID&gt;,&lt;TB-Type&gt;</p>

<p>e.g.:</p>

<div class=""highlighter-rouge""><div class=""highlight""><pre class=""highlight""><code>TBT_TST_501,1
TBT_TST_502,3
TBT_TST_503,5
TBT_TST_504,4
TBT_TST_505,2
</code></pre></div></div>

<p><strong>Please use the following Codes for the TB types:</strong></p>

<ul>
  <li>1 for Infiltrative</li>
  <li>2 for Focal</li>
  <li>3 for Tuberculoma</li>
  <li>4 for Miliary</li>
  <li>5 for Fibro-cavernous</li>
</ul>

<p><strong>You need to respect the following constraints:</strong></p>

<ul>
  <li>Patient-IDs are obtained as follows:
    <ul>
      <li>Image-IDs: {TBT_TST_001_01, TBT_TST_001_02, TBT_TST_001_03} –&gt; Patient-ID: TBT_TST_001</li>
      <li>Image-IDs: {TBT_TST_002_01} –&gt; Patient-ID: TBT_TST_002</li>
    </ul>
  </li>
  <li>Patient-IDs must be part of the predefined Patient-IDs</li>
  <li>All patient-IDs must be present in the runfiles</li>
  <li>Only use the defined codes for the various TB types</li>
  <li>Only use one TB type per patient</li>
</ul>

<h3 id=""citations"">Citations</h3>

<p>Information will be posted after the challenge ends.</p>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>
<p>The results will be evaluated using unweighted Cohen’s Kappa (sample <a href=""http://de.mathworks.com/matlabcentral/fileexchange/15365-cohen-s-kappa/content/kappa.m"" target=""_blank""> Matlab code </a>).</p>

<p>The leaderboard will be visible from the 01.05.2018. However, the submission system will remain open few more days.</p>

<h3 id=""resources"">Resources</h3>

<h3 id=""contact-us"">Contact us</h3>

<ul>
  <li>Technical issues : <a href=""https://gitter.im/crowdAI/imageclef-2018-tuberculosis-tbt-classification"" target=""_blank""> https://gitter.im/crowdAI/imageclef-2018-tuberculosis-tbt-classification </a></li>
  <li>Discussion Forum : <a href=""https://www.crowdai.org/challenges/imageclef-2018-tuberculosis-tbt-classification/topics"" target=""_blank""> https://www.crowdai.org/challenges/imageclef-2018-tuberculosis-tbt-classification/topics </a></li>
</ul>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

<ul>
  <li>Sharada Prasanna Mohanty: sharada.mohanty@epfl.ch</li>
  <li>Yashin Dicente Cid: yashin[DOT]dicente[AT]hevs[DOT]ch</li>
  <li>Henning Müller: henning[DOT]mueller[AT]hevs[DOT]ch</li>
  <li>Ivan Eggel: ivan[DOT]eggel[AT]hevs[DOT]ch</li>
</ul>

<h3 id=""more-information"">More information</h3>

<p>You can find additional information on the challenge here:
<a href=""http://imageclef.org/2018/tuberculosis"" target=""_blank""> http://imageclef.org/2018/tuberculosis </a></p>

<h3 id=""prizes"">Prizes</h3>

<p>ImageCLEF 2018 is an evaluation campaign that is being organized as part of the <a href=""http://clef2018.clef-initiative.eu/"" target=""_blank""> CLEF initiative </a> labs. The campaign offers several research tasks that welcome participation from teams around the world. The results of the campaign appear in the working notes proceedings, published by CEUR Workshop Proceedings (CEUR-WS.org). Selected contributions among the participants, will be invited for publication in the following year in the Springer Lecture Notes in Computer Science (LNCS) together with the annual lab overviews.</p>

<h3 id=""datasets-license"">Datasets License</h3>

"
8,"<blockquote class=""update"">
  <p>TL;DR : Make your first submission to this exciting new Reinforcement Learning challenge using : <a href=""https://github.com/Unity-Technologies/obstacle-tower-challenge"">https://github.com/Unity-Technologies/obstacle-tower-challenge</a></p>
</blockquote>

<blockquote class=""update"">
  <p>Only the final submission made by each team will be accepted for evaluation at the end of each round. Please be sure that your final submission before the end of each round reflects your best work, and be sure to re-submit that work if necessary.</p>
</blockquote>

<blockquote class=""update"">
  <p><strong>The timeline of this challenge has been extended</strong>. More details here : <a href=""https://discourse.aicrowd.com/t/announcement-unity-obstacle-tower-challenge-extension/885/2"">https://discourse.aicrowd.com/t/announcement-unity-obstacle-tower-challenge-extension/885/2</a></p>
</blockquote>

<p><img src=""https://raw.githubusercontent.com/Unity-Technologies/obstacle-tower-env/master/banner.png"" alt=""alt text"" title=""Obstacle Tower"" class=""md-image"" /></p>

<p>The Obstacle Tower is a procedurally generated environment consisting of an endless number of floors to be solved by a learning agent. It is designed to be a new benchmark for learning agents specifically in the areas of computer vision, locomotion skills, high-level planning, and generalization. The Obstacle Tower Challenge combines platforming-style gameplay with puzzles and planning problems, all in a tower with an endless number of floors for agents to learn to solve. Critically, the floors become progressively more difficult as the agent progresses. To learn more about the Obstacle Tower, read our full paper on the environment <a href=""https://arxiv.org/abs/1902.01378"">here</a>.</p>

<p><br /></p>
<iframe width=""560"" height=""315"" src=""https://www.youtube.com/embed/owKdLnCjy3o"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen=""""></iframe>

<p>Your mission in the Obstacle Tower Challenge is to create an agent that can successfully navigate the Obstacle Tower environments we have created.</p>

<h1 id=""dates"">Dates</h1>

<h2 id=""round-1"">Round 1</h2>

<ul>
  <li><strong>Start Date</strong>: 12:01 a.m. PST on Monday, February 11th, 2019;</li>
  <li><strong>End Date</strong>: ~~Friday, March 31st, 2019~~ <strong>11:59 p.m. PST Tuesday, April, 30th, 2019</strong></li>
</ul>

<p>Round 1 Winner Announcement: 12:01 a.m. PST on Wednesday, May 15th, 2019</p>

<h2 id=""round-2"">Round 2</h2>

<ul>
  <li><strong>Start Date</strong>: 12:01 a.m. PST on ~~Monday, April 15, 2019~~ <strong>Wednesday, May 15th, 2019</strong></li>
  <li><strong>Deadline</strong>: 11:59 p.m. PST ~~Friday, May 24th, 2019~~ <strong>Monday, July 15th, 2019</strong></li>
</ul>

<p>Round 2 Winner Announcement: 12:01 a.m. PST on Thursday, August 1st, 2019</p>

<h1 id=""teams"">Teams</h1>

<p>Up to a maximum of 4 people can enter the Obstacle Tower Challenge as a team. You must be at least 18 years of age and a resident of certain countries to enter the Challenge. For more eligibility criteria, please read the Official Rules (linked below).</p>

<h1 id=""entry"">Entry</h1>

<p>To create an Entry clone the Challenge Starter Kit and then develop code to create and train a model that is able to interact with the Obstacle Tower Environment (the agent). The goal is to create an agent that can advance further than everyone else’s. There are two Rounds, the first with a 25 level environment and the second with 100 levels!</p>

<h1 id=""evaluation-criteria"">Evaluation Criteria</h1>

<p>In each round, we will run each agent through the applicable Obstacle Tower environment and evaluate how the agent does across 5 randomly generated seeds and then the entries will be ranked, from most to least, in accordance to the average of the number of levels reached in each seed. Ties will be broken on the basis of the average number of doors and keys collected by the agent across the aforementioned 5 seeds. In order to move onto Round 2, an Agent must be able to reach an average score of 5 in Round 1. If fewer than 20 are able to do so in Round 1, the average level requirement will be 4.</p>

<h1 id=""rules"">Rules</h1>

<p>For the full requirements, please carefully read the Official Rules, available <a href=""https://gitlab.aicrowd.com/unity/obstacle-tower-challenge-resources/blob/master/Rules.md"">here</a>.</p>

<h1 id=""prizes"">Prizes</h1>

<p>Cash and Travel Credit Prizes provided by Unity and Google Cloud Platform (GCP) Credits sponsored by Google! (Prizes are per team)</p>

<h2 id=""round-1-1"">Round 1</h2>

<p>Up to the 50 highest ranking teams will receive <strong>$1,100 USD worth of GCP credits per Team</strong>. (Round 1 Average Retail Value (ARV) per Team Prize is $1,100 USD and for all Round 1 prizes worth 55,000 USD)</p>

<h2 id=""round-2-1"">Round 2</h2>

<p>First Place (ARV $19,500)</p>

<ul>
  <li><strong>Cash Prize of $12,000 USD</strong></li>
  <li>Travel Credit* of up to $2,500</li>
  <li>GCP credits worth $5,000 USD in value</li>
</ul>

<p>Second Place (ARV $15,500)</p>

<ul>
  <li><strong>Cash Prize of $8,000 USD</strong></li>
  <li>Travel Credit* of up to $2,500</li>
  <li>GCP credits worth $5,000 USD in value</li>
</ul>

<p>Third Place (ARV $12,500)</p>

<ul>
  <li><strong>Cash Prize of $5,000 USD</strong></li>
  <li>Travel Credit* of up to $2,500</li>
  <li>GCP credits worth $5,000 USD in value</li>
</ul>

<p>* Travel Credits are a reimbursement for travel to a mutually (Unity and Team) agreed upon AI or Machine Learning related conference within 1 year of the announcement of winners to go towards economy class round trip airfares from your nearest airport to the city of conference and up to 3 days of accommodations (food is not included) and an opportunity to publish their Challenge Result</p>

<h1 id=""general-resources"">General Resources</h1>

<ul>
  <li>Obstacle Tower research paper: <a href=""https://arxiv.org/abs/1902.01378""> https://arxiv.org/abs/1902.01378 </a></li>
  <li>Environment GitHub page: <a href=""https://github.com/Unity-Technologies/obstacle-tower-env""> https://github.com/Unity-Technologies/obstacle-tower-env </a></li>
  <li>Please visit the starter-kit page first for our comprehensive instruction to start this competition: <a href=""https://github.com/Unity-Technologies/obstacle-tower-challenge""> https://github.com/Unity-Technologies/obstacle-tower-challenge </a></li>
  <li>Learn how to train an Agent using Google Cloud Platform: 
<a href=""https://github.com/Unity-Technologies/obstacle-tower-env/tree/master/examples/gcp_training.md""> https://github.com/Unity-Technologies/obstacle-tower-env/tree/master/examples/gcp_training.md </a></li>
</ul>

<h1 id=""contact-us"">Contact Us</h1>

<ul>
  <li>Discussion Forum : <a href=""https://discourse.aicrowd.com/c/unity-obstacle-tower-challenge""> https://discourse.aicrowd.com/c/unity-obstacle-tower-challenge </a></li>
  <li>Gitter Channel : <a href=""https://gitter.im/AIcrowd-HQ/unity-obstacle-tower-challenge"">https://gitter.im/AIcrowd-HQ/unity-obstacle-tower-challenge</a></li>
</ul>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at:</p>

<ul>
  <li>mohanty@aicrowd.com</li>
  <li>OTC@unity3d.com</li>
</ul>
"
182,"<blockquote>
  <p><strong>UPDATE: Submissions Instructions for ROUND 2</strong> are available at : <a href=""https://github.com/crowdAI/mapping-challenge-round2-starter-kit"">https://github.com/crowdAI/mapping-challenge-round2-starter-kit</a></p>
</blockquote>

<blockquote>
  <p><strong>UPDATE: The prizes for this challenge have been updated and can be found on the bottom of this page.</strong></p>
</blockquote>

<p>We are in a period of increasing humanitarian crises, both in scale and number. Natural disasters continue to increase in frequency and impact, while long-term and reignited conflicts affect people in many parts of the world. <strong>Often, accurate maps either do not exist or are outdated by disaster or conflict.</strong></p>

<p><a href=""http://www.hi-us.org/inclusion"">Humanity &amp; Inclusion</a> is an aid organization working in some 60 countries, alongside people with disabilities and vulnerable populations. Our emergency sector responds quickly and effectively to natural and civil disasters.</p>

<ul>
  <li>
    <p>Many parts of the world have not been mapped; especially the most marginalized parts, that is, those most vulnerable to natural hazards.
Obtaining maps of these potential crisis areas greatly improves the response of emergency preparedness actors.</p>
  </li>
  <li>
    <p>During a disaster it is extremely useful to be able to map the impassable sections of road for example, as well as the most damaged residential areas, the most vulnerable schools and public buildings, population movements, etc. The objective is to adapt as quickly as possible the intervention procedures to the evolution of the context generated by the crisis.</p>
  </li>
  <li>
    <p>In the first days following the occurrence of a disaster, it is essential to have as fine a mapping as possible of communication networks, housing areas and infrastructures, areas dedicated to agriculture, etc.</p>
  </li>
</ul>

<p>Today, when new maps are needed they are drawn by hand, often by volunteers who participate in so called Mapathons. They draw roads and buildings on satellite images, and contribute to Open StreetMap.</p>

<p>For instance, <a href=""http://www.hi-us.org/inclusion"">Humanity &amp; Inclusion</a> has been involved in organising numerous Mapathons to draw new maps for our clearance teams in Laos.</p>

<p>In this challenge we want to explore how Machine Learning can help pave the way for automated analysis of satellite imagery to generate relevant and real-time maps.</p>

<h2 id=""task"">Task</h2>

<p>Satellite imagery is readily available to humanitarian organisations, but translating images into maps is an intensive effort. Today maps are produced by <a href=""https://www.cartong.org"">specialized organisations</a> or in volunteer events such as <a href=""https://www.missingmaps.org"">mapathons</a>, where imagery is annotated with roads, buildings, farms, rivers etc.</p>

<p>Images are increasingly available from a variety of sources, including nano-satellites, drones and conventional high altitude satellites. The data is available: the task is to produce intervention-specific maps with the relevant features, in a short timeframe and from disparate data sources.</p>

<p><img src=""https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/mapping_challenge_1.jpg"" alt=""4up-image.jpg"" /></p>

<p>In this challenge you will be provided with a dataset of individual tiles of satellite imagery as RGB images, and their corresponding annotations of where an image is there a building. The goal is to train a model which given a new tile can annotate all buildings.</p>

<p>Also, in context of this challenge, to make the barrier to entry much lower, we tried to remove all the domain specific jargon of Remote Sensing and Satellite Imagery Analysis, and are presenting this as a problem of Object Detection and Object Segmentation in Images.</p>

<p>The idea being,  once we collectively demonstrate that an approach works really well on RGB images with just 3 channels of information, we can then work on extending it to multi-channel information from rich satellite imagery.</p>

<h1 id=""datasets"">Datasets</h1>

<p>You can download the datasets in the <a href=""https://www.crowdai.org/challenges/mapping-challenge/dataset_files"">Datasets Section</a>. You are provided with :</p>

<ul>
  <li>
    <p><code class=""highlighter-rouge"">train.tar.gz</code> : This is the Training Set of <strong>280741</strong> tiles (as 300x300 pixel RGB images) of satellite imagery, along with their corresponding annotations in <a href=""http://cocodataset.org/#home"">MS-COCO format</a></p>
  </li>
  <li>
    <p><code class=""highlighter-rouge"">val.tar.gz</code>: This is the suggested Validation Set of <strong>60317</strong> tiles (as 300x300 pixel RGB images) of satellite imagery, along with their corresponding annotations in <a href=""http://cocodataset.org/#home"">MS-COCO format</a></p>
  </li>
  <li>
    <p><code class=""highlighter-rouge"">test_images.tar.gz</code> : This is the Test Set for Round-1, where you are provided with <strong>60697</strong> files (as 300x300 pixel RGB images) and your are required to submit annotations for all these files.</p>
  </li>
</ul>

<p>For more details about the dataset, and submission procedures etc, please refer to the following notebooks :</p>

<ul>
  <li><a href=""https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb"">Dataset Utils</a>
    <ul>
      <li><a href=""https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb#Import-dependencies"">Import Dependencies</a></li>
      <li><a href=""https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb#Configuration-Variables"">Configuration Variables</a></li>
      <li><a href=""https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb#Parsing-the-annotations"">Parsing Annotations</a></li>
      <li><a href=""https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb#Collecting-and-Visualizing-Images"">Collecting and Visualizing Images</a></li>
      <li><a href=""https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb#Understanding-Annotations"">Understanding Annotations</a></li>
      <li><a href=""https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb#Visualizing-Annotations"">Visualizing Annotations</a></li>
      <li><a href=""https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb#Advanced"">Advanced</a>
        <ul>
          <li><a href=""https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb#1.-Convert-poly-segmentation-to-rle"">Convert poly segmentation to rle</a></li>
          <li><a href=""https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb#2.-Convert-segmentation-to-pixel-level-masks"">Convert segmentation to pixel level masks</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href=""https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Random%20Submission.ipynb"">Random Submission</a>
    <ul>
      <li><a href=""https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Random%20Submission.ipynb#Submission-Format"">Submission Format</a></li>
      <li><a href=""https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Random%20Submission.ipynb#Generate-a-random-segmentation"">Generating a Random Segmentation</a></li>
      <li><a href=""https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Random%20Submission.ipynb#Generate-a-random-annotation-object"">Generating a Random Annotation Object</a></li>
      <li><a href=""https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Random%20Submission.ipynb#Generate-a-results-object"">Generating a Random Results Object</a></li>
      <li><a href=""https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Random%20Submission.ipynb#Submit-to-crowdAI-for-grading"">Submit to crowdAI for grading</a></li>
    </ul>
  </li>
  <li>
    <p><a href=""https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Local%20Evaluation.ipynb"">Locally test the evaluation function</a></p>
  </li>
  <li>Train <a href=""https://arxiv.org/abs/1703.06870"">Mask-RCNN</a>
    <ul>
      <li><a href=""https://github.com/crowdAI/crowdai-mapping-challenge-mask-rcnn"">Installation</a></li>
      <li><a href=""https://github.com/crowdAI/crowdai-mapping-challenge-mask-rcnn/blob/master/Training.ipynb"">Training</a></li>
      <li><a href=""https://github.com/crowdAI/crowdai-mapping-challenge-mask-rcnn/blob/master/Prediction-and-Submission.ipynb"">Prediction &amp; Submission</a></li>
      <li><strong>NOTE</strong> : This is in a separate repository, and we have also now added the pretrained weights from the baseline submission to the <a href=""https://www.crowdai.org/challenges/mapping-challenge/dataset_files"">datasets page</a>.</li>
    </ul>
  </li>
</ul>

<h1 id=""evaluation-criteria"">Evaluation Criteria</h1>

<p>For for a known ground truth mask <script type=""math/tex"">A</script>, you propose a mask <script type=""math/tex"">B</script>, then we first compute <script type=""math/tex"">IoU</script> (Intersection Over Union) :</p>

<script type=""math/tex; mode=display"">IoU(A, B) = \frac{A \cap B}{ A \cup B}</script>

<p><script type=""math/tex"">IoU</script> measures the overall overlap between the true region and the proposed region.
Then we consider it a True detection, when there is atleast half an overlap, or when <script type=""math/tex"">IoU \geq 0.5</script></p>

<p>Then we can define the following parameters :</p>

<ul>
  <li>
    <p>Precision (<script type=""math/tex"">IoU \geq 0.5</script>) <br />
<script type=""math/tex"">P_{IoU \geq 0.5} = \frac{TP_{IoU \geq 0.5}}{TP_{IoU \geq 0.5} + FP_{IoU \geq 0.5}}</script></p>
  </li>
  <li>
    <p>Recall (<script type=""math/tex"">IoU  \geq 0.5</script>) <br />
<script type=""math/tex"">R_{IoU \geq 0.5} = \frac{TP_{IoU \geq 0.5}}{TP_{IoU \geq 0.5} + FN_{IoU \geq 0.5}}</script>.</p>
  </li>
</ul>

<p>The final scoring parameters <script type=""math/tex"">AP_{IoU \geq 0.5}</script> and <script type=""math/tex"">AR_{IoU \geq 0.5}</script> are computed by averaging over all the precision and recall values for all known annotations in the ground truth.</p>

<h1 id=""challenge-rounds"">Challenge Rounds</h1>

<h2 id=""round-1"">Round 1</h2>

<p>We will stop accepting submissions for Round 1 on June 1, 2018.
All participants of Round 1 will be invited to complete in Round 2.</p>

<p>For instructions on submitting solutions for Round-2, please refer to the <a href=""https://github.com/crowdAI/mapping-challenge-starter-kit"">mapping-challenge-starter-kit</a>.</p>

<p><strong>Note</strong>: We will be adding more content to the starter kit to help you get started in the challenge. So please do keep a close eye on the starter-kit for updates. In the meantime, you can have a look at the examples of <a href=""https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocoDemo.ipynb"">cocoapi</a> on how to easily parse and explore the datasets.</p>

<h2 id=""round-2"">Round 2</h2>

<p>Round 2 participation is open to all. 
Participants are required to submit their code and models which <strong>will be internally tested by <a href=""https://unitar.org/unosat/"">UNOSAT</a>  and <a href=""https://www.unglobalpulse.org/"">UN Global Pulse</a> on a dataset (in a similar format as the currently released data) of an undisclosed location.</strong></p>

<p><strong>Starter-Kit for Round-2 can be found at</strong> : <a href=""https://github.com/crowdAI/mapping-challenge-round2-starter-kit"">https://github.com/crowdAI/mapping-challenge-round2-starter-kit</a></p>

<h2 id=""timeline"">Timeline</h2>

<ul>
  <li>Round 1 : 28.03.2018 - 01.06.2018</li>
  <li>Round 2 : 23.07.2018 - 20.08.2018</li>
  <li>Announcement of Overall Results : 20.08.2018</li>
</ul>

<h3 id=""evaluation-criteria-1"">Evaluation criteria</h3>

<h3 id=""resources"">Resources</h3>

<p>Here are some interesting blog posts written by participants:</p>

<ul>
  <li>
    <p><a href=""https://towardsdatascience.com/mapping-challenge-winning-solution-1aa1a13161b3"" target=""_blank"">Mapping Challenge winning solution</a></p>
  </li>
  <li>
    <p><a href=""https://spark-in.me/post/a-small-case-for-search-of-structure-within-your-data"" target=""_blank"">Playing with Crowd-AI mapping challenge - or how to improve your CNN performance with self-supervised techniques</a></p>
  </li>
</ul>

<p>Here is an open solution for this challenge, proposed by neptune.ml:</p>

<ul>
  <li><a href=""https://github.com/neptune-ml/open-solution-mapping-challenge"" target=""_blank"">https://github.com/neptune-ml/open-solution-mapping-challenge</a></li>
</ul>

<h2 id=""acknowledgements"">Acknowledgements</h2>

<p>A big shout out to our awesome community members <a href=""https://www.crowdai.org/participants/masterscrat"">@MasterScat (Florian Laurent)</a>, <a href=""snigdha.dagar@gmail.com"">Snigdha Dagar</a>, and <a href=""https://www.crowdai.org/participants/iuliana"">Iuliana Voinea</a>, for their help in preparing the datasets and designing the challenge.</p>

<h2 id=""contact-us"">Contact Us</h2>

<ul>
  <li>Gitter Channel : <a href=""https://gitter.im/crowdAI/crowdai-mapping-challenge"">crowdAI/crowdai-mapping-challenge</a></li>
  <li>Technical issues : <a href=""https://github.com/crowdAI/mapping-challenge-starter-kit/issues"" target=""_blank"">https://github.com/crowdAI/mapping-challenge-starter-kit/issues </a></li>
  <li>Discussion Forum : <a href=""https://www.crowdai.org/challenges/mapping-challenge/topics"">https://www.crowdai.org/challenges/mapping-challenge/topics</a></li>
</ul>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

<ul>
  <li><a href=""mailto:sharada.mohanty@epfl.ch"" target=""_blank""> sharada.mohanty@epfl.ch
 </a></li>
</ul>

<h3 id=""prizes"">Prizes</h3>

<p><strong>UPDATE</strong></p>

<p><strong>The prizes have been updated, as follows:</strong></p>

<p><strong>Top-1 participant of Round 2</strong>: Invitation to the <a href=""https://www.appliedmldays.org"">Applied Machine Learning Days</a> 2019 at EPFL, Switzerland in January 2019, with travel and accommodation covered.</p>

<p><strong>Top-5 participants of Round 2</strong>: Invitation to present at the <a href=""https://dsaa2018.isi.it/home"">IEEE DSAA 2018</a> in Turin, Italy, October 1-4, 2018, with conference registration, travel and accommodation covered (up to EUR 1000).
This prize is sponsored by Humanity &amp; Inclusion.</p>

<p><strong>Top-5 participants of Round 2</strong>: Invitation to submit a paper describing their solution to be published in the proceedings of <a href=""https://dsaa2018.isi.it/home"">IEEE DSAA 2018</a>, Turin, Italy.</p>

<p><strong>Top Community Contributor</strong>: Invitation to the <a href=""https://dsaa2018.isi.it/home"">IEEE DSAA 2018</a> in Turin, Italy, October 1-4, 2018, with travel and accommodation covered (up to EUR 1000).
This prize is aimed to reward the participant or team who contributed the most for the community to this challenge (e.g. releasing own code openly during challenge, helping other paricipants, etc)</p>

<p><strong>All participants (Round 1 and 2)</strong> : Certificate of participation from <a href=""https://handicap-international.ch"">Handicap International</a>, <a href=""https://unitar.org/unosat/"">UNOSAT</a>, <a href=""https://www.unglobalpulse.org/"">UN Global Pulse</a>, <a href=""https://www.epfl.ch/"">EPFL</a> and <a href=""https://www.crowdai.org/"">crowdAI</a>.</p>

<h2 id=""starter-kit"">Starter Kit</h2>

<p>A starter kit has been prepared which explains all the nuts and bolts required to get started in the challenge.
It can be accessed at : <a href=""https://github.com/crowdAI/mapping-challenge-starter-kit"">https://github.com/crowdAI/mapping-challenge-starter-kit</a></p>

<p><strong>Starter-Kit for Round-2 can be found at</strong> : <a href=""https://github.com/crowdAI/mapping-challenge-round2-starter-kit"">https://github.com/crowdAI/mapping-challenge-round2-starter-kit</a></p>

<h3 id=""datasets-license"">Datasets License</h3>

"
4,"<p>Welcome to the AMLD Document Digitization challenge. In this competition you will get a chance to use machine learning to understand legal documents, with the goal of simplifying the credit approval process in a large financial institution.</p>

<p>The documents to be processed are company incorporation documents from the British Virgin Islands jurisdiction.</p>

<p>The aim of the challenge is to answer a number of questions:</p>

<ul>
  <li>is the company empowered to borrow?</li>
  <li>what is the size of the board of directors?</li>
  <li>are the directors empowered to borrow?</li>
  <li>if yes, is a resolution of directors required to borrow?</li>
  <li>if yes, what is the quorum for such a resolution?</li>
</ul>
"
22,"<p><strong>The MineRL Competition for Sample-Efficient Reinforcement Learning</strong></p>
<iframe allowfullscreen=""allowFullScreen"" src=""https://www.youtube.com/embed/ggo1WAldyq0?ecver=1&amp;iv_load_policy=3&amp;rel=0&amp;showinfo=0&amp;yt:stretch=16:9&amp;autohide=1&amp;color=white&amp;width=560&amp;width=560"" width=""100%"" height=""395"" allowtransparency=""true"" frameborder=""0"" style=""margin:auto"">
</iframe>

<p><br /></p>

<h3 id=""announcement---round-2-is-now-closed"">Announcement - Round 2 is now closed!</h3>
<p>Congratulations to all of the teams for their submissions, we are excited for finalists to present their solutions at NeruIPS on Saturday December 14th, starting at 9:00 AM.</p>

<h3 id=""submissions-open"">Submissions Open!</h3>

<p>We are so excited to announce that Round 1 of the MineRL NeurIPS 2019 Competition is
now open for submissions! Our partners at AIcrowd just released their
<a href=""https://github.com/minerllabs/competition_submission_starter_template"">competition submission starter kit</a> that you can find <a href=""https://github.com/minerllabs/competition_submission_starter_template"">here</a>.</p>

<p><strong>Here’s how you submit in Round 1</strong>:</p>

<ol>
  <li>
    <p><strong>Sign up</strong> to join the competition <strong>with the ‘Participate’ button above</strong>!</p>
  </li>
  <li>
    <p><strong>Clone</strong> the <a href=""https://github.com/minerllabs/competition_submission_starter_template"">AIcrowd starter template</a> and start developing your submissions.</p>
  </li>
  <li>
    <p><strong>Submit</strong> an agent to the leaderboard:</p>

    <ul>
      <li>
        <p><strong>Train your agents locally</strong> (or on Azure) in under <strong>8,000,000 samples</strong> over <strong>4 days</strong>. Participants should use hardware <strong>no more powerful than NG6v2 instances on Azure</strong> (6 CPU cores, 112 GiB RAM, 736 GiB SDD, and a NVIDIA P100 GPU.)</p>
      </li>
      <li>
        <p><strong>Push your repository to <a href=""https://gitlab.aicrowd.com"">AIcrowd GitLab</a></strong>, which verifies that it can successfully be re-trained by the organizers at the end of Round 1 and then runs the test entrypoint to evaluate the trained agent’s performance!</p>
      </li>
    </ul>
  </li>
</ol>

<p>Once the full evaluation of the uploaded model/code is done, the your submission will appear on the leaderboard!</p>

<h3 style=""color:#DE4B46"">  <u> <a href=""https://github.com/minerllabs/competition_submission_starter_template""><strong style=""color:#DE4B46""> » Submit your first agent! « 
</strong>
</a>
</u>
</h3>

<ul>
  <li><a href=""https://www.aicrowd.com/challenges/neurips-2019-minerl-competition""><strong>MineRL Competition Page</strong></a> - Main registration page &amp; leaderboard.</li>
  <li><a href=""https://github.com/minerllabs/competition_submission_starter_template""><strong>Submission Starter Template</strong></a> - Starting code for  submisions and guide to submit!</li>
  <li><a href=""https://github.com/minerllabs/baselines""><strong>Example Baselines</strong></a> - A set of competition and non-competition baselines for <code class=""highlighter-rouge"">minerl</code>.</li>
  <li><a href=""https://gitlab.aicrowd.com/minerl/minerl-resources/blob/master/Rules.md""><strong>Read the Rules</strong></a> - Learn about the rules of the competition <a href=""https://gitlab.aicrowd.com/minerl/minerl-resources/blob/master/Rules.md"">here</a>!</li>
  <li><a href=""http://minerl.io/docs""><strong>Read the Docs</strong></a> - To get started with the environment and dataset <a href=""http://minerl.io/docs/tutorials/getting_started.html"">please check out our quick start guide here</a>!</li>
  <li><a href=""https://discord.gg/BT9uegr""><strong>Ask Questions</strong></a> - Direct any questions to the <a href=""https://discourse.aicrowd.com/c/neurips-2019-minerl-competition"">competition forum</a> or <a href=""https://discord.gg/BT9uegr"">discord server</a>!</li>
  <li><a href=""https://github.com/minerllabs/minerl/issues""><strong>Report Bugs</strong></a> -  Report bugs and issues using our github issue tracket</li>
</ul>

<iframe src=""https://discordapp.com/widget?id=565639094860775436&amp;theme=dark"" width=""100%"" height=""500"" allowtransparency=""true"" frameborder=""0""></iframe>

<h1 id=""abstract"">Abstract</h1>
<p>Although deep reinforcement learning has led to breakthroughs in many difficult domains, these successes have required an ever-increasing number of samples. Many of these systems cannot be applied to real-world problems, where environment samples are expensive. Resolution of these limitations requires new, sample-efficient methods.</p>

<p>This competition is designed to foster the development of algorithms which can drastically reduce the number of samples needed to solve complex, hierarchical, and sparse environments using human demonstrations. Participants compete to develop systems which solve a hard task in Minecraft, obtaining a diamond, with a limited number of samples.</p>

<h1 id=""task"">Task</h1>

<p>Some of the stages of obtaining a diamond: obtaining wood, a stone pickaxe, iron, and diamond.</p>

<p><img src=""https://i.imgur.com/XB1WORT.gif"" alt=""wood"" width=""300"" />
<img src=""https://i.imgur.com/2D7R08W.gif"" alt=""stonepick"" width=""300"" />
<img src=""https://i.imgur.com/s9rjJSF.gif"" alt=""iron_ironpick"" width=""300"" />
<img src=""https://i.imgur.com/qplZhTj.gif"" alt=""diamond"" width=""300"" /></p>

<p>This competition uses a set of Gym environments based on <a href=""https://github.com/Microsoft/malmo"">Malmo</a>. The environment and dataset loader is available through a pip package. See <a href=""http://www.minerl.io/docs/"">here</a> for documentation of the environment and accessing the data.</p>

<p><strong>The task of the competition is solving the <code class=""highlighter-rouge"">MineRLObtainDiamond</code> environment.</strong>
In this environment, the agent begins in a random starting location without any items, and is tasked with obtaining a diamond.
This task can only be accomplished by navigating the complex item hierarchy of Minecraft.</p>

<p><img src=""https://i.imgur.com/hhzJprE.png"" alt=""item hierarchy"" width=""600"" /></p>

<p>The agent receives a high reward for obtaining a diamond as well as smaller, auxiliary rewards for obtaining prerequisite items. In addition to the main environment, we provide a number of auxiliary environments. These consists of tasks which are either subtasks of <em>ObtainDiamond</em> or other tasks within Minecraft.</p>

<h2 id=""why-minecraft"">Why Minecraft?</h2>
<p>Minecraft is a rich environment in which to perform learning: it is an open-world environment, has sparse rewards, and has many innate task hierarchies and subgoals. Furthermore, it encompasses many of the problems that we must solve as we move towards more general AI (for example, what is the reward structure of “building a house”?). Besides all this, Minecraft has more than 90 million monthly active users, making it a good environment on which to collect a large-scale dataset.</p>

<h1 id=""competition-structure"">Competition Structure</h1>

<h2 id=""round-1-general-entry"">Round 1: General Entry</h2>

<p><img src=""https://i.imgur.com/CqFtNTT.png"" alt=""Round 1 Procedure"" /></p>

<p>In this round, teams of up to 6 individuals will do the following:</p>

<ol>
  <li>
    <p>Register on the AICrowd competition website and receive the following materials:</p>

    <ul>
      <li><a href=""https://github.com/minerllabs/competition_submission_starter_template""><strong>Starter code</strong></a> for running the environments for the competition task.</li>
      <li><a href=""https://github.com/minerllabs/baselines""><strong>Basic baseline</strong></a> implementations provided by Preferred Networks and the competition organizers.</li>
      <li><a href=""http://minerl.io/dataset/""><strong>The human demonstration dataset</strong></a> with different renders (one for methods development, the other for validation) with modified textures, lighting conditions, and/or minor game state changes.</li>
      <li><a href=""https://github.com/minerllabs/competition_submission_starter_template""><strong>Docker Images</strong></a> and <a href=""https://github.com/minerllabs/competition_submission_starter_template""><strong>Azure quick-start template</strong></a> that the competition organizers will use to validate the training performance of the competitor’s models.</li>
      <li><a href=""https://github.com/minerllabs/competition_submission_starter_template""><strong>Scripts</strong> enabling the procurement of the standard cloud compute</a>  used to evaluate the sample-efficiency of participants’ submissions. Note that, for this competition we will specifically be restricting competitors to NC6 v2 Azure instances with 6 CPU cores, 112 GiB RAM, 736 GiB SDD, and a single NVIDIA P100 GPU.</li>
    </ul>
  </li>
  <li>
    <p>(Optional) Form a team using the ‘Create Team’ button on the competition overview. Participants must be signed in to create a team.</p>
  </li>
  <li>
    <p>Use the provided human demonstrations to develop and test procedures for efficiently training models to solve the competition task.</p>
  </li>
  <li>
    <p>Train their models against <code class=""highlighter-rouge"">MineRLObtainDiamond-v0</code> using the <a href=""https://github.com/minerllabs/competition_submission_starter_template"">local training/azure training scripts in the competition starter template</a> with <strong>only 8,000,000 samples</strong> in less than <strong>four days</strong> using hardware <strong>no more powerful than a NG6v2 instance</strong> (6 CPU cores, 112 GiB RAM, 736 GiB SDD, and a single NVIDIA P100 GPU.)</p>
  </li>
  <li>
    <p>Submit their trained models for evaluation when satisfied with their models. The automated evaluation setup will evaluate the submissions against the validation environment, to compute and report the metrics on the leaderboard of the competition.</p>
  </li>
</ol>

<p>Once Round 1 is complete, the organizers will:</p>

<ol>
  <li>
    <p>Examine the code repositories of the top submissions on the leaderboard to ensure compliance with the competition rules. The top submissions which comply with the competition rules will then automatically be re-trained by the competition orchestration platform.</p>
  </li>
  <li>
    <p>Evaluate the resulting models again over several hundred episodes to determine the final ranking.</p>
  </li>
</ol>

<p>The code repositories associated with the corresponding submissions will be forked and scrubbed of any files larger than 15MB to ensure that participants are not using any pre-trained models in the subsequent round.</p>

<h2 id=""round-2-finals"">Round 2: Finals</h2>

<p><img src=""https://i.imgur.com/y2plGdG.png"" alt=""Round 2 Procedure"" /></p>

<p>In this round, the top 10 performing teams will continue to develop their algorithms.
Their work will be evaluated against a confidential, held-out test environment and test dataset, to which they will not have access.</p>

<p>Participants will be able to make a submission four times during Round 2. For each submission, the automated evaluator will train their procedure on the held out test dataset and simulator, evaluate the trained model, and report the score and metrics back to the participants. The final ranking for this round will be based on the best-performing submission by each team.</p>

<h1 id=""funding-opportunities-and-resources"">Funding Opportunities and Resources</h1>
<p>Through our generous sponsor, Microsoft, we will provide some compute grants for teams that self identify as lacking access to the necessary compute power to participate in the competition. We will also provide groups with the evaluation resources for their experiments in Round 2.</p>

<p>The competition team is committed to increasing the participation of groups traditionally underrepresented in reinforcement learning and, more generally, in machine learning (including, but not limited to: women, LGBTQ individuals, individuals in underrepresented racial and ethnic groups, and individuals with disabilities). To that end, we will offer Inclusion@NeurIPS scholarships/travel grants for some number of Round 1 participants who are traditionally underrepresented at NeurIPS to attend the conference. We also plan to provide travel grants to enable all of the top participants from Round 2 to attend our NeurIPS workshop.</p>

<p>~~The application for the Inclusion@NeurIPS travel grants can be found <a href=""https://minerl.typeform.com/to/wV18AM"" title=""MineRL Inclusion@NeurIPS Travel Scholarship Application"">here</a>.~~ Inclusion grant application is closed!</p>

<p>~~The application for the compute grants can be found <a href=""https://minerl.typeform.com/to/v2CUCN"" title=""MineRL Compute Grant Application"">here</a>.~~ Compute grant application is closed!</p>

<h1 id=""prizes"">Prizes</h1>
<p>The first place team in round 2 will receive a Titan RTX GPU curtsy of NVIDIA. The top three teams in round 2 will receive travel grants to attend NeruIPS</p>

<h1 id=""important-dates"">Important Dates</h1>
<p><strong>May 10, 2019:</strong> <em>Applications for Grants Open</em>. Participants can apply to receive travel grants and/or compute grants.</p>

<p><strong>Jun 8, 2019:</strong> <em>First Round Begins</em>. Participants invited to download starting materials and to begin developing their submission.</p>

<p><strong>Jun 26, 2019:</strong> <em>Application for Compute Grants Closes</em>. Participants can no longer apply for compute grants.</p>

<p><strong>Jul 8, 2019:</strong> <em>Notification of Compute Grant Winners</em>. Participants notified about whether they have received a compute grant.</p>

<p><strong>Oct 1, 2019 (UTC 23:00):</strong> <em>Inclusion@NeurIPS Travel Grant Application Closes</em>. Participants can no longer apply for travel grants.</p>

<p>~~Oct 9, 2019~~ <strong>Oct 16, 2019</strong> <em>Travel Grant Winners Notified</em>. Winners of Inclusion@NeurIPS travel grants are notified.</p>

<p>~~Sep 22, 2019~~
<strong>Oct 30, 2019 (UTC 12:00):</strong> <em>First Round Ends</em>. Submissions for consideration into entry into the final round are closed. Models will be evaluated by organizers and partners.</p>

<p>~~Sep 27, 2019~~
<strong>Nov 4, 2019:</strong> <em>First Round Results Posted</em>. Official results will be posted notifying finalists.</p>

<p><strong>Nov 6, 2019:</strong>  <em>Final Round Begins</em>. Finalists are invited to submit their models against the held out validation texture pack to ensure their models generalize well.</p>

<p><strong>Nov 25, 2019:</strong> <em>Final Round Closed</em>. Submissions for finalists are closed, evaluations are completed, and organizers begin reviewing submissions.</p>

<p><strong>Dec 8, 2019:</strong>  <em>NeurIPS 2019</em>! All Round 2 teams invited to the conference to present their results.</p>

<p><strong>Dec 12, 2019:</strong> <em>Final Round Results Posted</em>. Official results of model training and evaluation are posted.</p>

<p><strong>Dec 14, 2019:</strong>  <em>MineRL NeruIPS Workshop</em> All Round 2 teams present their results.</p>

<p><strong>Dec 14, 2019:</strong> <em>Main Track Awards</em>. Awards for competitors are given out at the MineRL workshop.</p>

<p><strong>Dec 14, 2019:</strong> <em>Special Awards</em>. Additional awards granted by the advisory committee are given out at the MineRL workshop.</p>

<h1 id=""important-links"">Important Links</h1>

<ul>
  <li>
    <p><a href=""https://arxiv.org/abs/1904.10079"" title=""MineRL Competition Proposal"">Competition Proposal</a></p>
  </li>
  <li>
    <p><a href=""http://minerl.io/competition/"" title=""MineRL Competition Website"">Competition Website</a></p>
  </li>
  <li>
    <p><a href=""http://minerl.io/docs/"" title=""MineRL Documentation"">MineRL Docs</a></p>
  </li>
  <li>
    <p><a href=""https://github.com/minerllabs/minerl"" title=""MineRL GitHub Repository"">MineRL Repo</a></p>
  </li>
  <li>
    <p>~~<a href=""https://minerl.typeform.com/to/wV18AM"" title=""MineRL Inclusion@NeurIPS Travel Scholarship Application"">MineRL Inclusion@NeurIPS Travel Scholarship Application</a>~~ Inclusion scholarship application is closed!</p>
  </li>
  <li>
    <p>~~<a href=""https://minerl.typeform.com/to/v2CUCN"" title=""MineRL Compute Grant Application"">MineRL Compute Grant Application</a>~~ Compute grant application is closed!</p>
  </li>
  <li>
    <p><a href=""https://twitter.com/minerl_official"" title=""Official MineRL Competition Twitter Account"">MineRL Twitter</a></p>
  </li>
</ul>

<h1 id=""sponsors"">Sponsors</h1>
<p>Thank you to our generous sponsors!</p>

<p><img src=""https://i.imgur.com/qjNSI9M.png"" alt=""Carnegie Mellon University"" width=""300"" />
<img src=""https://i.imgur.com/NNCv6oG.png"" alt=""Microsoft"" width=""300"" />
<img src=""https://i.imgur.com/tByZbtL.png"" alt=""Preferred Networks"" width=""200"" />
<img src=""https://i.imgur.com/nJVTIRl.png"" alt=""AIJ"" width=""150"" />
<img src=""https://i.imgur.com/qzJwZYR.png"" alt=""NVIDIA"" width=""200"" /></p>

<h1 id=""team"">Team</h1>
<p>The organizing team consists of:</p>

<ul>
  <li><a href=""http://wguss.ml"">William H. Guss</a> (Carnegie Mellon University)</li>
  <li>Mario Ynocente Castro (Preferred Networks)</li>
  <li>Cayden Codel (Carnegie Mellon University)</li>
  <li>Katja Hofmann (Microsoft Research)</li>
  <li>Brandon Houghton (Carnegie Mellon University)</li>
  <li>Noboru Kuno (Microsoft Research)</li>
  <li>Crissman Loomis (Preferred Networks)</li>
  <li><a href=""https://stephmilani.github.io"">Stephanie Milani</a> (Carnegie Mellon University)</li>
  <li>Sharada Mohanty (AIcrowd)</li>
  <li>Keisuke Nakata (Preferred Networks)</li>
  <li>Diego Perez Liebana (Queen Mary University of London)</li>
  <li>Ruslan Salakhutdinov (Carnegie Mellon University)</li>
  <li>Shinya Shiroshita (Preferred Networks)</li>
  <li>Nicholay Topin (Carnegie Mellon University)</li>
  <li>Avinash Ummadisingu (Preferred Networks)</li>
  <li>Manuela Veloso (Carnegie Mellon University)</li>
  <li>Phillip Wang (Carnegie Mellon University)</li>
</ul>

<p>The advisory committee consists of:</p>

<ul>
  <li>Chelsea Finn (Google Brain and UC Berkeley)</li>
  <li>Sergey Levine (UC Berkeley)</li>
  <li>Harm van Seijen (Microsoft Research)</li>
  <li>Oriol Vinyals (Google DeepMind)</li>
</ul>

<h1 id=""contact"">Contact</h1>
<p>If you have any questions, please feel free to contact us:</p>

<ul>
  <li>Organizers <a href=""mailto:competition@minerl.io"">competition@minerl.io</a></li>
  <li>William H. Guss <a href=""mailto:wguss@minerl.io"">wguss@minerl.io</a></li>
  <li>Brandon Houghton <a href=""mailto:brandon@minerl.io"">brandon@minerl.io</a></li>
  <li>Stephanie Milani <a href=""mailto:steph@minerl.io"">steph@minerl.io</a></li>
  <li>Sharada Mohanty <a href=""mailto:mohanty@aicrowd.com"">mohanty@aicrowd.com</a></li>
</ul>

"
178,"
<p><em>Note: This challenge is one of the two subtasks of the LifeCLEF <a href=""http://www.imageclef.org/node/230"" target=""_blank""> Bird identification challenge </a> 2018. For more information about the  other subtask click <a href=""/challenges/lifeclef-2018-bird-monophone"" target=""_blank""> here </a>. Both challenges share the same training dataset.</em></p>

<h3 id=""challenge-description"">Challenge description</h3>

<p>The goal of the task is to localize and identify all audible birds within the provided soundscape recordings. Each soundscape is divided into segments of 5 seconds, and a list of species associated to probability scores will have to be returned for each segment. Each prediction item (i.e. each line of the file) has to respect the following format:
&lt; MediaId;TC1-TC2;ClassId;probability&gt;
where probability is a real value in [0;1] decreasing with the confidence in the prediction, and where TC1-TC2 is a timecode interval with the format of hh:mm:ss with a length of 5 seconds (e.g.: 00:00:00-00:00:05, then 00:00:05-00:00:10).</p>

<p>Here is a short fake run example respecting this format on 3 segments of 5 seconds related to two MediaId:
<a href=""https://crowdai-prd.s3.eu-central-1.amazonaws.com/task_dataset_files/clef_task_5/26bff1c8-f089-4740-99d6-213205fb9198_soundscape_fake_run.csv?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAILFF3ZEGG7Y4HXEQ%2F20180503%2Feu-central-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20180503T124454Z&amp;X-Amz-Expires=604800&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Signature=9d210bc3e875eee3d6f758ee535fd752f2d1ba011582be72bc661a58b8e1beb2"">soundscape_fake_run</a></p>

<p>Each participating group is allowed to submit up to 4 runs built from different methods. Semi-supervised, interactive or crowdsourced approaches are allowed but will be compared independently from fully automatic methods. Any human assistance in the processing of the test queries has therefore to be signaled in the submitted runs.</p>

<p>Participants are allowed to use any of the provided metadata complementary to the audio content (.wav 44.1, 48 kHz or 96 kHz sampling rate), and will also be allowed to use any external training data but at the condition that (i) the experiment is entirely re-producible, i.e. that the used external resource is clearly referenced and accessible to any other research group in the world, (ii) participants submit at least one run without external training data so that we can study the contribution of such resources, (iii) the additional resource does not contain any of the test observations. It is in particular strictly forbidden to crawl training data from: <a href=""http://www.xeno-canto.org/"" target=""_blank""> www.xeno-canto.org </a></p>

<h3 id=""data"">Data</h3>

<p>The training set contains 36,496 monophone recordings of the Xeno-Canto network covering 1500 species of central and south America (the largest bioacoustic dataset in the literature). It has a massive class imbalance with a minimum of four recordings for Laniocera rufescens and a maximum of 160 recordings for Henicorhina leucophrys. Recordings are associated to various metadata such as the type of sound (call, song, alarm, flight, etc.), the date, the location, textual comments of the authors, multilingual common names and collaborative quality ratings. 
Complementary to that data, a validation set of soundscapes with time-coded labels will be provided as training data. It contains about 20 minutes of soundscapes representing 240 segments of 5 seconds and with a total of 385 bird species annotations.</p>

<p>The test set itself will contain about 6 hours of soundscapes split in 4382 segments of 5 seconds (to be processed as separate queries). Some of them will be Stereophonic, offering possible sources separation to enhance the recognition.</p>

<h3 id=""submission-instructions"">Submission instructions</h3>

<hr />
<p><em>As soon as the submission is open, you will find a “Create Submission” button on this page (just next to the tabs)</em></p>

<hr />

<h3 id=""results-tables-and-figures"">Results (tables and figures)</h3>
<p><a href=""http://www.imageclef.org/node/230"" target=""_blank"">(Official round during the LifeCLEF 2018 campaign)</a></p>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>
<p>The used metric will be the classification mean Average Precision (c-mAP), considering each class c of the ground truth as a query. This means that for each class c, we will extract from the run file all predictions with ClassId=c, rank them by decreasing probability and compute the average precision for that class. We will then take the mean across all classes. More formally:</p>

<p><img src=""https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/dff548c8fe3b97e373cc344242d125eb_cmap.png"" alt=""cmap.png"" /></p>

<p>where C is the number of species in the ground truth and AveP(c) is the average precision for a given species c computed as:</p>

<p><img src=""https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/59cd979a30b172d90dc6cd307a63ce3d_AveP.png"" alt=""AveP.png"" /></p>

<p>where k is the rank of an item in the list of the predicted segments containing c, n is the total number of predicted segments containing c, P(k) is the precision at cut-off k in the list, rel(k) is an indicator function equaling 1 if the segment at rank k is a relevant one (i.e. is labeled as containing c in the ground truth) and nrel is the total number of relevant segments for c.</p>

<h3 id=""resources"">Resources</h3>

<h3 id=""contact-us"">Contact us</h3>

<ul>
  <li>Technical issues : <a href=""https://gitter.im/crowdAI/lifeclef-2018-bird-soundscape"" target=""_blank""> https://gitter.im/crowdAI/lifeclef-2018-bird-soundscape </a></li>
  <li>Discussion Forum : <a href=""https://www.crowdai.org/challenges/lifeclef-2018-bird-soundscape/topics"" target=""_blank""> https://www.crowdai.org/challenges/lifeclef-2018-bird-soundscape/topics </a></li>
</ul>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

<ul>
  <li>Sharada Prasanna Mohanty: sharada.mohanty@epfl.ch</li>
  <li>Hervé Glotin: glotin[AT]univ-tln[DOT]fr</li>
  <li>Hervé Goëau: herve[DOT]goeau[AT]cirad[DOT]fr</li>
  <li>Alexis Joly: alexis[DOT]joly[AT]inria[DOT]fr</li>
  <li>Ivan Eggel: ivan[DOT]eggel[AT]hevs[DOT]ch</li>
</ul>

<h3 id=""more-information"">More information</h3>

<p>You can find additional information on the challenge here:
<a href=""http://imageclef.org/node/230"" target=""_blank""> http://imageclef.org/node/230 </a></p>

<h3 id=""baseline-repository"">Baseline Repository</h3>

<p>You can find a baseline system and a continuative tutorial can be found here: <a href=""link_url"" target=""_blank""> https://github.com/kahst/BirdCLEF-Baseline </a></p>

<p>We encourage all participants of the challenge to build upon the provided code base and share the results for future reference.</p>

<h3 id=""results-tables-and-figures-1"">Results (tables and figures)</h3>
<p><a href=""http://www.imageclef.org/node/230"" target=""_blank"">(Official round during the LifeCLEF 2018 campaign)</a></p>

<h3 id=""prizes"">Prizes</h3>

<h3 id=""datasets-license"">Datasets License</h3>

"
213,"<p><em>Note: Do not forget to read the <strong>Rules</strong> section on this page. Pressing the red <strong>Participate button</strong> leads you to a page where you have to agree with those rules. You will not be able to submit any results before agreeing with the rules.</em></p>

<p><em>Note: Before trying to submit results, read the <strong>Submission instructions</strong> section on this page.</em></p>

<h1 id=""challenge-description"">Challenge description</h1>
<p>The goal of the challenge is to identify plants in field pictures based on a training set of digitized herbarium specimens. Concretely, this will consist in a cross-domain classification task with a training set composed of digitized herbarium sheets and a test set composed of field pictures. To enable learning a mapping between the herbarium sheets domain and the field pictures domain, we will provide both herbarium sheets and field pictures for a subset of species.</p>

<h1 id=""motivation"">Motivation</h1>
<p>Despite recent progress in automated plant identification, a vast majority of the 300K+ plant species on earth can still not be recognized easily because of the lack of training data for that species. On the other side, for several centuries, botanists have collected, catalogued and systematically stored plant specimens in herbaria. These physical specimens are used to study the variability of species, their phylogenetic relationship, their evolution, or phenological trends. Millions of such specimens are now digitized and publicly available. Using them for training deep learning models is thus a very promising approach to help identifying data deficient species. However, their visual appearance is very different from field pictures which makes it a challenging cross-domain classification task.</p>

<h1 id=""data"">Data</h1>
<p>The challenge will rely on a large collection of more than 300K herbarium sheets coming from two sources:  the ``Herbier IRD de Guyane”, CAY) digitized in the context of the e-ReColNat project, and iDigBio, a large international platform hosting millions of images of herbarium specimens. A valuable asset of this collection is that a few hundreds of herbarium sheets are accompanied by a few pictures of the same specimen in the field. The test set is composed of about 3K in-the-field pictures collected by two botanists specialist of the Amazonian flora.</p>

<hr />

<p>A link to the the training dataset is available under the “Resources” tab.</p>

<hr />

<h1 id=""submission-instructions"">Submission instructions</h1>

<hr />
<p><em>As soon as the submission is open, you will find a “Create Submission” button on this page (next to the tabs).</em></p>

<hr />
<p><em>Before being allowed to submit your results, you have to first press the red participate button, which leads you to a page where you have to accept the challenges rules.</em></p>

<hr />

<p>More information regarding the submission instructions will be released soon.</p>

<h1 id=""evaluation-criteria"">Evaluation criteria</h1>

<p>The metrics used for the evaluation of the task will be the mean reciprocal rank (primary metric used for the leaderboard) and the classification accuracy (used as secondary metric).</p>

<h1 id=""rules"">Rules</h1>

<p>LifeCLEF lab is part of the Conference and Labs of the Evaluation Forum: CLEF 2020. CLEF 2020 consists of independent peer-reviewed workshops on a broad range of challenges in the fields of multilingual and multimodal information access evaluation, and a set of benchmarking activities carried in various labs designed to test different aspects of mono and cross-language Information retrieval systems. More details about the conference can be found <a href=""http://clef2020.clef-initiative.eu/"" target=""_blank"">here</a>.</p>

<p>Submitting a working note with the full description of the methods used in each run is mandatory. Any run that could not be reproduced thanks to its description in the working notes might be removed from the official publication of the results. Working notes are published within CEUR-WS proceedings, resulting in an assignment of an individual DOI (URN) and an indexing by many bibliography systems including DBLP. According to the CEUR-WS policies, a light review of the working notes will be conducted by LifeCLEF organizing committee to ensure quality. As an illustration, LifeCLEF 2019 working notes (task overviews and participant working notes) can be found within <a href=""http://ceur-ws.org/Vol-2380/"" target=""_blank"">CLEF 2019 CEUR-WS</a> proceedings.</p>

<h2 id=""important"">Important</h2>

<p>Participants of this challenge will automatically be registered at CLEF 2020. In order to be compliant with the CLEF registration requirements, please edit your profile by providing the following additional information:</p>

<ul>
  <li>
    <p>First name</p>
  </li>
  <li>
    <p>Last name</p>
  </li>
  <li>
    <p>Affiliation</p>
  </li>
  <li>
    <p>Address</p>
  </li>
  <li>
    <p>City</p>
  </li>
  <li>
    <p>Country</p>
  </li>
  <li>
    <p><em>Regarding the username, please choose a name that represents your team.</em></p>
  </li>
</ul>

<p><em>This information will not be publicly visible and will be exclusively used to contact you and to send the registration data to CLEF, which is the main organizer of all CLEF labs</em></p>

<h1 id=""citations"">Citations</h1>

<p>Information will be posted after the challenge ends.</p>

<h1 id=""prizes"">Prizes</h1>

<h2 id=""cloud-credit"">Cloud credit</h2>

<p>The winner of each of the challenge will be offered a cloud credit grant of 5k USD as part of Microsoft’s AI for earth program.</p>

<h2 id=""publication"">Publication</h2>

<p>LifeCLEF 2020 is an evaluation campaign that is being organized as part of the <a href=""http://clef2020.clef-initiative.eu/"" target=""_blank"">CLEF initiative</a> labs. The campaign offers several research tasks that welcome participation from teams around the world. The results of the campaign appear in the working notes proceedings, published by CEUR Workshop Proceedings (CEUR-WS.org). Selected contributions among the participants, will be invited for publication in the following year in the Springer Lecture Notes in Computer Science (LNCS) together with the annual lab overviews.</p>

<h1 id=""resources"">Resources</h1>

<h2 id=""contact-us"">Contact us</h2>

<p><em>Discussion Forum</em></p>

<ul>
  <li>You can ask questions related to this challenge on the Discussion Forum. Before asking a new question please make sure that question has not been asked before.</li>
  <li>Click on Discussion tab above or direct link: <a href=""https://discourse.aicrowd.com/c/lifeclef-2020-plant"" target=""_blank"">https://discourse.aicrowd.com/c/lifeclef-2020-plant</a></li>
</ul>

<p><em>Alternative channels</em></p>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

<ul>
  <li>herve[dot]goeau[at]cirad[dot]fr</li>
  <li>julien[dot]champ[at]inria[dot]fr</li>
  <li>pierre[dot]bonnet[at]cirad[dot]fr</li>
  <li>alexis[dot]joly[at]inria[dot]fr</li>
</ul>

<h2 id=""more-information"">More information</h2>

<p>You can find additional information on the challenge here:
<a href=""https://www.imageclef.org/PlantCLEF2020"" target=""_blank"">https://www.imageclef.org/PlantCLEF2020</a></p>

"
142,"<p>The International Space Station (ISS) has taken millions of images of Earth. About 30% of those images were taken at night. These  photograph are the highest-resolution night imagery available from orbit. <a href=""http://www.nasa.gov/mission_pages/station/research/news/crowdsourcing_night_images"">For technical reasons</a>, we don’t precisely know what the camera was pointing at when taking the images. However, if we knew exactly what these images were showing (stars, cities, auroras, etc.), we could use this wealth of information to address issues such as night pollution, population movement, energy usage, etc.</p>

<p>Until recently, it has been thought that algorithms cannot distinguish between stars, cities, and other objects. This challenge tries to prove otherwise. Thousands of volunteers have hand-labeled tens of thousands of images as part of a <a href=""http://crowdcrafting.org/project/darkskies/"">citizen science project</a>. The goal of this challenge is to build on this manually evaluated data set, and develop an image classification algorithm that can correctly identify whether an image shows stars, cities, or other objects.</p>

<p><img src=""https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/783da80339b677a621a28fc9c8649b64_spain_portugalpsp.jpg"" alt="""" />
<em>The Iberian Peninsula at night, showing Spain and Portugal. Madrid is the bright spot just above the center. Credits: NASA</em></p>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>
<p>Submissions will be evaluated using a Multi Class Log Loss evaluation function, which are defined as :</p>

<h3 id=""mean-f1-score"">Mean F1 score</h3>

<p>The F1 score is computed separately for all classes by using:</p>

<p><img src=""https://s3.amazonaws.com/salathegroup-static/plantvillage/mean_f1.png"" alt="""" class=""img-medium"" /></p>

<ul>
  <li><strong><em>p</em></strong> refers to the precision</li>
  <li><strong><em>r</em></strong> refers to the recall</li>
  <li><strong><em>tp</em></strong> refers to the number of true positives,</li>
  <li><strong><em>fp</em></strong> refers to the number of false positives</li>
  <li><strong><em>fn</em></strong> refers to the number of false negatives</li>
</ul>

<p>Then finally the mean of all the F1 scores across all the classes is used for come up with the combined mean F1 score.</p>

<h3 id=""mean-log-loss"">Mean Log Loss</h3>

<p><img src=""https://s3.amazonaws.com/salathegroup-static/plantvillage/mean+log+loss.png"" alt="""" class=""img-medium"" /></p>

<ul>
  <li><strong><em>N</em></strong> is the total number of examples in the test set</li>
  <li><strong><em>M</em></strong> is the total number of class labels (7 for  this challenge)</li>
  <li><strong><em>y ij</em></strong> is a boolean value representing if the i-th instance in the test set belongs to the j-th label.</li>
  <li><strong><em>p ij</em></strong> is the probability according to your submission that the i-th instance may belong to the j-th label.</li>
  <li><strong><em>Ln</em></strong> is the natural logarithmic function.</li>
</ul>

<p>All submissions will be evaluated on the test dataset in the docker containers referenced in the Resources section. The code archive will be uncompressed into the <code class=""highlighter-rouge"">/darkskies</code> path, and every code archive is expected to contain a <code class=""highlighter-rouge"">main.sh</code> script which takes path to a folder containing images as its first parameter. So to test your code submission, we will finally execute :</p>

<p><code class=""highlighter-rouge"">/darkskies/main.sh pathToFolderContainingTestImages</code></p>

<p>This is expected to output a CSV file containing the name of the file, and the associated probabilities for all the classes at the location :</p>

<p><code class=""highlighter-rouge"">/darkskies/classification.csv</code></p>

<h3 id=""resources"">Resources</h3>

<p>The challenge entries may be made using any of the frameworks listed below. Selected entries will be tested using the linked Docker containers, which are the standard development environments for the challenge.</p>

<p><strong>Caffe</strong>  : https://hub.docker.com/r/tleyden5iwx/caffe-gpu-master/
<strong>Tensorflow</strong> : https://hub.docker.com/r/tensorflow/tensorflow/
<strong>Torch7</strong> : https://hub.docker.com/r/kaixhin/cuda-torch/
<strong>Scikit-Learn</strong> :(Python-2): https://github.com/dataquestio/ds-containers/tree/master/python2
<strong>Scikit-Learn</strong> : (Python-3): https://github.com/dataquestio/ds-containers/tree/master/python3
<strong>Octave</strong> : https://hub.docker.com/r/schickling/octave/
<strong>Keras</strong> :  https://hub.docker.com/r/patdiscvrd/keras/~/dockerfile/</p>

<p>You can submit a question in the <a href=""https://www.crowdai.org/challenges/2/topics"">forum</a> should you need help with any of these containers or frameworks. We are here to help you learn!</p>

<h3 id=""prizes"">Prizes</h3>

<p>The author of the most highly ranked submission will be invited to the <strong>crowdAI winner’s symposium</strong> at EPFL in Switzerland on January 30/31, 2017. The educational award is given to the participant with the either the most insightful submission posts, or the best tutorial - the recipient of this award will also be invited to the symposium (the crowdAI team will pick the recipient of this award). Expenses for travel and accommodation are covered by crowdAI.</p>

<h3 id=""datasets-license"">Datasets License</h3>

<p>The images used in this challenge have been made available by the <strong>Earth Science and Remote Sensing Unit, NASA Johnson Space Center</strong>, and the original files can be found on the NASA website <a href=""http://eol.jsc.nasa.gov"">link</a>.
The images used in this challenge were labelled by a hugely successful <a href=""http://crowdcrafting.org/project/darkskies/"">crowdsourcing challenge</a> by the <a href=""www.citiesatnight.org"">Cities@Night</a> project on <a href=""http://crowdcrafting.org/"">CrowdCrafting</a>, where ~19000 people from all over the world came together to manually classify ~170000 images.</p>

<p>Further conditions surrounding the use of these images may be found <a href=""http://eol.jsc.nasa.gov/FAQ/default.htm#terms"">here</a>.</p>

"
214,"<p><em>Note: Do not forget to read the <strong>Rules</strong> section on this page. Pressing the red <strong>Participate button</strong> leads you to a page where you have to agree with those rules. You will not be able to submit any results before agreeing with the rules.</em></p>

<p><em>Note: Before trying to submit results, read the <strong>Submission instructions</strong> section on this page.</em></p>

<h1 id=""challenge-description"">Challenge description</h1>

<p>…</p>

<h1 id=""data"">Data</h1>

<hr />
<p><em>As soon as the data is released it will be available under the “Resources” tab.</em></p>

<hr />

<p>More information will follow soon.</p>

<h1 id=""submission-instructions"">Submission instructions</h1>

<hr />
<p><em>As soon as the submission is open, you will find a “Create Submission” button on this page (next to the tabs).</em></p>

<hr />
<p><em>Before being allowed to submit your results, you have to first press the red participate button, which leads you to a page where you have to accept the challenges rules.</em></p>

<hr />

<p>More information regarding the submission instructions will be released soon.</p>

<h1 id=""evaluation-criteria"">Evaluation criteria</h1>

<p>More information regarding the evaluation criteria will be released soon.</p>

<h1 id=""rules"">Rules</h1>

<p>LifeCLEF lab is part of the Conference and Labs of the Evaluation Forum: CLEF 2020. CLEF 2020 consists of independent peer-reviewed workshops on a broad range of challenges in the fields of multilingual and multimodal information access evaluation, and a set of benchmarking activities carried in various labs designed to test different aspects of mono and cross-language Information retrieval systems. More details about the conference can be found <a href=""http://clef2020.clef-initiative.eu/"" target=""_blank"">here</a>.</p>

<p>Submitting a working note with the full description of the methods used in each run is mandatory. Any run that could not be reproduced thanks to its description in the working notes might be removed from the official publication of the results. Working notes are published within CEUR-WS proceedings, resulting in an assignment of an individual DOI (URN) and an indexing by many bibliography systems including DBLP. According to the CEUR-WS policies, a light review of the working notes will be conducted by LifeCLEF organizing committee to ensure quality. As an illustration, LifeCLEF 2019 working notes (task overviews and participant working notes) can be found within <a href=""http://ceur-ws.org/Vol-2380/"" target=""_blank"">CLEF 2019 CEUR-WS</a> proceedings.</p>

<h2 id=""important"">Important</h2>

<p>Participants of this challenge will automatically be registered at CLEF 2020. In order to be compliant with the CLEF registration requirements, please edit your profile by providing the following additional information:</p>

<ul>
  <li>
    <p>First name</p>
  </li>
  <li>
    <p>Last name</p>
  </li>
  <li>
    <p>Affiliation</p>
  </li>
  <li>
    <p>Address</p>
  </li>
  <li>
    <p>City</p>
  </li>
  <li>
    <p>Country</p>
  </li>
  <li>
    <p><em>Regarding the username, please choose a name that represents your team.</em></p>
  </li>
</ul>

<p><em>This information will not be publicly visible and will be exclusively used to contact you and to send the registration data to CLEF, which is the main organizer of all CLEF labs</em></p>

<h1 id=""citations"">Citations</h1>

<p>Information will be posted after the challenge ends.</p>

<h1 id=""prizes"">Prizes</h1>

<h2 id=""cloud-credit"">Cloud credit</h2>

<p>The winner of each of the challenge will be offered a cloud credit grant of 5k USD as part of Microsoft’s AI for earth program.</p>

<h2 id=""publication"">Publication</h2>

<p>LifeCLEF 2020 is an evaluation campaign that is being organized as part of the <a href=""http://clef2020.clef-initiative.eu/"" target=""_blank"">CLEF initiative</a> labs. The campaign offers several research tasks that welcome participation from teams around the world. The results of the campaign appear in the working notes proceedings, published by CEUR Workshop Proceedings (CEUR-WS.org). Selected contributions among the participants, will be invited for publication in the following year in the Springer Lecture Notes in Computer Science (LNCS) together with the annual lab overviews.</p>

<h1 id=""resources"">Resources</h1>

<h2 id=""contact-us"">Contact us</h2>

<p><em>Discussion Forum</em></p>

<ul>
  <li>You can ask questions related to this challenge on the Discussion Forum. Before asking a new question please make sure that question has not been asked before.</li>
  <li>Click on Discussion tab above or direct link: <a href=""https://discourse.aicrowd.com/c/lifeclef-2020-snake"" target=""_blank"">https://discourse.aicrowd.com/c/lifeclef-2020-snake</a></li>
</ul>

<p><em>Alternative channels</em></p>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

<ul>
  <li>rafael[dot]ruizdecastaneda[at]unige[dot]ch</li>
  <li>amdurso[at]gmail[dot]com</li>
</ul>

<h2 id=""more-information"">More information</h2>

<p>You can find additional information on the challenge here:
<a href=""https://www.imageclef.org/SnakeCLEF2020"" target=""_blank"">https://www.imageclef.org/SnakeCLEF2020</a></p>

"
216,"<p><em>Note: ImageCLEF Lifelog 2020 is divided into 2 subtasks (challenges). This challenge is about Sport Performance Lifelog (<strong>SPLL</strong>). For information on the Lifelog moment retrieval (<strong>LMRT</strong>) challenge click <a href=""/challenges/imageclef-2020-lifelog-lmrt"" target=""_blank"">here</a>. Both challenges share the same dataset, so registering for one of these challenges will automatically give you access to the other one.</em></p>

<p><em>Note: Do not forget to read the <strong>Rules</strong> section on this page. Pressing the red <strong>Participate button</strong> leads you to a page where you have to agree with those rules. You will not be able to submit any results before agreeing with the rules.</em></p>

<p><em>Note: Before trying to submit results, read the <strong>Submission instructions</strong> section on this page.</em></p>

<h1 id=""lifelog-schedule"">Lifelog Schedule</h1>
<ul>
  <li>10.02.2020: registration opens</li>
  <li>17.02.2020: development data released</li>
  <li>14.04.2020: test data release starts</li>
  <li>15.05.2020: deadline for submitting the participants runs</li>
  <li>18.05.2020: release of the processed results by the task organizers</li>
  <li>30.05.2020: deadline for submission of working notes papers by the participants</li>
  <li>15.06.2020: notification of acceptance of the working notes papers</li>
  <li>29.06.2020: camera ready working notes papers</li>
  <li>22-25.09.2020: CLEF 2020<a href=""https://clef2020.clef-initiative.eu/"">https://clef2020.clef-initiative.eu/</a>, Thessaloniki, Greece</li>
</ul>

<h1 id=""challenge-description"">Challenge description</h1>
<p>New Lifelog Task: sport performance lifelog (SPLL, 1st edition). The participants are required to predict the expected performance (e.g., estimated finishing time, average heart rate and calorie consumption) for an athlete who trained for a sport event. Data. A new data set will be provided, e.g., information collected from 2-3 people that train for a 10km run, daily sleeping patterns, daily heart rate, sport activities, image logs of all food consumed during the training period, close up images of the eyes of the runner before and after training (time stamps December 2019 to April 2020).
…</p>

<h1 id=""data"">Data</h1>
<p>The 4th edition of this task will come with new, enriched data, focused on daily living activities and the chronological order of the moments and a completely new task for assessing sport performance.
*****
<em>As soon as the data is released it will be available under the “Resources” tab.</em></p>

<hr />

<p>More information will follow soon.</p>

<h1 id=""submission-instructions"">Submission instructions</h1>

<hr />
<p><em>As soon as the submission is open, you will find a “Create Submission” button on this page (next to the tabs).</em></p>

<hr />
<p><em>Before being allowed to submit your results, you have to first press the red participate button, which leads you to a page where you have to accept the challenges rules.</em></p>

<hr />

<p>More information regarding the submission instructions will be released soon.</p>

<h1 id=""evaluation-criteria"">Evaluation criteria</h1>

<p>For assessing performance, classic metrics will be deployed. These metrics are:</p>

<p>Cluster Recall at X (CR@X) - a metric that assesses how many different clusters from the ground truth are represented among the top X results;
Precision at X (P@X) - measures the number of relevant photos among the top X results;
F1-measure at X (F1@X) - the harmonic mean of the previous two.
Various cut off points are to be considered, e.g., X=5, 10, 20, 30, 40, 50. Official ranking metrics this year will be the F1-measure@10, which gives equal importance to diversity (via CR@10) and relevance (via P@10).</p>

<p>Participants are allowed to undertake the sub-tasks in an interactive or automatic manner. For interactive submissions, a maximum of five minutes of search time is allowed per topic. In particular, the organizers would like to emphasize methods that allow interaction with real users (via Relevance Feedback (RF), for example), i.e., beside of the best performance, the way of interaction (like number of iterations using RF), or innovation level of the method (for example, new way to interact with real users) are encouraged.</p>

<h1 id=""rules"">Rules</h1>

<p><em>Note: In order to participate in this challenge you have to sign an End User Agreement (EUA). You will find more information on the ‘Resources’ tab.</em></p>

<p>ImageCLEF lab is part of the Conference and Labs of the Evaluation Forum: CLEF 2020. CLEF 2020 consists of independent peer-reviewed workshops on a broad range of challenges in the fields of multilingual and multimodal information access evaluation, and a set of benchmarking activities carried in various labs designed to test different aspects of mono and cross-language Information retrieval systems. More details about the conference can be found <a href=""http://clef2020.clef-initiative.eu/"" target=""_blank"">here</a> .</p>

<p>Submitting a working note with the full description of the methods used in each run is mandatory. Any run that could not be reproduced thanks to its description in the working notes might be removed from the official publication of the results. Working notes are published within CEUR-WS proceedings, resulting in an assignment of an individual DOI (URN) and an indexing by many bibliography systems including DBLP. According to the CEUR-WS policies, a light review of the working notes will be conducted by ImageCLEF organizing committee to ensure quality. As an illustration, ImageCLEF 2019 working notes (task overviews and participant working notes) can be found within <a href=""http://ceur-ws.org/Vol-2380/"" target=""_blank"">CLEF 2019 CEUR-WS</a> proceedings.</p>

<h2 id=""important"">Important</h2>

<p>Participants of this challenge will automatically be registered at CLEF 2020. In order to be compliant with the CLEF registration requirements, please edit your profile by providing the following additional information:</p>

<ul>
  <li>
    <p>First name</p>
  </li>
  <li>
    <p>Last name</p>
  </li>
  <li>
    <p>Affiliation</p>
  </li>
  <li>
    <p>Address</p>
  </li>
  <li>
    <p>City</p>
  </li>
  <li>
    <p>Country</p>
  </li>
  <li>
    <p><em>Regarding the username, please choose a name that represents your team.</em></p>
  </li>
</ul>

<p><em>This information will not be publicly visible and will be exclusively used to contact you and to send the registration data to CLEF, which is the main organizer of all CLEF labs</em></p>

<h2 id=""participating-as-an-individual-non-affiliated-researcher"">Participating as an individual (non affiliated) researcher</h2>

<p>We welcome individual researchers, i.e. not affiliated to any institution, to participate. We kindly ask you to provide us with a motivation letter containing the following information:</p>

<ul>
  <li>
    <p>the presentation of your most relevant research activities related to the task/tasks</p>
  </li>
  <li>
    <p>your motivation for participating in the task/tasks and how you want to exploit the results</p>
  </li>
  <li>
    <p>a list of the most relevant 5 publications (if applicable)</p>
  </li>
  <li>
    <p>the link to your personal webpage</p>
  </li>
</ul>

<p>The motivation letter should be directly concatenated to the End User Agreement document or sent as a PDF file to bionescu at imag dot pub dot ro. The request will be analyzed by the ImageCLEF organizing committee. We reserve the right to refuse any applicants whose experience in the field is too narrow, and would therefore most likely prevent them from being able to finish the task/tasks.</p>

<h1 id=""citations"">Citations</h1>

<p>Information will be posted after the challenge ends.</p>

<h1 id=""prizes"">Prizes</h1>

<h2 id=""publication"">Publication</h2>

<p>ImageCLEF 2020 is an evaluation campaign that is being organized as part of the <a href=""http://clef2020.clef-initiative.eu/"" target=""_blank"">CLEF initiative</a> labs. The campaign offers several research tasks that welcome participation from teams around the world. The results of the campaign appear in the working notes proceedings, published by CEUR Workshop Proceedings (CEUR-WS.org). Selected contributions among the participants, will be invited for publication in the following year in the Springer Lecture Notes in Computer Science (LNCS) together with the annual lab overviews.</p>

<h1 id=""resources"">Resources</h1>

<h2 id=""contact-us"">Contact us</h2>

<p><em>Discussion Forum</em></p>

<ul>
  <li>You can ask questions related to this challenge on the Discussion Forum. Before asking a new question please make sure that question has not been asked before.</li>
  <li>Click on Discussion tab above or direct link: <a href=""https://discourse.aicrowd.com/c/imageclef-2020-lifelog-spll"" target=""_blank"">https://discourse.aicrowd.com/c/imageclef-2020-lifelog-spll</a></li>
</ul>

<p><em>Alternative channels</em></p>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

<ul>
  <li>ductien.dangnguyen[at]uib[dot]no&gt;</li>
  <li>zhou.liting2[at]mail[dot]dcu[dot]ie</li>
  <li>luca.piras[at]diee[dot]unica[dot]it</li>
  <li>michael[at]simula[dot]no</li>
  <li>tmtriet[at]hcmus[dot]edu[dot]vn</li>
  <li>mlux[at]itec[dot]aau[dot]at</li>
  <li>cgurrin[at]computing[dot]dcu[dot]ie</li>
</ul>

<h2 id=""more-information"">More information</h2>

<p>You can find additional information on the challenge here:
<a href=""https://www.imageclef.org/2020/lifelog"" target=""_blank"">https://www.imageclef.org/2020/lifelog</a></p>

"
31,"<blockquote class=""update"">
  <p>The Starter Kit for this challenge is available at : <a href=""https://github.com/AIcrowd/food-recognition-challenge-starter-kit"">https://github.com/AIcrowd/food-recognition-challenge-starter-kit</a></p>
</blockquote>

<h1 class=""mt-0"" id=""overview"">Overview</h1>

<p>Recognizing food from images is an extremely useful tool for a variety of use cases. In particular, it would allow people to track their food intake by simply taking a picture of what they consume. Food tracking can be of personal interest, and can often be of medical relevance as well. Medical studies have for some time been interested in the food intake of study participants, but had to rely on food frequency questionnaires that are known to be imprecise.</p>

<p>Image-based food recognition has in the past few years made substantial progress thanks to advances in deep learning. But food recognition remains a difficult problem for a variety of reasons.</p>

<h2 id=""problem-statement"">Problem Statement</h2>
<p>The goal of this challenge is to train models which can look at images of food items and detect the individual food items present in them.
We use a novel dataset of food images collected through the MyFoodRepo app where numerous volunteer Swiss users provide images of their daily food intake in the context of a digital cohort called Food &amp; You. This growing data set has been annotated - or automatic annotations have been verified - with respect to segmentation, classification (mapping the individual food items onto an ontology of Swiss Food items), and weight / volume estimation.</p>

<p>This is an evolving dataset, where we will release more data as the dataset grows over time.</p>

<p><img src=""https://i.imgur.com/zS2Nbf0.png"" alt=""image1"" /></p>

<h1 id=""datasets"">Datasets</h1>

<p>Finding annotated food images is difficult. There are some databases with some annotations, but they tend to be limited in important ways.</p>

<p>To put it bluntly: most food images on the internet are a lie. Search for any dish, and you’ll find beautiful stock photography of that particular dish. Same on social media: we share photos of dishes with our friends when the image is exceptionally beautiful. But algorithms need to work on real world images. In addition, annotations are generally missing - ideally, food images would be annotated with proper segmentation, classification, and volume / weight estimates.</p>

<p>The dataset for the <a href=""https://www.aicrowd.com/challenges/food-recognition-challenge"">AIcrowd Food Recognition Challenge</a> is available at <a href=""https://www.aicrowd.com/challenges/food-recognition-challenge/dataset_files"">https://www.aicrowd.com/challenges/food-recognition-challenge/dataset_files</a></p>

<p>This dataset contains : <br />
* <code class=""highlighter-rouge"">train-v0.2.tar.gz</code> : This is the Training Set of <strong>7949</strong> (as RGB images) food images, along with their corresponding annotations in <a href=""http://cocodataset.org/#home"">MS-COCO format</a></p>

<ul>
  <li>
    <p><code class=""highlighter-rouge"">val-v0.2.tar.gz</code>: This is the suggested Validation Set of <strong>418</strong> (as RGB images) food images, along with their corresponding annotations in <a href=""http://cocodataset.org/#home"">MS-COCO format</a></p>
  </li>
  <li>
    <p><code class=""highlighter-rouge"">test_images-v0.2.tar.gz</code> : This is the debug Test Set for Round-1, where you are provided the same images as the validation set.</p>
  </li>
</ul>

<p>To get started, we would advise you to download all the files, and untar them inside the <code class=""highlighter-rouge"">data/</code> folder of this repository, so that you have a directory structure like this :</p>

<div class=""highlighter-rouge""><div class=""highlight""><pre class=""highlight""><code>|-- data/
|   |-- test_images/ (has all images for prediction)(**NOTE** : They are the same as the validation set images)
|   |-- train/
|   |   |-- images (has all the images for training)
|   |   |__ annotation.json : Annotation of the data in MS COCO format
|   |   |__ annotation-small.json : Smaller version of the previous dataset
|   |-- val/
|   |   |-- images (has all the images for training)
|   |   |__ annotation.json : Annotation of the data in MS COCO format
|   |   |__ annotation-small.json : Smaller version of the previous dataset
</code></pre></div></div>

<h1 id=""an-open-benchmark"">An open benchmark</h1>

<p>For all the reasons mentioned above, food recognition is a difficult, but important problem. Algorithms who could tackle this problem would be extremely useful for everyone. That is why we are establishing this open benchmark for food recognition. The goal is simple: provide high quality data, and get developers around the world excited about addressing this problem in an open way.</p>

<p>Because of the complexity of the problem, a one shot approach won’t work. This is a benchmark for the long run.</p>

<p>If you are interested in providing more annotated data, please contact us.</p>

<p><img src=""https://i.imgur.com/zS2Nbf0.png"" width=""200"" />
<img src=""https://i.imgur.com/39OS2Gp.png"" width=""200"" />
<img src=""https://i.imgur.com/wtbiHSz.png"" width=""200"" /></p>

<h2 id=""available-notebooks"">Available Notebooks</h2>

<ul>
  <li><a href=""https://github.com/AIcrowd/food-recognition-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb"">Dataset Utils</a>
    <ul>
      <li><a href=""https://github.com/AIcrowd/food-recognition-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb#Import-dependencies"">Import Dependencies</a></li>
      <li><a href=""https://github.com/AIcrowd/food-recognition-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb#Configuration-Variables"">Configuration Variables</a></li>
      <li><a href=""https://github.com/AIcrowd/food-recognition-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb#Parsing-the-annotations"">Parsing Annotations</a></li>
      <li><a href=""https://github.com/AIcrowd/food-recognition-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb#Collecting-and-Visualizing-Images"">Collecting and Visualizing Images</a></li>
      <li><a href=""https://github.com/AIcrowd/food-recognition-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb#Understanding-Annotations"">Understanding Annotations</a></li>
      <li><a href=""https://github.com/AIcrowd/food-recognition-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb#Visualizing-Annotations"">Visualizing Annotations</a></li>
      <li><a href=""https://github.com/AIcrowd/food-recognition-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb#Advanced"">Advanced</a>
        <ul>
          <li><a href=""https://github.com/AIcrowd/food-recognition-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb#1.-Convert-poly-segmentation-to-rle"">Convert poly segmentation to rle</a></li>
          <li><a href=""https://github.com/AIcrowd/food-recognition-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb#2.-Convert-segmentation-to-pixel-level-masks"">Convert segmentation to pixel level masks</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href=""https://github.com/AIcrowd/food-recognition-challenge-starter-kit/blob/master/run.py"">Random Submission</a></li>
  <li><a href=""https://github.com/AIcrowd/food-recognition-challenge-starter-kit/blob/master/Local%20Evaluation.ipynb"">Locally test the evaluation function</a></li>
</ul>

<h1 id=""evaluation-criteria"">Evaluation Criteria</h1>

<p>For a known ground truth mask <script type=""math/tex"">A</script>, you propose a mask <script type=""math/tex"">B</script>, then we first compute <script type=""math/tex"">IoU</script> (Intersection Over Union) :</p>

<script type=""math/tex; mode=display"">IoU(A, B) = \frac{A \cap B}{ A \cup B}</script>

<p><script type=""math/tex"">IoU</script> measures the overall overlap between the true region and the proposed region.
Then we consider it a True detection, when there is atleast half an overlap, or when <script type=""math/tex"">IoU \geq 0.5</script></p>

<p>Then we can define the following parameters :</p>

<ul>
  <li>
    <p>Precision (<script type=""math/tex"">IoU \geq 0.5</script>) <br />
<script type=""math/tex"">P_{IoU \geq 0.5} = \frac{TP_{IoU \geq 0.5}}{TP_{IoU \geq 0.5} + FP_{IoU \geq 0.5}}</script></p>
  </li>
  <li>
    <p>Recall (<script type=""math/tex"">IoU  \geq 0.5</script>) <br />
<script type=""math/tex"">R_{IoU \geq 0.5} = \frac{TP_{IoU \geq 0.5}}{TP_{IoU \geq 0.5} + FN_{IoU \geq 0.5}}</script>.</p>
  </li>
</ul>

<p>The final scoring parameters <script type=""math/tex"">AP_{IoU \geq 0.5}</script> and <script type=""math/tex"">AR_{IoU \geq 0.5}</script> are computed by averaging over all the precision and recall values for all known annotations in the ground truth.</p>

<h1 id=""challenge-rounds"">Challenge Rounds</h1>

<p>This is an ongoing, multi-round benchmark. At each round, the specific tasks and / or datasets will be updated, and each round will have its own prizes. You can participate in multiple rounds, or in single rounds.</p>

<h1 id=""prizes"">Prizes</h1>
<p>The winner of the first round will be invited to the Applied Machine Learning Days in Switzerland at EPFL in January 2021. A travel grant of up to $2500 will cover the costs.</p>

<p>Top contributors will also be invited to coauthor a paper on the advances made in this round.</p>

<p>(more details to be announced soon)</p>

<h2 id=""felicity-participants-singularity"">Felicity Participants (Singularity)</h2>

<p>The competition is open for “Singularity” event for all the <a href=""http://felicity.iiit.ac.in/"">Felicity</a> participants. This category has prizes worth 40k INR. The food baseline threshold should be 0.573 to be eligible for prizes. And you need to register yourselves at <a href=""https://docs.google.com/forms/d/e/1FAIpQLScAjK8dJFUx-BWAE2NgsgbC-M1HE-KEErNVl9VKoUkzRJlOCw/viewform"">this Google Form</a>. End date: 15th March</p>

<h1 id=""contact"">Contact</h1>
<ul>
  <li><strong><a href=""mailto:mohanty@aicrowd.com"">Sharada Mohanty</a></strong></li>
</ul>
"
67,"<h1 id=""introduction"">Introduction</h1>

<p>See detailed instructions on the course github, including the PDF project description.</p>

<h1 id=""dataset"">Dataset</h1>

<h2 id=""file-descriptions--"">File descriptions -</h2>

<ul>
  <li>train_pos.txt and train_neg.txt - a small set of training tweets for each of the two classes. (Dataset available in the zip file, see link below)</li>
  <li>train_pos_full.txt and train_neg_full.txt - a complete set of training tweets for each of the two classes, about 1M tweets per class.  (Dataset available in the zip file, see link below)</li>
  <li>test_data.txt - the test set, that is the tweets for which you have to predict the sentiment label.</li>
  <li>sampleSubmission.csv - a sample submission file in the correct format, note that each test tweet is numbered. (submission of predictions: -1 = negative prediction, 1 = positive prediction)</li>
</ul>

<p>Note that all tweets have been tokenized already, so that the words and punctuation are properly separated by a whitespace.
# Evaluation Criteria</p>

<p>Your submission will be evaluated in terms of classification error (accuracy).</p>

<h1 id=""rules"">Rules</h1>

<p>Each participant is allowed to make <strong>5 submissions per day</strong> (i.e. up to 15 submissions per team per day). Failed submissions (e.g. wrong submission file format) do not count.</p>
"
180,"<p><em>Note: Do not forget to read the <strong>Rules</strong> section on this page</em></p>

<p><strong>Update 28/04/2019: All runs results available at : <a href=""https://www.imageclef.org/GeoLifeCLEF2019"">https://www.imageclef.org/GeoLifeCLEF2019</a></strong> <br />
<strong>Update 28/04/2019: Submission deadline extended to the 05/05/2019 at 23:00 (UTC).</strong><br />
<strong>Update 11/04/2019: SUBMISSION IS NOW OPEN! Use the “Create submission” button and /!\ be carefull /!\ to fill the required information BEFORE chosing the file.</strong><br />
<strong>Update 25/04/2019: Added a dataset fusioning all train occurrences (with geographic filter for noPlant occurrences).</strong><br />
<strong>Update 08/04/2019: Added information about runs format on this page.</strong><br />
<strong>Update 05/04/2019: Added the identification of TestSet species in the Table of species IDs and names. See the Dataset tab.</strong> <br />
<strong>Update 26/03/2019: The TEST SET is now available, download it from the Dataset tab</strong>
<strong>The Protocol Note is also available for detailed informations about train and test datasets construction</strong><br /></p>

<h3 id=""motivation"">Motivation</h3>

<p>Automatically predicting the list of species that are the most likely to be observed at a given location is useful for many scenarios in biodiversity informatics. First of all, it could improve species identification processes and tools by reducing the list of candidate species that are observable at a given location (be they automated, semi-automated or based on classical field guides or flora). More generally, it could facilitate biodiversity inventories through the development of location-based recommendation services (typically on mobile phones) as well as the involvement of non-expert nature observers. Last but not least, it might serve educational purposes thanks to biodiversity discovery applications providing functionalities such as contextualized educational pathways.</p>

<h3 id=""challenge-description"">Challenge description</h3>

<p>The aim of the challenge is to predict the list of species that are the most likely to be observed at a given location. Therefore, we will provide a large training set of species occurrences, each occurrence being associated to a multi-channel image characterizing the local environment. Indeed, it is usually not possible to learn a species distribution model directly from spatial positions because of the limited number of occurrences and the sampling bias. What is usually done in ecology is to predict the distribution on the basis of a representation in the environmental space, typically a feature vector composed of climatic variables (average temperature at that location, precipitation, etc.) and other variables such as soil type, land cover, distance to water, etc. The originality of GeoLifeCLEF is to generalize such niche modeling approach to the use of an image-based environmental representation space. Instead of learning a model from environmental feature vectors, the goal of the task will be to learn a model from k-dimensional image patches, each patch representing the value of an environmental variable in the neighborhood of the occurrence (see figure below for an illustration). From a machine learning point of view, the challenge will thus be treatable as an image classification task. Participants will learn their models on a train set constituted of valid and uncertain citizen sciences occurrences. They will then try to predict the most likely species on an independent test set made up of expert plant occurrences with accurate identification and spatial location over very diverse biotic areas in France: The Mediterranean and Alpine regions.</p>

<p><img src=""https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/05115f1f63ee37eb96761cd0b40744a2_figure-1.png"" alt=""  "" /></p>

<h3 id=""data"">Data</h3>

<p><strong>Train &amp; Test data downloadable on the “Dataset” tab.</strong></p>

<p><strong>Check out the Protocol Note for detailed informations about the dataset construction and Python scripts at <a href=""https://github.com/maximiliense/GLC19"">https://github.com/maximiliense/GLC19</a></strong> to simplify formatting of the dataset for the learning process.</p>

<p>This year, the train dataset is augmented compared to the 2018 edition. In a nutshell, it will first include 280,945 train and test georeferenced occurrences of plant species from last year (file <strong>GLC_2018.csv</strong>). Plus, 2,367,145 plant species occurrences with uncertain identifications are added (file <strong>PL_complete.csv</strong>). They come from automatic species identification of pictures produced in 2017-2018 by the smartphone application Pl@ntNet, where users are mainly amators botanists. A trusted extraction of this dataset is also provided (file <strong>PL_trusted.csv</strong>), insuring a reasonable level of identification certainty. Finally, 10,618,839 species occurrences from other kingdoms (as mammals, birds, amphibias, insects, fungis etc.) were selected from the GBIF database (file <strong>noPlant.csv</strong>). 33 environmental rasters (directory <strong>rasters GLC19/</strong>) covering the French territory are made available this year, so that each occurrence may be linked to an environmental tensor via a participant customizable Python code. These environmental rasters were constructed from various open datasets including Chelsea Climate [1], ESDB soil pedology data [2,3,4], Corine Land Cover 2012 soil occupation data, CGIAR-CSI evapotranspiration data [5,6], USGS Elevation data (Data available from the U.S. Geological Survey.) and BD Carthage hydrologic data.</p>

<p>The test occurrences data come from independents datasets of the French National Botanical Conservatories. This TestSet includes 844 plant species. It is a subset of those found in the train set. Those species are indicated in the column “test” of the Table of species IDs and names and identification of TestSet species, downloadable on the Dataset tab. A detailed description of the protocol used to build the datasets is available in the <strong>Protocol_Note</strong>, download from the “Dataset” tab.</p>

<h3 id=""submission-instructions"">Submission instructions</h3>

<p><strong>Submission is open!</strong></p>

<p>Each team is allowed to submit 20 runs maximum. A run is a .csv file with 4 columns separated by “;” and containing in this order : glc19TestOccId ; glc19SpId ; Rank ; Probability <br />
Here is an example of the 5 first lines of a run file :</p>

<p>1 ; 10 ; 1 ; 0.5<br />
1 ; 25 ; 2 ; 0.3<br />
1 ; 301 ; 3 ; 0.2<br />
2 ; 34 ; 1 ; 0.9<br />
2 ; 41 ; 2 ; 0.1<br /></p>

<p><strong>Please watch your runs format</strong>. The 1st, 2nd and 3rd columns (respectively glc19TestOccId, glc19SpId and Rank) should be integers, while the last column probability is a float. One can give up to 50 species (glc19SpId) for an occurrence ID (glc19TestOccId), which must be distinct and their ranks must be strictly consecutive starting from 1. Each occurrence ID in the submitted run must exist in the testSet file (glc19TestOccId). Each species ID must match be one the species (glc19SpId) marked as TRUE in the column “test” of the <strong>Table of  species Ids and names and identification of test set species</strong>.</p>

<p>WARNING: Any run inducing an error is NOT counted for the limit , except above 30 faulty runs, where it will count as a valid run. Exemples: <br />
- if a participant submits 30 faulty runs, he can still submit 20 more runs<br />
- if a participant submits 32 faulty runs, he can submit 18 more runs<br />
- if a participant submits 10 successful runs, he can submit 10 more runs<br />
- if a participant submits 7 faulty runs and 20 successful ones, he can submit 0 more runs <br /></p>

<p>WARNING: There is no leaderboard while submission is open for this task. We removed it to maximize the independence between submitted algorithms and test data, and thus significance of results for future research purposes.</p>

<h3 id=""citations"">Citations</h3>

<p>Information will be posted after the challenge ends.</p>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>
<p>The main evaluation criteria will be the accuracy based on the 30 first answers, also called Top30. It is the mean of the function scoring 1 when the good species is in the 30 first answers, and 0 otherwise, over all test set occurrences. This metric has been carefully chosen for this challenge because it account for the known scientific fact that some tens of plant species usually coexist in the perimeter of the geolocation uncertainty of the occurrences. The Mean Reciprocal Rank was chosen as secondary metric for enabling comparison with the 2018 edition.</p>

<h3 id=""resources"">Resources</h3>

<h3 id=""contact-us"">Contact us</h3>

<ul>
  <li>Discussion Forum : <a href=""https://www.crowdai.org/challenges/lifeclef-2019-geo/topics"" target=""_blank""> https://www.crowdai.org/challenges/lifeclef-2019-geo/topics </a></li>
</ul>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

<p>@Organisers:</p>

<ul>
  <li>
    <p>Maximilien SERVAJEAN, Maximilien.Servajean@lirmm.fr</p>
  </li>
  <li>
    <p>Christophe BOTELLA, christophe.botella@inria.fr</p>
  </li>
  <li>
    <p>Alexis JOLY, alexis.joly@inria.fr</p>
  </li>
</ul>

<h3 id=""more-information"">More information</h3>

<p>You can find additional information on the challenge here:
<a href=""https://www.imageclef.org/GeoLifeCLEF2019"" target=""_blank"">https://www.imageclef.org/GeoLifeCLEF2019 </a></p>

<h3 id=""prizes"">Prizes</h3>

<p>LifeCLEF 2019 is an evaluation campaign that is being organized as part of the <a href=""http://clef2019.clef-initiative.eu/"" target=""_blank""> CLEF initiative </a> labs. The campaign offers several research tasks that welcome participation from teams around the world. The results of the campaign appear in the working notes proceedings, published by CEUR Workshop Proceedings (CEUR-WS.org). Selected contributions among the participants, will be invited for publication in the following year in the Springer Lecture Notes in Computer Science (LNCS) together with the annual lab overviews.</p>

<h3 id=""datasets-license"">Datasets License</h3>

"
16,"<p><strong>NEWS:</strong> Deadlines <a href=""http://www.cs.ox.ac.uk/isg/challenges/sem-tab/"">updated</a>. Please join our <a href=""https://groups.google.com/d/forum/sem-tab-challenge"">discussion group</a>.</p>

<p>This is a task of <a href=""https://iswc2019.semanticweb.org/challenges/"">ISWC 2019</a> “Semantic Web Challenge on Tabular Data to Knowledge Graph Matching”. The task is to annotate a column pair within a table with a property of DBPedia Ontology. <a href=""http://www.cs.ox.ac.uk/isg/challenges/sem-tab/"">Click here</a> for the official challenge website.</p>

<h1 id=""task-description"">Task Description</h1>

<p>Each submission should be one CSV file. Each line should contain one property annotation for one column pair which is identified by a table id, a head column id and a tail column id. Note that the order of head column and tail column matters. The annotation properties should come DBPedia with the prefix of http://dbpedia.org/ontology/. Each column pair should be annotated by one property that is as fine grained as possible but correct. Case is NOT sensitive.</p>

<p>Briefly each line of the submission file should include “table ID”, “head column ID”, “tail column ID”, and “DBpedia property”. The header should be excluded from the submission file. Here is one line example:
“50245608_0_871275842592178099”,”0”,”1”,”http://dbpedia.org/ontology/releaseDate”</p>

<p>Notes:</p>

<p>1) Table ID does not include filename extension; make sure you remove the .csv extension from the filename.</p>

<p>2) Column ID is the position of the column in the table file, starting from 0, i.e., first column’s ID is 0.</p>

<p>3) At most one property should be annotated for one column pair.</p>

<p>4) One submission file should have NO duplicate lines (annotations) for  one column pair.</p>

<p>6) Annotations for column pairs out of the targets are ignored.</p>

<h1 id=""datasets"">Datasets</h1>

<p>Table set for Round #1: <a href=""https://www.cs.ox.ac.uk/isg/challenges/sem-tab/data/CPA_Round1.tar.gz"">CPA_Round1.tar.gz</a>.</p>

<p>Table set for Round #2: <a href=""https://www.cs.ox.ac.uk/isg/challenges/sem-tab/data/Tables_Round2.tar.gz"">Tables</a>, <a href=""https://www.cs.ox.ac.uk/isg/challenges/sem-tab/data/CPA_Round2_Targets.csv"">Target Column Pairs</a></p>

<p>Table set for Round #3: <a href=""https://www.cs.ox.ac.uk/isg/challenges/sem-tab/data/Tables_Round3.tar.gz"">Tables</a>, <a href=""https://www.cs.ox.ac.uk/isg/challenges/sem-tab/data/CPA_Round3_Targets.csv"">Target Column Pairs</a></p>

<p>Data Description: One table is stored in one CSV file. Each line corresponds to a table row. Note that the first row may either be the table header or content. The column pairs for annotation are saved in a CSV file.</p>

<h1 id=""evaluation-criteria"">Evaluation Criteria</h1>

<p>Precision, Recall and F1 Score will be calculated:</p>

<p>Precision = (# correct annotations) / (# annotations)</p>

<p>Recall = (# correct annotations) / (# target column pairs)</p>

<p>F1 Score = (2 * Precision * Recall) / (Precision + Recall)</p>

<p>Notes:</p>

<p>1) # denotes the number.</p>

<p>2) F1 Score is used as the primary score; Precision is used as the secondary score.</p>

<p>3) An empty annotation of a column pair will lead to an annotated cell; we suggest to exclude the cell with empty annotation in the submission file.</p>

<h1 id=""prizes"">Prizes</h1>

<p>SIRIUS sponsors the prize for the best systems</p>

<h1 id=""rules"">Rules</h1>

<ol>
  <li>
    <p>Selected systems with the best results in Round 1 and 2 will be invited to present their results during the <a href=""https://iswc2019.semanticweb.org/"">ISWC conference</a> and the <a href=""http://om2019.ontologymatching.org/"">Ontology Matching workshop</a>.</p>
  </li>
  <li>
    <p>The prize winners will be announced during the <a href=""https://iswc2019.semanticweb.org/"">ISWC conference</a> (on October 30, 2019). We will take into account all evaluation rounds specially the ones running till the conference dates.</p>
  </li>
  <li>
    <p>Participants are encouraged to submit a system paper describing their tool and the obtained results. Papers will be published online as a volume of <a href=""http://ceur-ws.org/"">CEUR-WS</a> as well as indexed on <a href=""https://dblp.uni-trier.de/"">DBLP</a>. By submitting a paper, the authors accept the CEUR-WS and DBLP publishing rules.</p>
  </li>
  <li>
    <p>Please see additional information at our <a href=""http://www.cs.ox.ac.uk/isg/challenges/sem-tab/"">official website</a></p>
  </li>
</ol>
"
174,"
<h3 id=""evaluation-criteria"">Evaluation criteria</h3>

<h3 id=""resources"">Resources</h3>

<h3 id=""prizes"">Prizes</h3>

<h3 id=""datasets-license"">Datasets License</h3>

"
26,"<h1 class=""mt-0"" id=""introduction"">Introduction</h1>

<p>We depend on edible plants just as we depend on oxygen. Without crops, there is no food, and without food, there is no life. It’s no accident that human civilization began to thrive with the invention of agriculture.</p>

<p>Today, modern technology allows us to grow crops in quantities necessary for a steady food supply for billions of people. But diseases remain a major threat to this supply, and a large fraction of crops are lost each year to diseases. The situation is particularly dire for the 500 million smallholder farmers around the globe, whose livelihoods depend on their crops doing well. In Africa alone, 80% of the agricultural output comes from smallholder farmers.</p>

<p>With billions of smartphones around the globe, wouldn’t it be great if the smartphone could be turned into a disease diagnostics tool, recognizing diseases from images it captures with its camera? This challenge is the first of many steps turning this vision into a reality. PlantVillage is a not-for-profit project by Penn State University in the US and EPFL in Switzerland. We have collected - and continue to collect - tens of thousands of images of diseased and healthy crops. The goal of this challenge is to develop algorithms than can accurately diagnose a disease based on an image.</p>

<p>Here are the 38 classes of crop disease pairs that the dataset is offering:</p>

<p><img src=""https://s3.amazonaws.com/salathegroup-static/plantvillage/plantvillage-min.png"" alt="""" /></p>

<p>To learn more about the background of the dataset, please refer to the following paper: http://arxiv.org/abs/1511.08060. You must cite this paper if you use the dataset.</p>

<h1 id=""evaluation-criteria"">Evaluation Criteria</h1>

<p>Submissions will be evaluated using a Multi Class Log Loss evaluation function, which are defined as :</p>

<h2 id=""mean-f1-score"">Mean F1 score</h2>

<p>The F1 score is computed separately for all classes by using:</p>

<p><img src=""https://s3.amazonaws.com/salathegroup-static/plantvillage/mean_f1.png"" alt="""" class=""img-medium"" /></p>

<ul>
  <li><strong><em>p</em></strong> refers to the precision</li>
  <li><strong><em>r</em></strong> refers to the recall</li>
  <li><strong><em>tp</em></strong> refers to the number of True Positives,</li>
  <li><strong><em>fp</em></strong> refers to the number of False Positives</li>
  <li><strong><em>fn</em></strong> refers to the number of False Negatives</li>
</ul>

<p>Then finally the Mean of all the F1 scores across all the classes is used for come up with the combined Mean F1 score.</p>

<h2 id=""mean-log-loss"">Mean Log Loss</h2>

<p><img src=""https://s3.amazonaws.com/salathegroup-static/plantvillage/mean+log+loss.png"" alt="""" class=""img-medium"" /></p>

<ul>
  <li><strong><em>N</em></strong> is the total number of examples in the test set</li>
  <li><strong><em>M</em></strong> is the total number of class labels (38 for  this challenge)</li>
  <li><strong><em>y ij</em></strong> is a boolean value representing if the i-th instance in the test set belongs to the j-th label.</li>
  <li><strong><em>p ij</em></strong> is the probability according to your submission that the i-th instance may belong to the j-th label.</li>
  <li><strong><em>Ln</em></strong> is the natural logarithmic function.</li>
</ul>

<p>All submissions will be evaluated on the test dataset in the docker containers referenced in the Resources section. The code archive will be uncompressed into the <code class=""highlighter-rouge"">/plantvillage</code> path, and every code archive is expected to contain a <code class=""highlighter-rouge"">main.sh</code> script which takes path to a folder containing images as its first parameter. So to test your code submission, we will finally execute :</p>

<p><code class=""highlighter-rouge"">/plantvillage/main.sh pathToFolderContainingTestImages</code></p>

<p>This is expected to output a CSV file containing the name of the file, and the associated probabilities for all the classes at the location :</p>

<p><code class=""highlighter-rouge"">/plantvillage/classification.csv</code></p>

<h1 id=""rules"">Rules</h1>
<ul>
  <li>
    <p>Participants are allowed a maximum of five submissions each 24 hours.</p>
  </li>
  <li>
    <p>Use of any external datasets (or any pre-trained trained models) in any form is not allowed.</p>
  </li>
  <li>
    <p>All images are released under the Creative Commons Attribution-ShareAlike 3.0 Unported (CC BY-SA 3.0) license, with the clarification that algorithms trained on the data fall under the same license.</p>
  </li>
  <li>
    <p>In order to be eligible for the winner’s prize, you must release the source code used to generate the winning submission on a public GitHub repository, licensed under the Creative Commons Attribution-ShareAlike 3.0 Unported license.</p>
  </li>
  <li>
    <p>crowdAI reserves the right to modify challenge rules as required.</p>
  </li>
</ul>

<h1 id=""prizes"">Prizes</h1>
<p>The author of the most highly ranked submission will be invited to the <strong>crowdAI winner’s symposium</strong> at EPFL in Switzerland on January 30/31, 2017. The educational award is given to the participant with the either the most insightful submission posts, or the best tutorial - the recipient of this award will also be invited to the symposium (the crowdAI team will pick the recipient of this award). Expenses for travel and accommodation are covered by crowdAI.</p>

<h1 id=""resources"">Resources</h1>

<p>References to Docker Containers where the submissions will be tested ::</p>

<p><strong>Caffe</strong>  : https://hub.docker.com/r/tleyden5iwx/caffe-gpu-master/</p>

<p><strong>Tensorflow</strong> : https://hub.docker.com/r/tensorflow/tensorflow/</p>

<p><strong>Torch7</strong> : https://hub.docker.com/r/kaixhin/cuda-torch/</p>

<p><strong>Scikit-Learn</strong> :(Python-2): https://github.com/dataquestio/ds-containers/tree/master/python2</p>

<p><strong>Scikit-Learn</strong> : (Python-3): https://github.com/dataquestio/ds-containers/tree/master/python3</p>

<p><strong>Octave</strong> : https://hub.docker.com/r/schickling/octave/</p>

<p><strong>Keras</strong> :  https://hub.docker.com/r/patdiscvrd/keras/~/dockerfile/</p>

<p>Feel free to shoot us an email if you want to be able to submit code in your favorite language or framework :D We would be happy to help :)</p>

<blockquote>
  <p>Update: This challenge was migrated from CrowdAI</p>
</blockquote>
"
66,"<p>See detailed instruction see also the <a href=""https://github.com/epfml/ML_course/blob/master/projects/project1/project1_description.pdf"">Project 1 PDF description</a> available on the <a href=""https://mlo.epfl.ch/page-146520.html"">ML course web site</a>.</p>

<h2 id=""file-descriptions"">File descriptions</h2>

<p><strong>train.csv</strong> - Training set of 250000 events. The file starts with the ID column, then the label column (the y you have to predict), and finally 30 feature columns.  <br />
<strong>test.csv</strong> - The test set of around 568238 events - Everything as above, except the label is missing.  <br />
<strong>sample-submission.csv</strong> - a sample submission file in the correct format. The sample submission always predicts -1, that is ‘background’.</p>

<p>Zip file containing all 3 above files can be downloaded from the resource section.</p>

<p>For detailed information on the semantics of the features, labels, and weights, see the technical documentation from the LAL website on the task. Note that here for the EPFL course, we use a simpler evaluation metric instead (classification error).</p>

<p><strong>Some details to get started:</strong></p>

<ul>
  <li>all variables are floating point, except PRI_jet_num which is integer</li>
  <li>variables prefixed with PRI (for PRImitives) are “raw” quantities about the bunch collision as measured by the detector.</li>
  <li>variables prefixed with DER (for DERived) are quantities computed from the primitive features, which were selected by the physicists of ATLAS.</li>
  <li>it can happen that for some entries some variables are meaningless or cannot be computed; in this case, their value is −999.0, which is outside the normal range of all variables.</li>
</ul>

"
141,"<p>We depend on edible plants just as we depend on oxygen. Without crops, there is no food, and without food, there is no life. It’s no accident that human civilization began to thrive with the invention of agriculture.</p>

<p>Today, modern technology allows us to grow crops in quantities necessary for a steady food supply for billions of people. But diseases remain a major threat to this supply, and a large fraction of crops are lost each year to diseases. The situation is particularly dire for the 500 million smallholder farmers around the globe, whose livelihoods depend on their crops doing well. In Africa alone, 80% of the agricultural output comes from smallholder farmers.</p>

<p>With billions of smartphones around the globe, wouldn’t it be great if the smartphone could be turned into a disease diagnostics tool, recognizing diseases from images it captures with its camera? This challenge is the first of many steps turning this vision into a reality. PlantVillage is a not-for-profit project by Penn State University in the US and EPFL in Switzerland. We have collected - and continue to collect - tens of thousands of images of diseased and healthy crops. The goal of this challenge is to develop algorithms than can accurately diagnose a disease based on an image.</p>

<p>Here are the 38 classes of crop disease pairs that the dataset is offering:</p>

<p><img src=""https://s3.amazonaws.com/salathegroup-static/plantvillage/plantvillage-min.png"" alt="""" /></p>

<p>To learn more about the background of the dataset, please refer to the following paper: http://arxiv.org/abs/1511.08060. You must cite this paper if you use the dataset.</p>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>
<p>Submissions will be evaluated using a Multi Class Log Loss evaluation function, which are defined as :</p>

<h3 id=""mean-f1-score"">Mean F1 score</h3>

<p>The F1 score is computed separately for all classes by using:</p>

<p><img src=""https://s3.amazonaws.com/salathegroup-static/plantvillage/mean_f1.png"" alt="""" class=""img-medium"" /></p>

<ul>
  <li><strong><em>p</em></strong> refers to the precision</li>
  <li><strong><em>r</em></strong> refers to the recall</li>
  <li><strong><em>tp</em></strong> refers to the number of True Positives,</li>
  <li><strong><em>fp</em></strong> refers to the number of False Positives</li>
  <li><strong><em>fn</em></strong> refers to the number of False Negatives</li>
</ul>

<p>Then finally the Mean of all the F1 scores across all the classes is used for come up with the combined Mean F1 score.</p>

<h3 id=""mean-log-loss"">Mean Log Loss</h3>

<p><img src=""https://s3.amazonaws.com/salathegroup-static/plantvillage/mean+log+loss.png"" alt="""" class=""img-medium"" /></p>

<ul>
  <li><strong><em>N</em></strong> is the total number of examples in the test set</li>
  <li><strong><em>M</em></strong> is the total number of class labels (38 for  this challenge)</li>
  <li><strong><em>y ij</em></strong> is a boolean value representing if the i-th instance in the test set belongs to the j-th label.</li>
  <li><strong><em>p ij</em></strong> is the probability according to your submission that the i-th instance may belong to the j-th label.</li>
  <li><strong><em>Ln</em></strong> is the natural logarithmic function.</li>
</ul>

<p>All submissions will be evaluated on the test dataset in the docker containers referenced in the Resources section. The code archive will be uncompressed into the <code class=""highlighter-rouge"">/plantvillage</code> path, and every code archive is expected to contain a <code class=""highlighter-rouge"">main.sh</code> script which takes path to a folder containing images as its first parameter. So to test your code submission, we will finally execute :</p>

<p><code class=""highlighter-rouge"">/plantvillage/main.sh pathToFolderContainingTestImages</code></p>

<p>This is expected to output a CSV file containing the name of the file, and the associated probabilities for all the classes at the location :</p>

<p><code class=""highlighter-rouge"">/plantvillage/classification.csv</code></p>

<h3 id=""resources"">Resources</h3>

<p>References to Docker Containers where the submissions will be tested ::</p>

<p><strong>Caffe</strong>  : https://hub.docker.com/r/tleyden5iwx/caffe-gpu-master/
<strong>Tensorflow</strong> : https://hub.docker.com/r/tensorflow/tensorflow/
<strong>Torch7</strong> : https://hub.docker.com/r/kaixhin/cuda-torch/
<strong>Scikit-Learn</strong> :(Python-2): https://github.com/dataquestio/ds-containers/tree/master/python2
<strong>Scikit-Learn</strong> : (Python-3): https://github.com/dataquestio/ds-containers/tree/master/python3
<strong>Octave</strong> : https://hub.docker.com/r/schickling/octave/
<strong>Keras</strong> :  https://hub.docker.com/r/patdiscvrd/keras/~/dockerfile/</p>

<p>Feel free to shoot us an email if you want to be able to submit code in your favourite language or framework :D We would be happy to help :)</p>

<h3 id=""prizes"">Prizes</h3>

<p>The author of the most highly ranked submission will be invited to the <strong>crowdAI winner’s symposium</strong> at EPFL in Switzerland on January 30/31, 2017. The educational award is given to the participant with the either the most insightful submission posts, or the best tutorial - the recipient of this award will also be invited to the symposium (the crowdAI team will pick the recipient of this award). Expenses for travel and accommodation are covered by crowdAI.</p>

<h3 id=""datasets-license"">Datasets License</h3>

<p>All images are released under the Creative Commons Attribution-ShareAlike 3.0 Unported (CC BY-SA 3.0), with the clarification that algorithms trained on the data fall under the same license.</p>

"
143,"<p>In the OpenFood project, labels are being scanned in Swiss supermarkets for all foods available for sale. A database of images of food packages has been prepared, and now nutritional information needs to be extracted from the images into a CSV file.</p>

<p>Nutritional data is presented in tables on food packaging, and nutritional tables contain at a minimum:</p>

<ul>
  <li>Nutrient</li>
  <li>Units per 100g</li>
</ul>

<p>Optionally additional columns may also be presented:</p>

<ul>
  <li>Units per serving size</li>
  <li>Percentage of recommended daily intake</li>
  <li>Other optional columns</li>
</ul>

<p>The nutritional data needs to be extracted into a CSV file which will then be loaded into the OpenFood database.</p>

<h3 id=""nutrients-master-list"">Nutrients Master List</h3>

<p>Nutrients will be one of the following items in the list below, and using the standard unit of measure as indicated. The nutritional information may be in upper, lower or sentence case, and presented in:</p>

<ul>
  <li>German (de)</li>
  <li>French (fr)</li>
  <li>Italian (it)</li>
  <li>English (en)</li>
</ul>

<p>Note that information may be presented in multiple languages. The challenge submission should refer to the nutrients via an integer field as indicated in the <strong>nutrient_id</strong> field in the list below.</p>

<table>
  <thead>
    <tr>
      <th>nutrient_id</th>
      <th style=""text-align: left"">Nutrient by language</th>
      <th>Unit</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td style=""text-align: left"">de=&gt;Energie en=&gt;energy fr=&gt;énergie it=&gt;energia</td>
      <td>kJ</td>
    </tr>
    <tr>
      <td>2</td>
      <td style=""text-align: left"">de=&gt;Energie (kCal) en=&gt;energy (kCal) fr=&gt;énergie (kCal) it=&gt;energia (kCal)</td>
      <td>kCal</td>
    </tr>
    <tr>
      <td>3</td>
      <td style=""text-align: left"">de=&gt;Eiweiss en=&gt;protein fr=&gt;protéines it=&gt;proteine</td>
      <td>g</td>
    </tr>
    <tr>
      <td>4</td>
      <td style=""text-align: left"">de=&gt;Fett en=&gt;fat fr=&gt;graisses it=&gt;grassi</td>
      <td>g</td>
    </tr>
    <tr>
      <td>5</td>
      <td style=""text-align: left"">de=&gt;Kohlenhydrate en=&gt;carbohydrates fr=&gt;glucides it=&gt;carboidrati</td>
      <td>g</td>
    </tr>
    <tr>
      <td>6</td>
      <td style=""text-align: left"">de=&gt;Zucker en=&gt;sugars fr=&gt;sucres it=&gt;zuccheri</td>
      <td>g</td>
    </tr>
    <tr>
      <td>7</td>
      <td style=""text-align: left"">de=&gt;Salz en=&gt;salt fr=&gt;sel it=&gt;sale</td>
      <td>g</td>
    </tr>
    <tr>
      <td>8</td>
      <td style=""text-align: left"">de=&gt;Ballaststoffe en=&gt;fibre fr=&gt;fibres alimentaires it=&gt;fibre</td>
      <td>g</td>
    </tr>
    <tr>
      <td>9</td>
      <td style=""text-align: left"">de=&gt;Gesättigte Fettsäuren en=&gt;saturated fat fr=&gt;graisses saturées it=&gt;grassi saturi</td>
      <td>g</td>
    </tr>
    <tr>
      <td>10</td>
      <td style=""text-align: left"">de=&gt;Vitamin C en=&gt;Vitamin C fr=&gt;Vitamine C it=&gt;Vitamina C</td>
      <td>mg</td>
    </tr>
    <tr>
      <td>11</td>
      <td style=""text-align: left"">de=&gt;Vitamin B2 (Riboflavin) en=&gt;Vitamin B2 (Riboflavin) fr=&gt;Vitamine B2 (Riboflavine) it=&gt;Vitamina B2 (Riboflavin)</td>
      <td>mg</td>
    </tr>
    <tr>
      <td>12</td>
      <td style=""text-align: left"">de=&gt;Natrium en=&gt;Sodium fr=&gt;Sodium it=&gt;Sodio</td>
      <td>g</td>
    </tr>
    <tr>
      <td>13</td>
      <td style=""text-align: left"">de=&gt;Selen en=&gt;Selenium fr=&gt;Selenium it=&gt;Selenio</td>
      <td>µg</td>
    </tr>
    <tr>
      <td>14</td>
      <td style=""text-align: left"">de=&gt;Vitamin E en=&gt;Vitamin E fr=&gt;Vitamine E it=&gt;Vitamina E</td>
      <td>mg</td>
    </tr>
    <tr>
      <td>15</td>
      <td style=""text-align: left"">de=&gt;Kalzium en=&gt;Calcium fr=&gt;Calcium it=&gt;calcio</td>
      <td>mg</td>
    </tr>
    <tr>
      <td>16</td>
      <td style=""text-align: left"">de=&gt;Magnesium en=&gt;Magnesium fr=&gt;Magnesium it=&gt;Magnesio</td>
      <td>mg</td>
    </tr>
  </tbody>
</table>

<h3 id=""processing"">Processing</h3>

<p>A set of images will be provided, with a corresponding CSV file containing all product information which has been manually extracted. The submitted code will process the images and write the values into a single CSV file called <strong>product_nutrients.csv</strong>.</p>

<p>Participants will submit both the results and the model. A set of additional images will be processed against the model for final scoring of the submission.</p>

<h3 id=""submission"">Submission</h3>

<ul>
  <li>
    <p>Submissions will be run by crowdAI against a Docker container. Details may be found in the <a href=""https://www.crowdai.org/challenges/3#resources"">Resources</a> section.</p>
  </li>
  <li>
    <p>The <strong>/project</strong> folder will hold the submission scripts, the install and run scripts (see below)</p>
  </li>
  <li>
    <p>Participants may optionally submit an installation script an <code class=""highlighter-rouge"">install.sh</code>. When executed in the container this will download and install any necessary code and libraries.</p>
  </li>
  <li>
    <p>If an external API is used it must be indicated on the <strong>submission page</strong>. A list of APIs may be found in the <a href=""https://www.crowdai.org/challenges/3#resources"">Resources</a> section.</p>
  </li>
  <li>
    <p>Participants must include a script called <code class=""highlighter-rouge"">run.sh</code> which is executed against a folder of images in the <strong>/project</strong> folder. It will produce a single <strong>product_nutrients.csv</strong> file as output, also in the <strong>/project</strong> folder.</p>
  </li>
  <li>
    <p>Participant code must search for each nutrient type and record the value in the CSV file. If a nutrient is not specified on the package, the field is completed with ‘-1.0’</p>
  </li>
</ul>

<h3 id=""examples"">Examples</h3>

<p>Listed below are examples of Nutritional data raw images and the extracted raw data (the data should be submitted as a single CSV file for all products, but has been presented as a table for illustration purposes).</p>

<h4 id=""product-7207"">Product 7207</h4>

<p>This product is an example of a label where a single column of data only is available. In this case the data falls into the <strong>per_hundred</strong> column, as per the example below.</p>

<p><img src=""https://s3.amazonaws.com/salathegroup-static/openfood/product_7207_raw.png"" alt="""" /></p>

<table>
  <thead>
    <tr>
      <th>product_id</th>
      <th>nutrition_id</th>
      <th>per_hundred</th>
      <th>per_portion</th>
      <th>percent</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>7207</td>
      <td>1</td>
      <td>1180.0</td>
      <td>-1.0</td>
      <td>-1.0</td>
    </tr>
    <tr>
      <td>7207</td>
      <td>2</td>
      <td>282.0</td>
      <td>-1.0</td>
      <td>-1.0</td>
    </tr>
    <tr>
      <td>7207</td>
      <td>3</td>
      <td>14.0</td>
      <td>-1.0</td>
      <td>-1.0</td>
    </tr>
    <tr>
      <td>7207</td>
      <td>4</td>
      <td>25.0</td>
      <td>-1.0</td>
      <td>-1.0</td>
    </tr>
    <tr>
      <td>7207</td>
      <td>5</td>
      <td>1.0</td>
      <td>-1.0</td>
      <td>-1.0</td>
    </tr>
    <tr>
      <td>7207</td>
      <td>7</td>
      <td>1.7</td>
      <td>-1.0</td>
      <td>-1.0</td>
    </tr>
    <tr>
      <td>7207</td>
      <td>6</td>
      <td>-1.0</td>
      <td>-1.0</td>
      <td>-1.0</td>
    </tr>
    <tr>
      <td>7207</td>
      <td>8</td>
      <td>-1.0</td>
      <td>-1.0</td>
      <td>-1.0</td>
    </tr>
    <tr>
      <td>7207</td>
      <td>9</td>
      <td>-1.0</td>
      <td>-1.0</td>
      <td>-1.0</td>
    </tr>
    <tr>
      <td>7207</td>
      <td>10</td>
      <td>-1.0</td>
      <td>-1.0</td>
      <td>-1.0</td>
    </tr>
    <tr>
      <td>7207</td>
      <td>11</td>
      <td>-1.0</td>
      <td>-1.0</td>
      <td>-1.0</td>
    </tr>
    <tr>
      <td>7207</td>
      <td>12</td>
      <td>-1.0</td>
      <td>-1.0</td>
      <td>-1.0</td>
    </tr>
    <tr>
      <td>7207</td>
      <td>13</td>
      <td>-1.0</td>
      <td>-1.0</td>
      <td>-1.0</td>
    </tr>
    <tr>
      <td>7207</td>
      <td>14</td>
      <td>-1.0</td>
      <td>-1.0</td>
      <td>-1.0</td>
    </tr>
    <tr>
      <td>7207</td>
      <td>15</td>
      <td>-1.0</td>
      <td>-1.0</td>
      <td>-1.0</td>
    </tr>
    <tr>
      <td>7207</td>
      <td>16</td>
      <td>-1.0</td>
      <td>-1.0</td>
      <td>-1.0</td>
    </tr>
  </tbody>
</table>

<h4 id=""product-7276"">Product 7276</h4>

<p>This product has a serving site (45g) as well as a percentage. These are columns 3 and 4 in the example below. Sometimes there is an additional column of data, which may be ignored for the purposes of this challenge.</p>

<p><img src=""https://s3.amazonaws.com/salathegroup-static/openfood/product_7276_raw.png"" alt="""" /></p>

<table>
  <thead>
    <tr>
      <th>product_id</th>
      <th>nutrition_id</th>
      <th>per_hundred</th>
      <th>per_portion</th>
      <th>percent</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>7276</td>
      <td>1</td>
      <td>1720.0</td>
      <td>774.0</td>
      <td>9.0</td>
    </tr>
    <tr>
      <td>7276</td>
      <td>2</td>
      <td>410.0</td>
      <td>185.0</td>
      <td>9.0</td>
    </tr>
    <tr>
      <td>7276</td>
      <td>3</td>
      <td>7.2</td>
      <td>3.24</td>
      <td>6.0</td>
    </tr>
    <tr>
      <td>7276</td>
      <td>4</td>
      <td>19.8</td>
      <td>8.91</td>
      <td>12.0</td>
    </tr>
    <tr>
      <td>7276</td>
      <td>5</td>
      <td>49.2</td>
      <td>22.14</td>
      <td>8.0</td>
    </tr>
    <tr>
      <td>7276</td>
      <td>6</td>
      <td>14.7</td>
      <td>6.62</td>
      <td>7.0</td>
    </tr>
    <tr>
      <td>7276</td>
      <td>7</td>
      <td>1.05</td>
      <td>0.47</td>
      <td>7.0</td>
    </tr>
    <tr>
      <td>7276</td>
      <td>9</td>
      <td>10.8</td>
      <td>4.86</td>
      <td>24.0</td>
    </tr>
    <tr>
      <td>7276</td>
      <td>8</td>
      <td>-1.0</td>
      <td>-1.0</td>
      <td>-1.0</td>
    </tr>
    <tr>
      <td>7276</td>
      <td>10</td>
      <td>-1.0</td>
      <td>-1.0</td>
      <td>-1.0</td>
    </tr>
    <tr>
      <td>7276</td>
      <td>11</td>
      <td>-1.0</td>
      <td>-1.0</td>
      <td>-1.0</td>
    </tr>
    <tr>
      <td>7276</td>
      <td>12</td>
      <td>-1.0</td>
      <td>-1.0</td>
      <td>-1.0</td>
    </tr>
    <tr>
      <td>7276</td>
      <td>13</td>
      <td>-1.0</td>
      <td>-1.0</td>
      <td>-1.0</td>
    </tr>
    <tr>
      <td>7276</td>
      <td>14</td>
      <td>-1.0</td>
      <td>-1.0</td>
      <td>-1.0</td>
    </tr>
    <tr>
      <td>7276</td>
      <td>15</td>
      <td>-1.0</td>
      <td>-1.0</td>
      <td>-1.0</td>
    </tr>
    <tr>
      <td>7276</td>
      <td>16</td>
      <td>-1.0</td>
      <td>-1.0</td>
      <td>-1.0</td>
    </tr>
  </tbody>
</table>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>
<ul>
  <li>% of correct fields per product averaged across all products.</li>
  <li>A submission must have a grade of 80% or above to be eligible for prizes.</li>
</ul>

<h3 id=""resources"">Resources</h3>

<h3 id=""docker-container"">Docker Container</h3>

<p>The <a href=""https://hub.docker.com/r/jupyter/scipy-notebook"">Jupyter Notebook Scientific Python Stack</a> Docker container will be the development and evaluation environment for this challenge. The challenge <a href=""https://www.crowdai.org/challenges/3/dataset_files"">dataset</a> is comprised of source images <strong>source_images.tar</strong> and the pre-populated answer file <strong>product_nutrients.csv</strong>.</p>

<h4 id=""sourceimagestar"">source_images.tar</h4>

<p>The images archive will contain 500 images with the product id embedded in the name, eg:</p>

<div class=""highlighter-rouge""><div class=""highlight""><pre class=""highlight""><code> .
 image-2324.jpg
 image-2325.jpg
 image-2326.jpg
 image-2327.jpg
 .
</code></pre></div></div>

<h3 id=""installation-of-software"">Installation of software</h3>

<p>Participants may use any FOSS (free and open source) resources to produce the solution, and if used must be installed using a bash shell script called <strong>install.sh</strong></p>

<p>For example</p>

<p><code class=""highlighter-rouge"">sh
#!/bin/sh
apt-get update -y
apt-get install curl -y
</code></p>

<h3 id=""external-apis"">External APIs</h3>

<p>The following APIs may be used in this project. If they are used, the solution must work with the free tier.</p>

<p><a href=""https://cloud.google.com/vision"">Google Vision</a>
<a href=""http://www.ibm.com/watson/developercloud/visual-recognition.html"">IBM Watson</a>
<a href=""https://www.microsoft.com/cognitive-services/en-us/computer-vision-api"">Microsoft Vision API</a>
<a href=""Clarifai"">clarifai</a></p>

<p>If you wish to use another API please <a href=""https://www.crowdai.org/pages/contact"">contact us.</a></p>

<h3 id=""prizes"">Prizes</h3>

<p>The author of the most highly ranked submission above 80% will be invited to the crowdAI winner’s symposium at EPFL in Switzerland on January 30/31, 2017. This symposium is part of the Applied Machine Learning Days to which the winner will have full access. The educational award is given to the participant with the either the most insightful submission posts, or the best tutorial - the recipient of this award will also be invited to the symposium (the crowdAI team will pick the recipient of this award). Expenses for travel and accommodation are covered by crowdAI.</p>

<p>In addition, there is a CHF 2,000 (~ USD 2,000) prize on the most highly ranked submission above 80%.</p>

<h3 id=""datasets-license"">Datasets License</h3>

"
144,"<p>The <a href=""https://www.openfood.ch"">OpenFood</a> project has photographed and manually extracted text from 1243 food packages, and we expect to photograph more than 20,000 in the course of the project. The goal of this challenge is to produce an automated solution for extracting and classifying by language the ingredients lists.</p>

<p>Photographs of ingredients lists from food packaging are available in the Dataset section, with manually extracted text in French, German, Italian and English. Photographs and extracted text for 800 products has been provided to participants.</p>

<p>The challenge is to build an OCR script and language classification model, that will process each folder of product images and produce a text file of the contents.</p>

<p>The code must perform two tasks:</p>

<ul>
  <li>Convert the images to text</li>
  <li>Classify the language of the text</li>
</ul>

<p>It is possible there will be some other languages, other than German (de), French (fr), Italian (it) and English (en). Text for additional languages is not required, and is to be discarded.</p>

<h3 id=""example-images"">Example images</h3>

<p><strong>product_15665 Image</strong>
<img src=""https://s3.eu-central-1.amazonaws.com/crowdai-prd/challenge_images/openfood/image-15665.jpg"" alt="""" class=""img-medium"" /></p>

<p><strong>product_15665 json</strong></p>

<p><code class=""highlighter-rouge"">
{""product_id"":15665,""ingredients"":{""de"": ""Getreidemehle (Reis 50%, Hafer 26%, Mais 3%), Zucker, Inulin, Salz, Aroma, Farbstoff (Ammoniak-Zuckercouleur, Annatto)"", ""fr"": ""farines de céréales (riz 50%, avoine 26%, maïs 3%), sucre, inuline, sel, arôme, colorant (caramel E150c, Rocou)""}}
</code></p>

<p><strong>product_1978 image</strong>
<img src=""https://s3.eu-central-1.amazonaws.com/crowdai-prd/challenge_images/openfood/image-1978.jpg"" alt="""" class=""img-medium"" /></p>

<p><strong>product_1978 json</strong></p>

<p><code class=""highlighter-rouge"">
{""product_id"":1978,""ingredients"":{""de"": ""Tomatenmark, Zucker, Trinkwasser, Essig (Weisswein, Gerstenmalz, Branntwein / Weingeist), Melasse, modifizierte Stärke, Speisesalz, Gewürze (mit Senfsaat), Rapsöl, Raucharomen, Gerstenmalzextrakt, Stabilisator  (Xanthan), Säuerungsmittel (Citronensäure), Konservierungsstoff (Kaliumsorbat), Anchovis, Tamarindenextrakt, Aromen""}}
</code></p>

<p><strong>product_15668 image</strong>
<img src=""https://s3.eu-central-1.amazonaws.com/crowdai-prd/challenge_images/openfood/image-15668.jpg"" alt="""" class=""img-medium"" /></p>

<p><strong>product_15668 json</strong></p>

<p><code class=""highlighter-rouge"">
{""product_id"":15668,""ingredients"":{""de"": ""Wasser, Kokosnussextrakt 33%, Zucker, Maniokstärke, Verdickungsmittel(E440, E406), Kaffee-Extrakt 0.6%, Zitronensaftkonzentrat"", ""fr"": ""eau, extrait de noix de coco 33%, sucre, fécule de manioc, épaississants(E440, E406), extrait de café 0.6%, concentré de jus de citron""}}
</code></p>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>
<ul>
  <li>
    <p>An evaluation set of 443 ingredients lists images will be used.</p>
  </li>
  <li>
    <p>Submissions will be run by crowdAI against a Docker container running the official Ubuntu 14.04 LTS container, loaded with the test images.</p>
  </li>
  <li>
    <p>Participants must produce a folder of code with an install script called <code class=""highlighter-rouge"">install.sh</code>. When executed in the container this will download and install any necessary code and libraries.</p>
  </li>
  <li>
    <p>Participants must also include a script <code class=""highlighter-rouge"">run.sh</code> which is run against a folder of images.</p>
  </li>
  <li>
    <p>Participants can expect the 443 images to exist in the <code class=""highlighter-rouge"">/home/ubuntu/images/</code> directory</p>
  </li>
  <li>
    <p>The text classification predictions file is to be written to <code class=""highlighter-rouge"">/home/ubuntu/predictions.json.txt</code>. The training_predictions.json.txt file found under the Dataset link demonstrates the required format, which is one line per entry.</p>
  </li>
  <li>
    <p>The predictions.json.txt file will be evaluated against the answer file using the Python <strong>difflib</strong> library’s <a href=""https://docs.python.org/3/library/difflib.html#difflib.SequenceMatcher.ratio"">ratio</a> function, which provides a score between 0 and 1 as an indication of similarity. The score will be an average of all scores for the 443 images.</p>
  </li>
  <li>
    <p>Each line of the output file will be processed individually, so the order of JSON entries in the file is irrelevant.</p>
  </li>
</ul>

<h3 id=""resources"">Resources</h3>

<p>Participants should use the official Ubuntu 14.04 image for this challenge.</p>

<p><code class=""highlighter-rouge"">docker run -it ubuntu:trusty /bin/bash</code></p>

<ul>
  <li>
    <p>Any required libraries and packages may be installed, and this should be defined by the participant in an install.sh script, to be run as <code class=""highlighter-rouge"">bash /home/ubuntu/install.sh</code></p>
  </li>
  <li>
    <p>The images will be loaded into the <code class=""highlighter-rouge"">/home/ubuntu/images/</code> directory.</p>
  </li>
  <li>
    <p>The participant will provide a <code class=""highlighter-rouge"">run.sh</code> script which will be executed as <code class=""highlighter-rouge"">bash /home/ubuntu/run.sh</code></p>
  </li>
  <li>
    <p>The predictions file should be written to <code class=""highlighter-rouge"">/home/ubuntu/predictions.json.txt</code></p>
  </li>
</ul>

<p>Participants may use any FOSS (free and open source) resources to produce the solution, including programming languages, frameworks and public APIs.</p>

<h3 id=""prizes"">Prizes</h3>

<p>The author of the most highly ranked submission above 80% will be invited to the crowdAI winner’s symposium at the <em>2nd</em> Applied Machine Learning Days (note that the challenge will end after the upcoming <em>1st</em> <a href=""https://www.appliedmldays.org/"">Applied Machine Learning Days</a> ). The educational award is given to the participant with the either the most insightful submission posts, or the best tutorial - the recipient of this award will also be invited to the symposium (the crowdAI team will pick the recipient of this award). Expenses for travel and accommodation are covered by crowdAI.</p>

<p>In addition, there is a CHF 2,000 (~ USD 2,000) prize on the most highly ranked submission above 90%.</p>

<h3 id=""datasets-license"">Datasets License</h3>

"
30,"<blockquote class=""update"">
  <p>NOTE: This challenge was migrated from <a href=""https://www.aicrowd.com/blogs/welcome-to-aicrowd"">crowdAI</a></p>
</blockquote>

<h1 id=""introduction"">Introduction</h1>

<p>We are in a period of increasing humanitarian crises, both in scale and number. Natural disasters continue to increase in frequency and impact, while long-term and reignited conflicts affect people in many parts of the world. <strong>Often, accurate maps either do not exist or are outdated by disaster or conflict.</strong></p>

<p><a href=""http://www.hi-us.org/inclusion"">Humanity &amp; Inclusion</a> is an aid organization working in some 60 countries, alongside people with disabilities and vulnerable populations. Our emergency sector responds quickly and effectively to natural and civil disasters.</p>

<ul>
  <li>
    <p>Many parts of the world have not been mapped; especially the most marginalized parts, that is, those most vulnerable to natural hazards.
Obtaining maps of these potential crisis areas greatly improves the response of emergency preparedness actors.</p>
  </li>
  <li>
    <p>During a disaster it is extremely useful to be able to map the impassable sections of road for example, as well as the most damaged residential areas, the most vulnerable schools and public buildings, population movements, etc. The objective is to adapt as quickly as possible the intervention procedures to the evolution of the context generated by the crisis.</p>
  </li>
  <li>
    <p>In the first days following the occurrence of a disaster, it is essential to have as fine a mapping as possible of communication networks, housing areas and infrastructures, areas dedicated to agriculture, etc.</p>
  </li>
</ul>

<p>Today, when new maps are needed they are drawn by hand, often by volunteers who participate in so called Mapathons. They draw roads and buildings on satellite images, and contribute to Open StreetMap.</p>

<p>For instance, <a href=""http://www.hi-us.org/inclusion"">Humanity &amp; Inclusion</a> has been involved in organising numerous Mapathons to draw new maps for our clearance teams in Laos.</p>

<p>In this challenge we want to explore how Machine Learning can help pave the way for automated analysis of satellite imagery to generate relevant and real-time maps.</p>

<h2 id=""task"">Task</h2>

<p>Satellite imagery is readily available to humanitarian organisations, but translating images into maps is an intensive effort. Today maps are produced by <a href=""https://www.cartong.org"">specialized organisations</a> or in volunteer events such as <a href=""https://www.missingmaps.org"">mapathons</a>, where imagery is annotated with roads, buildings, farms, rivers etc.</p>

<p>Images are increasingly available from a variety of sources, including nano-satellites, drones and conventional high altitude satellites. The data is available: the task is to produce intervention-specific maps with the relevant features, in a short timeframe and from disparate data sources.</p>

<p><img src=""https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/mapping_challenge_1.jpg"" alt=""4up-image.jpg"" /></p>

<p>In this challenge you will be provided with a dataset of individual tiles of satellite imagery as RGB images, and their corresponding annotations of where an image is there a building. The goal is to train a model which given a new tile can annotate all buildings.</p>

<p>Also, in context of this challenge, to make the barrier to entry much lower, we tried to remove all the domain specific jargon of Remote Sensing and Satellite Imagery Analysis, and are presenting this as a problem of Object Detection and Object Segmentation in Images.</p>

<p>The idea being,  once we collectively demonstrate that an approach works really well on RGB images with just 3 channels of information, we can then work on extending it to multi-channel information from rich satellite imagery.</p>

<h1 id=""datasets"">Datasets</h1>

<p>You can download the datasets in the <a href=""https://www.aicrowd.com/challenges/mapping-challenge/dataset_files"">Datasets Section</a>. You are provided with :</p>

<ul>
  <li>
    <p><code class=""highlighter-rouge"">train.tar.gz</code> : This is the Training Set of <strong>280741</strong> tiles (as 300x300 pixel RGB images) of satellite imagery, along with their corresponding annotations in <a href=""http://cocodataset.org/#home"">MS-COCO format</a></p>
  </li>
  <li>
    <p><code class=""highlighter-rouge"">val.tar.gz</code>: This is the suggested Validation Set of <strong>60317</strong> tiles (as 300x300 pixel RGB images) of satellite imagery, along with their corresponding annotations in <a href=""http://cocodataset.org/#home"">MS-COCO format</a></p>
  </li>
  <li>
    <p><code class=""highlighter-rouge"">test_images.tar.gz</code> : This is the Test Set for Round-1, where you are provided with <strong>60697</strong> files (as 300x300 pixel RGB images) and your are required to submit annotations for all these files.</p>
  </li>
</ul>

<p>For more details about the dataset, and submission procedures etc, please refer to the following notebooks :</p>

<ul>
  <li><a href=""https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb"">Dataset Utils</a>
    <ul>
      <li><a href=""https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb#Import-dependencies"">Import Dependencies</a></li>
      <li><a href=""https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb#Configuration-Variables"">Configuration Variables</a></li>
      <li><a href=""https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb#Parsing-the-annotations"">Parsing Annotations</a></li>
      <li><a href=""https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb#Collecting-and-Visualizing-Images"">Collecting and Visualizing Images</a></li>
      <li><a href=""https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb#Understanding-Annotations"">Understanding Annotations</a></li>
      <li><a href=""https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb#Visualizing-Annotations"">Visualizing Annotations</a></li>
      <li><a href=""https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb#Advanced"">Advanced</a>
        <ul>
          <li><a href=""https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb#1.-Convert-poly-segmentation-to-rle"">Convert poly segmentation to rle</a></li>
          <li><a href=""https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb#2.-Convert-segmentation-to-pixel-level-masks"">Convert segmentation to pixel level masks</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href=""https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Random%20Submission.ipynb"">Random Submission</a>
    <ul>
      <li><a href=""https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Random%20Submission.ipynb#Submission-Format"">Submission Format</a></li>
      <li><a href=""https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Random%20Submission.ipynb#Generate-a-random-segmentation"">Generating a Random Segmentation</a></li>
      <li><a href=""https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Random%20Submission.ipynb#Generate-a-random-annotation-object"">Generating a Random Annotation Object</a></li>
      <li><a href=""https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Random%20Submission.ipynb#Generate-a-results-object"">Generating a Random Results Object</a></li>
      <li><a href=""https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Random%20Submission.ipynb#Submit-to-crowdAI-for-grading"">Submit to crowdAI for grading</a></li>
    </ul>
  </li>
  <li>
    <p><a href=""https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Local%20Evaluation.ipynb"">Locally test the evaluation function</a></p>
  </li>
  <li>Train <a href=""https://arxiv.org/abs/1703.06870"">Mask-RCNN</a>
    <ul>
      <li><a href=""https://github.com/crowdAI/crowdai-mapping-challenge-mask-rcnn"">Installation</a></li>
      <li><a href=""https://github.com/crowdAI/crowdai-mapping-challenge-mask-rcnn/blob/master/Training.ipynb"">Training</a></li>
      <li><a href=""https://github.com/crowdAI/crowdai-mapping-challenge-mask-rcnn/blob/master/Prediction-and-Submission.ipynb"">Prediction &amp; Submission</a></li>
      <li><strong>NOTE</strong> : This is in a separate repository, and we have also now added the pretrained weights from the baseline submission to the <a href=""https://www.crowdai.org/challenges/mapping-challenge/dataset_files"">datasets page</a>.</li>
    </ul>
  </li>
</ul>

<h1 id=""evaluation-criteria"">Evaluation Criteria</h1>

<p>For for a known ground truth mask <script type=""math/tex"">A</script>, you propose a mask <script type=""math/tex"">B</script>, then we first compute <script type=""math/tex"">IoU</script> (Intersection Over Union) :</p>

<script type=""math/tex; mode=display"">IoU(A, B) = \frac{A \cap B}{ A \cup B}</script>

<p><script type=""math/tex"">IoU</script> measures the overall overlap between the true region and the proposed region.
Then we consider it a True detection, when there is atleast half an overlap, or when <script type=""math/tex"">IoU \geq 0.5</script></p>

<p>Then we can define the following parameters :</p>

<ul>
  <li>
    <p>Precision (<script type=""math/tex"">IoU \geq 0.5</script>) <br />
<script type=""math/tex"">P_{IoU \geq 0.5} = \frac{TP_{IoU \geq 0.5}}{TP_{IoU \geq 0.5} + FP_{IoU \geq 0.5}}</script></p>
  </li>
  <li>
    <p>Recall (<script type=""math/tex"">IoU  \geq 0.5</script>) <br />
<script type=""math/tex"">R_{IoU \geq 0.5} = \frac{TP_{IoU \geq 0.5}}{TP_{IoU \geq 0.5} + FN_{IoU \geq 0.5}}</script>.</p>
  </li>
</ul>

<p>The final scoring parameters <script type=""math/tex"">AP_{IoU \geq 0.5}</script> and <script type=""math/tex"">AR_{IoU \geq 0.5}</script> are computed by averaging over all the precision and recall values for all known annotations in the ground truth.</p>

<h1 id=""challenge-rounds"">Challenge Rounds</h1>

<h2 id=""round-1"">Round 1</h2>

<p>We will stop accepting submissions for Round 1 on June 1, 2018.
All participants of Round 1 will be invited to complete in Round 2.</p>

<p>For instructions on submitting solutions for Round-2, please refer to the <a href=""https://github.com/crowdAI/mapping-challenge-starter-kit"">mapping-challenge-starter-kit</a>.</p>

<p><strong>Note</strong>: We will be adding more content to the starter kit to help you get started in the challenge. So please do keep a close eye on the starter-kit for updates. In the meantime, you can have a look at the examples of <a href=""https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocoDemo.ipynb"">cocoapi</a> on how to easily parse and explore the datasets.</p>

<h2 id=""round-2"">Round 2</h2>

<p>Round 2 participation is open to all. 
Participants are required to submit their code and models which <strong>will be internally tested by <a href=""https://unitar.org/unosat/"">UNOSAT</a>  and <a href=""https://www.unglobalpulse.org/"">UN Global Pulse</a> on a dataset (in a similar format as the currently released data) of an undisclosed location.</strong></p>

<p><strong>Starter-Kit for Round-2 can be found at</strong> : <a href=""https://github.com/crowdAI/mapping-challenge-round2-starter-kit"">https://github.com/crowdAI/mapping-challenge-round2-starter-kit</a></p>

<h1 id=""timeline"">Timeline</h1>

<ul>
  <li>Round 1 : 28.03.2018 - 01.06.2018</li>
  <li>Round 2 : 23.07.2018 - 20.08.2018</li>
  <li>Announcement of Overall Results : 20.08.2018</li>
</ul>

<h1 id=""rules"">Rules</h1>

<p>The following rules have to be observed by all participants:</p>

<ul>
  <li>Participants are allowed at most 5 submissions per day.</li>
  <li>Participants are welcome to form teams. Teams should submit their predictions under a single account. The submitted paper will mention all members.</li>
  <li>Participants have to release their solution under an <a href=""https://opensource.org/licenses"">Open Source License</a> of their choice to be eligible for prizes. We encourage all participants to open-source their code!</li>
  <li>Participants are not allowed to use any other datasets other than the ones released in context of this challenge. The use of pre-trained models is nevertheless permitted.</li>
  <li>While submissions by Admins and Organizers can serve as baselines, they won’t be considered in the final leaderboard.</li>
  <li>In case of conflicts, the decision of the Organizers will be final and binding.</li>
  <li>Organizers reserve the right to make changes to the rules and timeline.</li>
  <li>Violation of the rules or other unfair activity may result in disqualification.</li>
</ul>

<h1 id=""prizes"">Prizes</h1>

<p><strong>UPDATE</strong></p>

<p><strong>The prizes have been updated, as follows:</strong></p>

<p><strong>Top-1 participant of Round 2</strong>: Invitation to the <a href=""https://www.appliedmldays.org"">Applied Machine Learning Days</a> 2019 at EPFL, Switzerland in January 2019, with travel and accommodation covered.</p>

<p><strong>Top-5 participants of Round 2</strong>: Invitation to present at the <a href=""https://dsaa2018.isi.it/home"">IEEE DSAA 2018</a> in Turin, Italy, October 1-4, 2018, with conference registration, travel and accommodation covered (up to EUR 1000).
This prize is sponsored by Humanity &amp; Inclusion.</p>

<p><strong>Top-5 participants of Round 2</strong>: Invitation to submit a paper describing their solution to be published in the proceedings of <a href=""https://dsaa2018.isi.it/home"">IEEE DSAA 2018</a>, Turin, Italy.</p>

<p><strong>Top Community Contributor</strong>: Invitation to the <a href=""https://dsaa2018.isi.it/home"">IEEE DSAA 2018</a> in Turin, Italy, October 1-4, 2018, with travel and accommodation covered (up to EUR 1000).
This prize is aimed to reward the participant or team who contributed the most for the community to this challenge (e.g. releasing own code openly during challenge, helping other paricipants, etc)</p>

<p><strong>All participants (Round 1 and 2)</strong> : Certificate of participation from <a href=""https://handicap-international.ch"">Handicap International</a>, <a href=""https://unitar.org/unosat/"">UNOSAT</a>, <a href=""https://www.unglobalpulse.org/"">UN Global Pulse</a>, <a href=""https://www.epfl.ch/"">EPFL</a> and <a href=""https://www.crowdai.org/"">crowdAI</a>.</p>

<h2 id=""starter-kit"">Starter Kit</h2>

<p>A starter kit has been prepared which explains all the nuts and bolts required to get started in the challenge.
It can be accessed at : <a href=""https://github.com/crowdAI/mapping-challenge-starter-kit"">https://github.com/crowdAI/mapping-challenge-starter-kit</a></p>

<p><strong>Starter-Kit for Round-2 can be found at</strong> : <a href=""https://github.com/crowdAI/mapping-challenge-round2-starter-kit"">https://github.com/crowdAI/mapping-challenge-round2-starter-kit</a></p>

<h1 id=""resources"">Resources</h1>

<p>Here are some interesting blog posts written by participants:</p>

<ul>
  <li>
    <p><a href=""https://towardsdatascience.com/mapping-challenge-winning-solution-1aa1a13161b3"" target=""_blank"">Mapping Challenge winning solution</a></p>
  </li>
  <li>
    <p><a href=""https://spark-in.me/post/a-small-case-for-search-of-structure-within-your-data"" target=""_blank"">Playing with AIcrowd mapping challenge - or how to improve your CNN performance with self-supervised techniques</a></p>
  </li>
</ul>

<p>Here is an open solution for this challenge, proposed by neptune.ml:</p>

<ul>
  <li><a href=""https://github.com/neptune-ml/open-solution-mapping-challenge"" target=""_blank"">https://github.com/neptune-ml/open-solution-mapping-challenge</a></li>
</ul>

<h2 id=""acknowledgements"">Acknowledgements</h2>

<p>A big shout out to our awesome community members <a href=""https://www.crowdai.org/participants/masterscrat"">@MasterScat (Florian Laurent)</a>, <a href=""snigdha.dagar@gmail.com"">Snigdha Dagar</a>, and <a href=""https://www.crowdai.org/participants/iuliana"">Iuliana Voinea</a>, for their help in preparing the datasets and designing the challenge.</p>

<h2 id=""contact-us"">Contact Us</h2>

<ul>
  <li>Gitter Channel : <a href=""https://gitter.im/crowdAI/crowdai-mapping-challenge"">crowdAI/crowdai-mapping-challenge</a></li>
  <li>Technical issues : <a href=""https://github.com/crowdAI/mapping-challenge-starter-kit/issues"" target=""_blank"">https://github.com/crowdAI/mapping-challenge-starter-kit/issues </a></li>
  <li>Discussion Forum : <a href=""https://www.crowdai.org/challenges/mapping-challenge/topics"">https://www.crowdai.org/challenges/mapping-challenge/topics</a></li>
</ul>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

<ul>
  <li><a href=""mailto:mohanty@aicrowd.com"" target=""_blank""> mohanty@aicrowd.com
 </a></li>
</ul>

<p>Here are some interesting blog posts written by participants:</p>

<ul>
  <li>
    <p><a href=""https://towardsdatascience.com/mapping-challenge-winning-solution-1aa1a13161b3"" target=""_blank"">Mapping Challenge winning solution</a></p>
  </li>
  <li>
    <p><a href=""https://spark-in.me/post/a-small-case-for-search-of-structure-within-your-data"" target=""_blank"">Playing with AIcrowd mapping challenge - or how to improve your CNN performance with self-supervised techniques</a></p>
  </li>
</ul>

<p>Here is an open solution for this challenge, proposed by neptune.ml:</p>

<ul>
  <li><a href=""https://github.com/neptune-ml/open-solution-mapping-challenge"" target=""_blank"">https://github.com/neptune-ml/open-solution-mapping-challenge</a></li>
</ul>

<h2 id=""acknowledgements-1"">Acknowledgements</h2>

<p>A big shout out to our awesome community members <a href=""https://www.crowdai.org/participants/masterscrat"">@MasterScat (Florian Laurent)</a>, <a href=""snigdha.dagar@gmail.com"">Snigdha Dagar</a>, and <a href=""https://www.crowdai.org/participants/iuliana"">Iuliana Voinea</a>, for their help in preparing the datasets and designing the challenge.</p>

<h2 id=""contact-us-1"">Contact Us</h2>

<ul>
  <li>Technical issues : <a href=""https://github.com/crowdAI/mapping-challenge-starter-kit/issues"" target=""_blank"">https://github.com/crowdAI/mapping-challenge-starter-kit/issues </a></li>
  <li>Discussion Forum : [https://discourse.aicrowd.com/c/mapping-challenge)</li>
</ul>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

<ul>
  <li><a href=""mailto:mohanty@aicrowd.com"" target=""_blank""> mohanty@aicrowd.com
 </a></li>
</ul>
"
155,"<hr />
<p><strong>Important note:</strong></p>

<p><em>The <strong>ImageCLEF 2018 VQA-Med challenge has officially ended</strong> and we would like to thank everyone for their participation. The official results are already emailed to corresponding participants.</em></p>

<p><em>Post-challenge submissions and the leaderboard will remain enabled  for a few weeks so you will still be able to submit result files and have them continuously evaluated during a limited period. 
Please consider that in order to see the version of the leaderboard with the post-challenge submissions integrated, you have to turn on the switch <strong>Show post-challenge submission</strong> right below the leaderboard.</em></p>

<p><em>At the same time we’d like to encourage you to submit a <a href=""http://clef2018.clef-initiative.eu/index.php?page=Pages/InstructionsforCLEF2018WorkingNotes.html"">CLEF Working notes paper</a> until the end of May.</em></p>

<p><em>Please also note that participants registering from now on will not be
automatically registered with CLEF anymore.</em></p>

<hr />

<p><em>Note: Do not forget to read the <strong>Rules</strong> section on this page</em></p>

<h3 id=""motivation"">Motivation</h3>

<p>With the increasing interest in artificial intelligence (AI) to support clinical decision making and improve patient engagement, opportunities to generate and leverage algorithms for automated medical image interpretation are currently being explored. Since patients may now access structured and unstructured data related to their healthcare utilization via patient portals, such access also motivates the need to help them better understand their conditions in line their available data, including medical images.</p>

<p>Clinicians’ confidence in interpreting complex medical images can be significantly enhanced by “second opinion” provided by an automated system. In addition, patients may be interested in the morphology/physiology and disease-status of anatomical structures around a lesion that has been well characterized by their healthcare providers – and they may not necessarily be willing to pay significant amounts for a separate office- or hospital visit just to address such questions. Although patients often turn to search engines (e.g. Google) to disambiguate complex terms or obtain answers to confusing aspects of the medical image, results from search engines may be nonspecific, erroneous and misleading, or overwhelming in terms of the volume of information.</p>

<h3 id=""challenge-description"">Challenge description</h3>

<p>Visual Question Answering is a new and exciting problem that combines natural language processing and computer vision techniques. Inspired by the recent success of <a href=""http://visualqa.org/"" target=""_blank""> visual question answering in the general domain </a>, we propose a pilot task this year to focus on visual question answering in the medical domain. Given a medical image accompanied with a set of clinically relevant questions, participating systems are tasked with answering the questions based on the visual image content.</p>

<h3 id=""data"">Data</h3>

<p>The data will tentatively include a training set (5K) and a validation set (0.5K) with medical images accompanied with question-answer pairs, and a test set (0.5K) of images with questions only. To create the datasets for the proposed task, we would consider the medical domain images extracted from PubMed articles (essentially a subset of the ImageCLEF 2017 caption prediction task).</p>

<h3 id=""submission-instructions"">Submission instructions</h3>

<hr />
<p><em>As soon as the submission is open, you will find a “Create Submission” button on this page (just next to the tabs)</em></p>

<div class=""highlighter-rouge""><div class=""highlight""><pre class=""highlight""><code>-Each team is allowed to submit a maximum of 5 runs.

-We expect the following format for the result submission file: &lt;QA-ID&gt;&lt;TAB&gt;&lt;Image-ID&gt;&lt;TAB&gt;&lt;Answer&gt;
</code></pre></div></div>

<p>For example:</p>

<div class=""highlighter-rouge""><div class=""highlight""><pre class=""highlight""><code>1 rjv03401 answer of the first question in one single line
2 AIAN-14-313-g002 answer of the second question
3 wjem-11-76f3 answer of the third question
</code></pre></div></div>

<p>-You need to respect the following constraints:</p>

<div class=""highlighter-rouge""><div class=""highlight""><pre class=""highlight""><code>• The separator between &lt;QA-ID&gt;, &lt;Image-ID&gt; and &lt;Answer&gt; has to be a tabular white space (tab).
• Each &lt;QA-ID&gt; of the test set must be included in the run file exactly once.
• You should not include special characters in the &lt;Answer&gt; field.
• All 500 &lt;QA-ID&gt; and &lt;Image-ID&gt; pairs must be present in a participant’s run file in the same order as the VQAMed2018Test-QA.csv file.
</code></pre></div></div>

<p>-Participants are allowed to use other resources asides from the official training/validation datasets, however the use of the additional resources must have to be explicitly stated. For meaningful comparison, we will separately group systems who exclusively use the official training data and who incorporate additional sources.</p>

<p><em>Please provide the necessary information and select a submission file. As soon as a submission file is selected the form is submitted automatically. 
After the submission of the form the grading process will be initiated where an external grader validates/evaluates the submitted file and eventually returns the score back to CrowdAI. Depending on the file size, the evaluation algorithm and the total grading workload this could take a while. You can see the status of your submission in the “Submissions” tab of this challenge’s page, where you will redirected to automatically after having submitted. In case the evaluation failed, the “Status” field shows “failed” and an error message in the “Message” field is displayed.</em></p>

<hr />

<h3 id=""acknowledgements"">Acknowledgements</h3>

<p><a href=""http://www.ncbi.nlm.nih.gov/pmc/"" target=""_blank""> PubMed Central </a></p>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>
<p>Information will be posted soon.</p>

<h3 id=""resources"">Resources</h3>

<h3 id=""contact-us"">Contact us</h3>

<ul>
  <li>Technical issues : <a href=""https://gitter.im/crowdAI/imageclef-2018-vqa-med"" target=""_blank""> https://gitter.im/crowdAI/imageclef-2018-vqa-med </a></li>
  <li>Discussion Forum : <a href=""https://www.crowdai.org/challenges/imageclef-2018-vqa-med/topics"" target=""_blank""> https://www.crowdai.org/challenges/imageclef-2018-vqa-med/topics </a></li>
</ul>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

<ul>
  <li>Sharada Prasanna Mohanty: sharada.mohanty@epfl.ch</li>
  <li>Sadid Hasan: sadid[DOT]hasan[AT]philips[DOT]com</li>
  <li>Yuan Ling: yuan[DOT]ling[AT]philips[DOT]com</li>
  <li>Henning Müller: henning[DOT]mueller[AT]hevs[DOT]ch</li>
  <li>Ivan Eggel: ivan[DOT]eggel[AT]hevs[DOT]ch</li>
</ul>

<h3 id=""more-information"">More information</h3>

<p>You can find additional information on the challenge here:
<a href=""http://imageclef.org/2018/VQA-Med"" target=""_blank""> http://imageclef.org/2018/VQA-Med </a></p>

<h3 id=""prizes"">Prizes</h3>

<p>ImageCLEF 2018 is an evaluation campaign that is being organized as part of the <a href=""http://clef2018.clef-initiative.eu/"" target=""_blank""> CLEF initiative </a> labs. The campaign offers several research tasks that welcome participation from teams around the world. The results of the campaign appear in the working notes proceedings, published by CEUR Workshop Proceedings (CEUR-WS.org). Selected contributions among the participants, will be invited for publication in the following year in the Springer Lecture Notes in Computer Science (LNCS) together with the annual lab overviews.</p>

<h3 id=""datasets-license"">Datasets License</h3>

"
173,"<p>Welcome to the Criteo Ad Placement challenge, accompanying the <a href=""https://sites.google.com/view/causalnips2017"" target=""_blank"">NIPS’17 Workshop on Causal Inference and Machine Learning</a>.</p>

<p>Consider a display advertising scenario: a user arrives at a website where we have an advertisement slot (“an impression”). We have a pool of potential products to advertise during that impression (“a candidate set”). A “policy” chooses which product to display so as to maximize the number of clicks by the users.</p>

<p>The goal of the challenge is to find a good policy that, knowing the candidate set and impression context, chooses products to display such that the aggregate click-through-rate (“CTR”) from deploying the policy is maximized.</p>

<p>To enable you to find a good policy, Criteo has generously donated several gigabytes of &lt;user impression, candidate set, selected product, click/no-click&gt; logs from a randomized version of their in-production policy! Go to the <a href=""https://www.crowdai.org/challenges/nips-17-workshop-criteo-ad-placement-challenge/dataset_files"" target=""_blank"">Dataset section</a> of the challenge to access the dataset and visit the <a href=""https://github.com/crowdai/crowdai-criteo-ad-placement-challenge-starter-kit"" target=""_blank"">GitHub repo</a> to access a starter-kit for counterfactual policy learning with the dataset.</p>

<p><strong>Prizes worth $4500 sponsored by Criteo!</strong></p>

<p><img src=""https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/18c2b716ccbcf02885aa6fb4c9e9e8c6_criteo_research_logo.png"" alt=""criteo_research_logo.png"" /></p>

<p>This policy learning challenge is subtly different from the <a href=""https://www.kaggle.com/c/criteo-display-ad-challenge"" target=""_blank"">2014 Criteo challenge</a> to compare CTR prediction algorithms. CTR prediction is a standard supervised regression problem: for any ad in the training data, the correct regression target is given (click/no-click). For policy learning, when the in-production policy chose a product that was not clicked, we do not know “what-if” (counterfactual) we chose a different product to display. These differences and some baseline approaches are detailed in the <a href=""http://www.cs.cornell.edu/~adith/Criteo/"" target=""_blank"">dataset companion paper</a>.</p>

<p>The objective of this challenge is to spur participants to explore several issues on a real-world benchmark dataset, and share their findings during the NIPS’17 workshop, such as:</p>

<ul>
  <li>Discover new training objectives, learning algorithms and regularization mechanisms for counterfactual learning scenarios.</li>
  <li>Find appropriate ways to perform model selection (analogous to cross-validation for supervised learning) using large amounts of logged interaction data.</li>
  <li>Develop algorithms that can scale to massive datasets (typically orders of magnitude larger than labeled datasets).</li>
</ul>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>
<p>In the dataset, each impression is represented by <code class=""highlighter-rouge"">M</code> lines where <code class=""highlighter-rouge"">M</code> is the number of candidate ads. Each line has feature information for every other candidate ad.
In addition, the first line corresponds to the candidate that was displayed by the logging policy, an indicator whether the displayed product was clicked by the user (“click” encoded as <code class=""highlighter-rouge"">0.001</code>, “no-click” encoded as <code class=""highlighter-rouge"">0.999</code>), and the <strong>inverse propensity</strong> of the stochastic logging policy to pick that specific candidate (see the  <a href=""http://www.cs.cornell.edu/~adith/Criteo/""> companion paper </a>. for details).
Each <code class=""highlighter-rouge"">&lt;user context-candidate product&gt;</code> pair is described using <strong>33 categorical (multi-set) features and 2 numeric features</strong>. Of these, 10 features are only-context-dependent while the remaining 25 features depend on both the context and the candidate product. These categorical feature representations have been post-processed to a 74000-dimensional vector with sparse one-hot encoding for each categorical feature. <strong>The semantics behind the features will not be disclosed</strong>.</p>

<p>These post-processed dataset files are available <a href=""https://www.crowdai.org/challenges/nips-17-workshop-criteo-ad-placement-challenge/dataset_files""> here </a>.</p>

<p>Your task is to build a function <code class=""highlighter-rouge"">_policy</code> which takes <code class=""highlighter-rouge"">M</code> candidates, each represented by a <strong>74000-dimensional sparse vector</strong>, and outputs scores for each of the candidates between <code class=""highlighter-rouge"">1</code> and <code class=""highlighter-rouge"">M</code>.</p>

<p>The reward for an individual impression is, did the selected candidate get clicked <em>(reward = 1)</em> or not <em>(reward = 0)</em>? The reward for function <code class=""highlighter-rouge"">f</code> is the aggregate reward over all impressions on a held out test set of impressions.</p>

<p>We will be using an unbiased estimate of the aggregate reward using inverse propensity scoring (see <a href=""http://www.cs.cornell.edu/~adith/Criteo/NIPS16_Benchmark.pdf""> the companion paper </a> for details).</p>

<p>For further details on evaluation please refer to the Getting Started guide in the <a href=""https://github.com/crowdai/crowdai-criteo-ad-placement-challenge-starter-kit"">challenge starter kit</a>.</p>

<h3 id=""resources"">Resources</h3>

<h3 id=""contact"">Contact:</h3>

<ul>
  <li>Technical issues : <a href=""https://gitter.im/crowdAI/NIPS17-Criteo-Ad-Placement-Challenge"">https://gitter.im/crowdAI/NIPS17-Criteo-Ad-Placement-Challenge</a></li>
  <li>Discussion Forum : <a href=""https://www.crowdai.org/challenges/nips-17-workshop-criteo-ad-placement-challenge/topics"">https://www.crowdai.org/challenges/nips-17-workshop-criteo-ad-placement-challenge/topics</a></li>
</ul>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

<ul>
  <li>Adith Swaminathan <a href=""mailto:adswamin@microsoft.com"" target=""_blank""> adswamin@microsoft.com </a></li>
  <li>Sharada Prasanna Mohanty <a href=""mailto:sharada.mohanty@epfl.ch"" target=""_blank""> sharada.mohanty@epfl.ch </a></li>
</ul>

<h3 id=""prizes"">Prizes</h3>

<ul>
  <li>1st  - <strong>$2000</strong></li>
  <li>2nd - <strong>$1500</strong></li>
  <li>3rd  - <strong>$1000</strong></li>
</ul>

<p>Additionally:</p>

<ul>
  <li>
    <p>Invitation to <a href=""https://sites.google.com/view/causalnips2017"" target=""_blank""> NIPS’17 Causal Inference and Machine Learning Workshop </a></p>
  </li>
  <li>
    <p>Invitation to the <a href=""https://www.appliedmldays.org/"">2nd Applied Machine Learning Days</a> at EPFL in Switzerland on January 29 &amp; 30, 2018.</p>
  </li>
</ul>

<h3 id=""datasets-license"">Datasets License</h3>

"
197,"
<p><strong>INTRODUCTION TO BE OBTAINED FROM UNOSAT</strong></p>

<h2 id=""task"">Task</h2>

<p>In this challenge you will be provided with a dataset of individual tiles of satellite imagery (as RGB images) from a refugee camp. Each of these tiles may have up to 5 types of shelters.
The corresponding annotations of the type and location of a shelter in an image is provided for the training set. The goal is to train a model which given a new tile can annotate all types of shelters available in the image.</p>

<p>Also, in context of this challenge, to make the barrier to entry much lower, we tried to remove all the domain specific jargon of Remote Sensing and Satellite Imagery Analysis, and are presenting this as a problem of Object Detection and Object Segmentation in Images.</p>

<p>The idea being,  once we collectively demonstrate that an approach works really well on RGB images with just 3 channels of information, we can then work on extending it to multi-channel information from rich satellite imagery.</p>

<h1 id=""datasets"">Datasets</h1>

<p>You can download the datasets in the <a href=""https://www.crowdai.org/challenges/mapping-challenge/dataset_files"">Datasets Section</a>. You are provided with :</p>

<ul>
  <li>
    <p><code class=""highlighter-rouge"">train.tar.gz</code> : This is the Training Set of <strong>2000</strong> tiles (as 300x300 pixel RGB images) of satellite imagery, along with their corresponding annotations in <a href=""http://cocodataset.org/#home"">MS-COCO format</a> <strong>*TO BE ADDED</strong></p>
  </li>
  <li>
    <p><code class=""highlighter-rouge"">test_images.tar.gz</code> : This is the Test Set for Round-1, where you are provided with <strong>2400</strong> files (as 300x300 pixel RGB images) and your are required to submit annotations for all these files.   <strong>*TO BE ADDED</strong></p>
  </li>
</ul>

<p><img src=""https://dnczkxd1gcfu5.cloudfront.net/images/challenges/image_file/42/unosat-mapping-challenge.png"" alt=""dataset"" /></p>

<p>For more details about the dataset, and submission procedures etc, please refer to the following notebooks :</p>

<ul>
  <li><a href=""https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb"">Dataset Utils</a>
    <ul>
      <li><a href=""https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb#Import-dependencies"">Import Dependencies</a></li>
      <li><a href=""https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb#Configuration-Variables"">Configuration Variables</a></li>
      <li><a href=""https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb#Parsing-the-annotations"">Parsing Annotations</a></li>
      <li><a href=""https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb#Collecting-and-Visualizing-Images"">Collecting and Visualizing Images</a></li>
      <li><a href=""https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb#Understanding-Annotations"">Understanding Annotations</a></li>
      <li><a href=""https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb#Visualizing-Annotations"">Visualizing Annotations</a></li>
      <li><a href=""https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb#Advanced"">Advanced</a>
        <ul>
          <li><a href=""https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb#1.-Convert-poly-segmentation-to-rle"">Convert poly segmentation to rle</a></li>
          <li><a href=""https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Dataset%20Utils.ipynb#2.-Convert-segmentation-to-pixel-level-masks"">Convert segmentation to pixel level masks</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href=""https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Random%20Submission.ipynb"">Random Submission</a>
    <ul>
      <li><a href=""https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Random%20Submission.ipynb#Submission-Format"">Submission Format</a></li>
      <li><a href=""https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Random%20Submission.ipynb#Generate-a-random-segmentation"">Generating a Random Segmentation</a></li>
      <li><a href=""https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Random%20Submission.ipynb#Generate-a-random-annotation-object"">Generating a Random Annotation Object</a></li>
      <li><a href=""https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Random%20Submission.ipynb#Generate-a-results-object"">Generating a Random Results Object</a></li>
      <li><a href=""https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Random%20Submission.ipynb#Submit-to-crowdAI-for-grading"">Submit to crowdAI for grading</a></li>
    </ul>
  </li>
  <li>
    <p><a href=""https://github.com/crowdAI/mapping-challenge-starter-kit/blob/master/Local%20Evaluation.ipynb"">Locally test the evaluation function</a></p>
  </li>
  <li>Train <a href=""https://arxiv.org/abs/1703.06870"">Mask-RCNN</a>
    <ul>
      <li><a href=""https://github.com/crowdAI/crowdai-mapping-challenge-mask-rcnn"">Installation</a></li>
      <li><a href=""https://github.com/crowdAI/crowdai-mapping-challenge-mask-rcnn/blob/master/Training.ipynb"">Training</a></li>
      <li><a href=""https://github.com/crowdAI/crowdai-mapping-challenge-mask-rcnn/blob/master/Prediction-and-Submission.ipynb"">Prediction &amp; Submission</a></li>
      <li><strong>NOTE</strong> : This is in a separate repository, and we have also now added the pretrained weights from the baseline submission to the <a href=""https://www.crowdai.org/challenges/mapping-challenge/dataset_files"">datasets page</a>.</li>
    </ul>
  </li>
</ul>

<h1 id=""evaluation-criteria"">Evaluation Criteria</h1>

<p>For for a known ground truth mask <script type=""math/tex"">A</script>, you propose a mask <script type=""math/tex"">B</script>, then we first compute <script type=""math/tex"">IoU</script> (Intersection Over Union) :</p>

<script type=""math/tex; mode=display"">IoU(A, B) = \frac{A \cap B}{ A \cup B}</script>

<p><script type=""math/tex"">IoU</script> measures the overall overlap between the true region and the proposed region.
Then we consider it a True detection, when there is atleast half an overlap, or when <script type=""math/tex"">IoU \geq 0.5</script></p>

<p>Then we can define the following parameters :</p>

<ul>
  <li>
    <p>Precision (<script type=""math/tex"">IoU \geq 0.5</script>) <br />
<script type=""math/tex"">P_{IoU \geq 0.5} = \frac{TP_{IoU \geq 0.5}}{TP_{IoU \geq 0.5} + FP_{IoU \geq 0.5}}</script></p>
  </li>
  <li>
    <p>Recall (<script type=""math/tex"">IoU  \geq 0.5</script>) <br />
<script type=""math/tex"">R_{IoU \geq 0.5} = \frac{TP_{IoU \geq 0.5}}{TP_{IoU \geq 0.5} + FN_{IoU \geq 0.5}}</script>.</p>
  </li>
</ul>

<p>The final scoring parameters <script type=""math/tex"">AP_{IoU \geq 0.5}</script> and <script type=""math/tex"">AR_{IoU \geq 0.5}</script> are computed by averaging over all the precision and recall values for all known annotations in the ground truth.</p>

<h1 id=""challenge-rounds"">Challenge Rounds</h1>

<ul>
  <li>Round 1 : <strong>Dates to be decided</strong></li>
  <li>Round 2 : <strong>Dates to be decided</strong></li>
  <li>Announcement of Overall Results : <strong>Dates to be decided</strong></li>
</ul>

<h3 id=""evaluation-criteria-1"">Evaluation criteria</h3>

<h3 id=""resources"">Resources</h3>

<h3 id=""prizes"">Prizes</h3>

<p>** TO BE DECIDED **</p>

<h3 id=""datasets-license"">Datasets License</h3>

"
21,"<blockquote class=""update mt-0"">
  <p><strong>Best Performance Track</strong> (<a href=""https://www.aicrowd.com/challenges/neurips-2019-learn-to-move-walk-around/leaderboards"">leaderboard</a>):<br />
<strong>Winner: PARL</strong><br />
<small>
    - Score: 1490.073<br />
    - <a href=""https://youtu.be/FD2lGv-4BLE"">Video</a><br />
    - <a href=""https://github.com/PaddlePaddle/PARL/tree/develop/examples/NeurIPS2019-Learn-to-Move-Challenge"">Test code</a><br />
</small>
<strong>Second Place: scitator</strong><br />
<small>
    - Score: 1346.939<br />
    - <a href=""https://youtu.be/WuqNdNBVzzI"">Video</a><br />
</small>
<strong>Third Place: SimBodyWithDummyPlug</strong><br />
<small>
    - Score: 1303.727<br />
    - <a href=""https://youtu.be/DfPPiTsuB4E"">Video</a><br />
</small></p>
</blockquote>

<blockquote class=""update mt-0"">
  <p><strong>Machine Learning Track</strong>:<br />
The best and finalist papers are accepted to the <a href=""https://sites.google.com/view/deep-rl-workshop-neurips-2019/home#h.p_6387jp8vOkV6"">NeurIPS 2019 Deep Reinforcement Learning Workshop</a><br />
<strong>Best Paper:</strong><br />
<small>
    - <em>Efficient and robust reinforcement learning with uncertainty-based value expansion</em>, [PARL] Bo Zhou, Hongsheng Zeng, Fan Wang, Yunxiang Li, and Hao Tian<br />
</small>
<strong>Finalists:</strong><br />
<small>
    - <em>Distributed Soft Actor-Critic with Multivariate Reward Representation and Knowledge Distillation</em>, [SimBodyWithDummyPlug] Dmitry Akimov<br />
    - <em>Sample Efficient Ensemble Learning with Catalyst.RL</em>, [scitator] Sergey Kolesnikov and Valentin Khrulkov<br />
<strong>Reviewers</strong> provided throughtful feedback<br />
    - Yunfei Bai (Google X), Glen Berseth (UC Berkeley), Nuttapong Chentanez (NVIDIA), Sehoon Ha (Google Brain), Jemin Hwangbo (ETH Zurich), Seunghwan Lee (Seoul National University), Libin Liu (DeepMotion), Josh Merel (DeepMind), Jie Tan (Google)<br />
And the <strong>review board</strong> selected the papers<br />
    - Xue Bin (Jason) Peng (UC Berkeley), Seungmoon Song (Stanford University), Łukasz Kidziński (Stanford University), Sergey Levine (UC Berkeley)<br />
</small></p>
</blockquote>

<blockquote class=""update mt-0"">
  <p><strong>Neuromechanics Track</strong>:<br />
No winner was selected for the neuromechanics track.<br /></p>
</blockquote>

<blockquote class=""update mt-0"">
  <p><strong>Notification</strong>:<br />
- OCT 26: <strong><a href=""#timeline"">Timeline</a> is adjusted</strong><br />
<small>
    - Paper submission due date: November 3<br />
    - Winners announcement: November 29<br />
    - Time zone: Anywhere on Earth<br />
</small>
- OCT 18: <strong>Submission for Round 2 is open!!</strong><br />
<small>
    - We only accept <a href=""https://github.com/stanfordnmbl/neurips2019-learning-to-move-starter-kit"">docker submissions</a><br />
    - Evaluation will be done with <a href=""https://github.com/stanfordnmbl/osim-rl/commit/9be49551ab617768d30488c961b0ac78cf209c88"">osim-rl <s>v3.0.9</s>v3.0.11</a><br />
    - All submssions will be re-evaluated with the last version<br />
</small>
- OCT 6: Reward function for Round 2<br />
<small>
    - <a href=""https://github.com/stanfordnmbl/osim-rl/blob/516f25aced11ec5332b631f78674b99591ac41d7/osim/env/osim.py#L818-L877"">Code</a><br />
    - <a href=""https://github.com/stanfordnmbl/osim-rl/blob/master/envs/target/test_v_tgt.py"">Target velocity test code</a><br />
    - <a href=""http://osim-rl.stanford.edu/docs/nips2019/environment/#round-2"">Documentation</a><br />
    - <a href=""https://discourse.aicrowd.com/t/reward-for-round-2/2032"">Forum</a><br />
</small>
- AUG 5: <a href=""#get-started""><strong>New submission option available</strong></a><br />
- AUG 5: <a href=""http://osim-rl.stanford.edu/docs/nips2019/training/"">Example code of training an arm model</a><br />
- AUG 5: <a href=""#competition-tracks-and-prizes"">Google Cloud Credits to first 200 teams</a><br />
- JUL 17: Evaluation environment for Round 1 is set as <code class=""highlighter-rouge"">difficulty=2</code>, <code class=""highlighter-rouge"">model='3D'</code>, <code class=""highlighter-rouge"">project=True</code>, and <code class=""highlighter-rouge"">obs_as_dict=True</code><br />
- JUL 17: <a href=""#competition-tracks-and-prizes""><strong>Competition Tracks and Prizes</strong></a></p>
</blockquote>

<blockquote class=""update mt-0"">
  <p><strong>Simulation update</strong>:<br />
- OCT 20: <a href=""https://github.com/stanfordnmbl/osim-rl/commit/9be49551ab617768d30488c961b0ac78cf209c88""><strong>[v3.0.11]</strong> Round 2</a>: <code class=""highlighter-rouge"">difficulty=3</code>, <code class=""highlighter-rouge"">model='3D'</code>, <code class=""highlighter-rouge"">project=True</code>, <code class=""highlighter-rouge"">obs_as_dict=True</code><br />
- OCT 18: <a href=""https://github.com/stanfordnmbl/osim-rl/commit/cd143c2871d62e86ed267e12179cb10de77dd610""><strong>[v3.0.9]</strong></a> <s>Round 2</s><br />
- AUG 9: <a href=""https://github.com/stanfordnmbl/osim-rl/commit/55db058307cb60ffdc9852350f912e4bb317c411""><strong>[v3.0.6]</strong> success reward fixed</a><br />
- AUG 5: <a href=""https://github.com/stanfordnmbl/osim-rl/commit/12fc66c84750c6691db9164b9e5ecb7b30ecf6b5""><strong>[v3.0.5]</strong> observation_space updated</a><br />
- JUL 27: <a href=""https://github.com/stanfordnmbl/osim-rl/commit/5d9ec97501a671dfcfe1c2f7e2edf19d3615d891"">observation_space updated</a><br />
- JUN 28: <a href=""https://github.com/stanfordnmbl/osim-rl/commit/43ac8db3d89b4c14d4a9592ae49a75ef16c88849"">observation_space defined</a><br />
- JUN 15: <a href=""https://github.com/stanfordnmbl/osim-rl/commit/609be9610f5ede418260c04c0a3c95a16cc46282"">action space reordered</a></p>
</blockquote>

<h1 id=""introduction"">Learn to Move: Walk Around</h1>

<p>Welcome to the <strong>Learn to Move: Walk Around</strong> challenge, one of the official challenges in the <a href=""https://neurips.cc/Conferences/2019/CallForCompetitions"">NeurIPS 2019 Competition Track</a>. Your task is to develop a controller for a physiologically plausible 3D human model to walk or run following velocity commands with minimum effort. You are provided with a human musculoskeletal model and a physics-based simulation environment, OpenSim. There will be three tracks: 1) <strong>Best performance</strong>, 2) <strong>Novel ML solution</strong>, and 3) <strong>Novel biomechanical solution</strong>, where all the winners of each track will be awarded.</p>

<h2 id=""get-started"">Get Started</h2>
<ul>
  <li>Download code at <a href=""https://github.com/stanfordnmbl/osim-rl""><strong>the osim-rl github repo</strong></a>.</li>
  <li>Find details on the task and environment at <a href=""http://osim-rl.stanford.edu/docs/nips2019/environment/""><strong>the osim-rl project page: L2M2019 - Environment</strong></a>.</li>
  <li>Submit your solution
    <ul>
      <li>Option 1: <a href=""https://github.com/stanfordnmbl/neurips2019-learning-to-move-starter-kit""><strong>submit solution in docker container</strong></a></li>
      <li>Option 2: <a href=""https://github.com/stanfordnmbl/osim-rl/blob/master/examples/submission.py""><strong>run controller on server environment</strong></a></li>
    </ul>
  </li>
</ul>

<p><img src=""http://osim-rl.stanford.edu/docs/nips2019/fig/L2M2019.png"" alt=""Learn to Move"" /></p>

<h1 id=""competition-tracks-and-prizes"">Competition Tracks and Prizes</h1>
<ul>
  <li><strong>Best Performance Track</strong>
    <ul>
      <li>Rank in the top 50 of Round 1</li>
      <li>Get the highest score in Round 2</li>
      <li>Prizes for the winner:
        <ul>
          <li>NVIDIA GPU</li>
          <li>Travel grant (up to $1,000 to NeurIPS 2019 or Stanford)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>ML (Machine Learning) Track</strong> (find more details <a href=""http://osim-rl.stanford.edu/docs/nips2019/track_ML/"">here</a>)
    <ul>
      <li>Rank in the top 50 of Round 1</li>
      <li>Submit a paper describing your machine learning approach in <a href=""https://media.neurips.cc/Conferences/NeurIPS2019/Styles/neurips_2019.pdf"">NeurIPS format</a></li>
      <li>We will select the best paper</li>
      <li>Prizes for the winner:
        <ul>
          <li>2x NVIDIA GPU</li>
          <li>Travel grant (up to $1,000 to NeurIPS 2019 or Stanford)</li>
          <li>Paper acceptance into the NeurIPS Deep RL workshop</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>NM (Neuromechanics) Track</strong> (find more details <a href=""http://osim-rl.stanford.edu/docs/nips2019/track_NM/"">here</a>)
    <ul>
      <li>Rank in the top 50 of Round 1</li>
      <li>Submit a paper describing your novel neuromechanical findings/approach in <a href=""https://jneuroengrehab.biomedcentral.com/submission-guidelines/preparing-your-manuscript/research-articles"">Journal of NeuroEngineering and Rehabilitation format</a></li>
      <li>We will select the best paper</li>
      <li>Prizes for the winner:
        <ul>
          <li><a href=""https://www.xsens.com/products/xsens-mvn-animate/"">Xsens MVN Awinda &amp; MVN Analyze software (1 year license)</a></li>
          <li>Travel grant (up to $1,000 to NeurIPS 2019 or Stanford)</li>
          <li>Paper acceptance (upon revision) to the <a href=""https://jneuroengrehab.biomedcentral.com/"">Journal of NeuroEngineering and Rehabilitation</a> with no publication fee</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Other support</strong>
    <ul>
      <li>Top 50 of Round 1: $500 Google Cloud credit</li>
      <li>First 200 submissions: $200 Google Cloud credit
        <ul>
          <li>Credits will be sent out to the new teams every weekend</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h1 id=""timeline"">Timeline</h1>
<ul>
  <li>Round 1: June 6 ~ October 13, 2019
    <ul>
      <li>Get in top 50 to proceed to Round 2</li>
      <li>Evaluation will be done with <em>model=3D</em> and <em>difficulty=2</em></li>
    </ul>
  </li>
  <li>Round 2: October 14 ~ 27, 2019
    <ul>
      <li>See notification at the top of the page</li>
    </ul>
  </li>
  <li>Paper submission for Track 2 and 3: October 14 ~ <s>27</s> November 3, 2019</li>
  <li>Winners announcement: November <s>22</s> 29, 2019</li>
</ul>

<h1 id=""resources"">Resources</h1>
<ul>
  <li><a href=""http://osim-rl.stanford.edu/"">osim-rl project page</a></li>
  <li>Additional tools:
    <ul>
      <li><a href=""http://osim-rl.stanford.edu/docs/nips2019/controller1/"">Walking controller 1</a>, a physiologically plausible simple control model</li>
      <li><a href=""http://osim-rl.stanford.edu/docs/nips2019/experimental/"">Human gait data</a></li>
    </ul>
  </li>
  <li>Want to collaborate with experts in different fields? Find team mates at <a href=""https://discourse.aicrowd.com/t/call-for-collaboration/1283"">our forum</a></li>
  <li>Chat live with other participants on <a href=""https://gitter.im/crowdAI/NIPS-Learning-To-Run-Challenge"">Gitter</a></li>
</ul>

<h1 id=""sponsors-and-partners"">Sponsors and Partners</h1>
<p><a href=""https://cloud.google.com/""><img src=""https://dnczkxd1gcfu5.cloudfront.net/images/challenge_partners/image_file/27/google-cloud-logo.png"" height=""80"" title=""Google Cloud Platform"" /></a>
    <a href=""https://www.xsens.com/""><img src=""https://osim-rl.stanford.edu/docs/nips2019/fig/XSENS_logo.png"" height=""100"" title=""XSENS"" /></a>
    <a href=""https://www.nvidia.com/en-us/""><img src=""https://osim-rl.stanford.edu/docs/nips2019/fig/nvidia_logo.png"" height=""100"" title=""NVIDIA"" /></a>
    <a href=""https://ai.google/research/""><img src=""https://osim-rl.stanford.edu/docs/nips2019/fig/Google Logo.png"" height=""80"" title=""Google"" /></a>
    <a href=""https://www.tri.global/""><img src=""https://osim-rl.stanford.edu/docs/nips2019/fig/toyota_logo.png"" height=""100"" title=""TRI"" /></a>
    <a href=""https://jneuroengrehab.biomedcentral.com/""><img src=""https://osim-rl.stanford.edu/docs/nips2019/fig/JNER_logo.png"" height=""100"" title=""JNER"" /></a></p>

<h1 id=""more-info"">More Info</h1>

<h2 id=""contact-us"">Contact Us</h2>
<ul>
  <li>Seungmoon Song, <a href=""mailto:smsong@stanford.edu"" target=""_blank"">smsong@stanford.edu</a></li>
  <li>Łukasz Kidziński, <a href=""mailto:lukasz.kidzinski@stanford.edu"" target=""_blank"">lukasz.kidzinski@stanford.edu</a></li>
</ul>

<h2 id=""past-competitions"">Past competitions</h2>
<ul>
  <li><a href=""https://www.crowdai.org/challenges/nips-2018-ai-for-prosthetics-challenge"">NeurIPS 2018: AI for Prosthetics Challenge</a></li>
  <li><a href=""https://www.crowdai.org/challenges/nips-2017-learning-to-run"">NIPS 2017: Learning to Run</a></li>
</ul>

<h2 id=""media"">Media</h2>
<p><a href=""https://techcrunch.com/2017/08/07/dueling-ais-compete-in-learning-to-walk-secretly-manipulating-images-and-more-at-nips/""><img src=""https://seeklogo.com/images/T/techcrunch-logo-B444826970-seeklogo.com.png"" alt=""TechCrunch"" class=""img-logo"" /></a>
<a href=""http://med.stanford.edu/news/all-news/2018/07/virtual-athletes-compete-to-take-on-a-medical-challenge.html""><img src=""https://cehg.stanford.edu/sites/default/files/styles/large-scaled/public/c876e3f31ce0c5ba771fbdccdcb3c1dc.png?itok=-83R2NJW"" alt=""Stanford News"" class=""img-logo"" /></a>
<a href=""http://insights.globalspec.com/article/6167/watch-computer-generated-skeletons-run-for-cerebral-palsy""><img src=""https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/50fa0a860a431c503132b1fa0cac8377_logo%20%283%29.png"" alt=""IEEE"" class=""img-logo"" /></a></p>
"
210,"<p><em>Note: LifeCLEF Bird 2020 is divided into 2 subtasks (challenges). This challenge is about <strong>Stereo</strong>. For information on the <strong>Monophone</strong> challenge click <a href=""/challenges/lifeclef-2020-bird-monophone"" target=""_blank"">here</a>. Both challenges share the same dataset, so registering for one of these challenges will automatically give you access to the other one.</em></p>

<p><em>Note: Do not forget to read the <strong>Rules</strong> section on this page. Pressing the red <strong>Participate button</strong> leads you to a page where you have to agree with those rules. You will not be able to submit any results before agreeing with the rules.</em></p>

<p><em>Note: Before trying to submit results, read the <strong>Submission instructions</strong> section on this page.</em></p>

<h1 id=""challenge-description"">Challenge description</h1>

<p>…</p>

<h1 id=""data"">Data</h1>

<hr />
<p><em>As soon as the data is released it will be available under the “Resources” tab.</em></p>

<hr />

<p>More information will follow soon.</p>

<h1 id=""submission-instructions"">Submission instructions</h1>

<hr />
<p><em>As soon as the submission is open, you will find a “Create Submission” button on this page (next to the tabs).</em></p>

<hr />
<p><em>Before being allowed to submit your results, you have to first press the red participate button, which leads you to a page where you have to accept the challenges rules.</em></p>

<hr />

<p>More information regarding the submission instructions will be released soon.</p>

<h1 id=""evaluation-criteria"">Evaluation criteria</h1>

<p>More information regarding the evaluation criteria will be released soon.</p>

<h1 id=""rules"">Rules</h1>

<p>LifeCLEF lab is part of the Conference and Labs of the Evaluation Forum: CLEF 2020. CLEF 2020 consists of independent peer-reviewed workshops on a broad range of challenges in the fields of multilingual and multimodal information access evaluation, and a set of benchmarking activities carried in various labs designed to test different aspects of mono and cross-language Information retrieval systems. More details about the conference can be found <a href=""http://clef2020.clef-initiative.eu/"" target=""_blank"">here</a>.</p>

<p>Submitting a working note with the full description of the methods used in each run is mandatory. Any run that could not be reproduced thanks to its description in the working notes might be removed from the official publication of the results. Working notes are published within CEUR-WS proceedings, resulting in an assignment of an individual DOI (URN) and an indexing by many bibliography systems including DBLP. According to the CEUR-WS policies, a light review of the working notes will be conducted by LifeCLEF organizing committee to ensure quality. As an illustration, LifeCLEF 2019 working notes (task overviews and participant working notes) can be found within <a href=""http://ceur-ws.org/Vol-2380/"" target=""_blank"">CLEF 2019 CEUR-WS</a> proceedings.</p>

<h2 id=""important"">Important</h2>

<p>Participants of this challenge will automatically be registered at CLEF 2020. In order to be compliant with the CLEF registration requirements, please edit your profile by providing the following additional information:</p>

<ul>
  <li>
    <p>First name</p>
  </li>
  <li>
    <p>Last name</p>
  </li>
  <li>
    <p>Affiliation</p>
  </li>
  <li>
    <p>Address</p>
  </li>
  <li>
    <p>City</p>
  </li>
  <li>
    <p>Country</p>
  </li>
  <li>
    <p><em>Regarding the username, please choose a name that represents your team.</em></p>
  </li>
</ul>

<p><em>This information will not be publicly visible and will be exclusively used to contact you and to send the registration data to CLEF, which is the main organizer of all CLEF labs</em></p>

<h1 id=""citations"">Citations</h1>

<p>Information will be posted after the challenge ends.</p>

<h1 id=""prizes"">Prizes</h1>

<h2 id=""cloud-credit"">Cloud credit</h2>

<p>The winner of each of the challenge will be offered a cloud credit grant of 5k USD as part of Microsoft’s AI for earth program.</p>

<h2 id=""publication"">Publication</h2>

<p>LifeCLEF 2020 is an evaluation campaign that is being organized as part of the <a href=""http://clef2020.clef-initiative.eu/"" target=""_blank"">CLEF initiative</a> labs. The campaign offers several research tasks that welcome participation from teams around the world. The results of the campaign appear in the working notes proceedings, published by CEUR Workshop Proceedings (CEUR-WS.org). Selected contributions among the participants, will be invited for publication in the following year in the Springer Lecture Notes in Computer Science (LNCS) together with the annual lab overviews.</p>

<h1 id=""resources"">Resources</h1>

<h2 id=""contact-us"">Contact us</h2>

<p><em>Discussion Forum</em></p>

<ul>
  <li>You can ask questions related to this challenge on the Discussion Forum. Before asking a new question please make sure that question has not been asked before.</li>
  <li>Click on Discussion tab above or direct link: <a href=""https://discourse.aicrowd.com/c/lifeclef-2020-bird-stereo"" target=""_blank"">https://discourse.aicrowd.com/c/lifeclef-2020-bird-stereo</a></li>
</ul>

<p><em>Alternative channels</em></p>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

<ul>
  <li>stefan[dot]kahl[at]informatik[dot]tu-chemnitz[dot]de</li>
  <li>wp[at]xeno-canto[dot]org</li>
  <li>glotin[at]univ-tln[dot]fr</li>
  <li>herve[dot]goeau[at]cirad[dot]fr</li>
  <li>fabian-robert[dot]stoter[at]inria[dot]fr</li>
</ul>

<h2 id=""more-information"">More information</h2>

<p>You can find additional information on the challenge here:
<a href=""https://www.imageclef.org/BirdCLEF2020"" target=""_blank"">https://www.imageclef.org/BirdCLEF2020</a></p>

"
212,"<p><em>Note: Do not forget to read the <strong>Rules</strong> section on this page. Pressing the red <strong>Participate button</strong> leads you to a page where you have to agree with those rules. You will not be able to submit any results before agreeing with the rules.</em></p>

<p><em>Note: Before trying to submit results, read the <strong>Submission instructions</strong> section on this page.</em></p>

<h1 id=""challenge-description"">Challenge description</h1>

<h2 id=""motivation"">Motivation</h2>
<p>Automatic prediction of the list of species most likely to be observed at a given location is useful for many scenarios related to biodiversity management and conservation. First, it could improve species identification tools (whether automatic, semi-automatic or based on traditional field guides) by reducing the list of candidate species observable at a given site. More generally, this could facilitate biodiversity inventories through the development of location-based recommendation services (e.g. on mobile phones), encourage the involvement of citizen scientist observers, and accelerate the annotation and validation of species observations to produce large, high-quality data sets. Last but not least, this could be used for educational purposes through biodiversity discovery applications with features such as contextualized educational pathways.</p>

<h2 id=""task"">Task</h2>
<p>The occurrence dataset will be split in a training set with known species name labels and a test set used for the evaluation. For each occurrence (with geographic images) in the test set, the goal of the task will be to return a candidate set of species with associated confidence scores. The evaluation metrics will be the top-K accuracy (for different values of K) and a set-valued prediction metric to be precised later.</p>

<h1 id=""data"">Data</h1>
<p>The challenge relies on a collection of millions of occurrences of plants and animals in the US and France, coming from iNaturalist for the US and from Pl@ntNet for France. In addition to geo-coordinates and species name, each occurrence will be matched with a set of geographic images characterizing the local landscape and environment around the occurrence. In more detail, this will include: (i) high resolution (1 meter per pixel) remotely sensed imagery (from NAIP for the US and from IGN for France), (ii) bio-climatic rasters from WorldClim (1 km resolution) and (iii), land cover rasters (from NLCD for the US (30m resolution) and from Cesbio for France (10m resolution).
<strong>***
<em>As soon as the data is released it will be available under the “Resources” tab.</em>
**</strong>*</p>

<h1 id=""submission-instructions"">Submission instructions</h1>

<hr />
<p><em>As soon as the submission is open, you will find a “Create Submission” button on this page (next to the tabs).</em></p>

<hr />
<p><em>Before being allowed to submit your results, you have to first press the red participate button, which leads you to a page where you have to accept the challenges rules.</em></p>

<hr />

<p>More information regarding the submission instructions will be released soon.</p>

<h1 id=""evaluation-criteria"">Evaluation criteria</h1>

<p>The evaluation criterion will be an adaptive top-K accuracy. For each submission, we will first compute the threshold t of the confidence score that leads to keep K results on average over all test samples (yet each sample may be associated to a different number of predictions). Then, we will compute the pourcentage of test samples for which the correct species is in the kept results.</p>

<p>K will be fixed to 30, which corresponds to the average observed plant species richness across the inventoried plots of the French botanical data of Sophy [1].</p>

<h1 id=""rules"">Rules</h1>

<p>LifeCLEF lab is part of the Conference and Labs of the Evaluation Forum: CLEF 2020. CLEF 2020 consists of independent peer-reviewed workshops on a broad range of challenges in the fields of multilingual and multimodal information access evaluation, and a set of benchmarking activities carried in various labs designed to test different aspects of mono and cross-language Information retrieval systems. More details about the conference can be found <a href=""http://clef2020.clef-initiative.eu/"" target=""_blank"">here</a>.</p>

<p>Submitting a working note with the full description of the methods used in each run is mandatory. Any run that could not be reproduced thanks to its description in the working notes might be removed from the official publication of the results. Working notes are published within CEUR-WS proceedings, resulting in an assignment of an individual DOI (URN) and an indexing by many bibliography systems including DBLP. According to the CEUR-WS policies, a light review of the working notes will be conducted by LifeCLEF organizing committee to ensure quality. As an illustration, LifeCLEF 2019 working notes (task overviews and participant working notes) can be found within <a href=""http://ceur-ws.org/Vol-2380/"" target=""_blank"">CLEF 2019 CEUR-WS</a> proceedings.</p>

<h2 id=""important"">Important</h2>

<p>Participants of this challenge will automatically be registered at CLEF 2020. In order to be compliant with the CLEF registration requirements, please edit your profile by providing the following additional information:</p>

<ul>
  <li>
    <p>First name</p>
  </li>
  <li>
    <p>Last name</p>
  </li>
  <li>
    <p>Affiliation</p>
  </li>
  <li>
    <p>Address</p>
  </li>
  <li>
    <p>City</p>
  </li>
  <li>
    <p>Country</p>
  </li>
  <li>
    <p><em>Regarding the username, please choose a name that represents your team.</em></p>
  </li>
</ul>

<p><em>This information will not be publicly visible and will be exclusively used to contact you and to send the registration data to CLEF, which is the main organizer of all CLEF labs</em></p>

<h1 id=""citations"">Citations</h1>

<p>Information will be posted after the challenge ends.</p>

<h1 id=""prizes"">Prizes</h1>

<h2 id=""cloud-credit"">Cloud credit</h2>

<p>The winner of each of the challenge will be offered a cloud credit grant of 5k USD as part of Microsoft’s AI for earth program.</p>

<h2 id=""publication"">Publication</h2>

<p>LifeCLEF 2020 is an evaluation campaign that is being organized as part of the <a href=""http://clef2020.clef-initiative.eu/"" target=""_blank"">CLEF initiative</a> labs. The campaign offers several research tasks that welcome participation from teams around the world. The results of the campaign appear in the working notes proceedings, published by CEUR Workshop Proceedings (CEUR-WS.org). Selected contributions among the participants, will be invited for publication in the following year in the Springer Lecture Notes in Computer Science (LNCS) together with the annual lab overviews.</p>

<h1 id=""resources"">Resources</h1>

<h2 id=""contact-us"">Contact us</h2>

<p><em>Discussion Forum</em></p>

<ul>
  <li>You can ask questions related to this challenge on the Discussion Forum. Before asking a new question please make sure that question has not been asked before.</li>
  <li>Click on Discussion tab above or direct link: <a href=""https://discourse.aicrowd.com/c/lifeclef-2020-geo"" target=""_blank"">https://discourse.aicrowd.com/c/lifeclef-2020-geo</a></li>
</ul>

<p><em>Alternative channels</em></p>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

<ul>
  <li>benjamin[dot]deneu[at]inria[dot]fr</li>
  <li>christophe[dot]botella[at]cirad[dot]fr</li>
  <li>maximilien[dot]servajean[at]lirmm[dot]fr</li>
  <li>ecole[at]caltech[dot]edu</li>
</ul>

<h2 id=""more-information"">More information</h2>

<p>You can find additional information on the challenge here:
<a href=""https://www.imageclef.org/GeoLifeCLEF2020"" target=""_blank"">https://www.imageclef.org/GeoLifeCLEF2020</a></p>

<h2 id=""references"">References</h2>

<p>[1] Ruffray, P., B.H.G.r.G.H.M.: “sophy”, une banque de données phytosociologiques; son intérêt pour la conservation de la nature. Actes du colloque “Plantes sauvages et menacées de France: bilan et protection”, Brest, 8-10 octobre 1987 pp. 129–150 (1989).</p>

"
223,"<p><em>Note: ImageCLEF 2020 VQA-Med includes 2 tasks. This page is about the Visual Question Generation (<strong>VQG</strong>) task. For information about the Visual Question Answering task (<strong>VQA</strong>) click <a href=""/challenges/imageclef-2020-vqa-med-vqa"" target=""_blank"">here</a>.   Both challenges share the same dataset, so registering for one of these challenges will automatically give you access to the other one.</em></p>

<hr />

<p><em>Note: Please do not forget to read the <strong>Rules</strong> section on this page. Pressing the red <strong>Participate button</strong> leads you to a page where you have to agree with those rules. You will not be able to submit any results before agreeing with the rules.</em></p>

<p><em>Note: Before trying to submit results, please read the <strong>Submission instructions</strong> section on this page.</em></p>

<h1 id=""vqg-task-description"">VQG task description</h1>

<p>VQG is introduced for the first time in this third edition of the VQA-Med challenge. The task consists in generating relevant natural language questions about radiology images using their visual content.</p>

<h1 id=""data"">Data</h1>

<hr />
<p><em>As soon as the data is released it will be available under the “Resources” tab.</em></p>

<hr />

<h1 id=""schedule"">Schedule</h1>

<ul>
  <li>25/02/2020: Release of the training and validation datasets</li>
  <li>10/04/2020: Release of the test set</li>
  <li>10/05/2020: Run submission deadline</li>
</ul>

<h1 id=""submission-instructions"">Submission instructions</h1>

<hr />
<p><em>As soon as the submission is open, you will find a “Create Submission” button on this page (next to the tabs).</em></p>

<hr />
<p><em>Before being allowed to submit your results, you have to first press the red participate button, which leads you to a page where you have to accept the challenges rules.</em></p>

<hr />

<p>More information regarding the submission instructions will be released soon.</p>

<h1 id=""evaluation-criteria"">Evaluation criteria</h1>

<p>More information regarding the evaluation criteria will be released soon.</p>

<h1 id=""rules"">Rules</h1>

<p><em>Note: In order to participate in this challenge you have to sign an End User Agreement (EUA). You will find more information on the ‘Resources’ tab.</em></p>

<p>ImageCLEF lab is part of the Conference and Labs of the Evaluation Forum: CLEF 2020. CLEF 2020 consists of independent peer-reviewed workshops on a broad range of challenges in the fields of multilingual and multimodal information access evaluation, and a set of benchmarking activities carried in various labs designed to test different aspects of mono and cross-language Information retrieval systems. More details about the conference can be found <a href=""http://clef2020.clef-initiative.eu/"" target=""_blank"">here</a> .</p>

<p>Submitting a working note with the full description of the methods used in each run is mandatory. Any run that could not be reproduced thanks to its description in the working notes might be removed from the official publication of the results. Working notes are published within CEUR-WS proceedings, resulting in an assignment of an individual DOI (URN) and an indexing by many bibliography systems including DBLP. According to the CEUR-WS policies, a light review of the working notes will be conducted by ImageCLEF organizing committee to ensure quality. As an illustration, ImageCLEF 2019 working notes (task overviews and participant working notes) can be found within <a href=""http://ceur-ws.org/Vol-2380/"" target=""_blank"">CLEF 2019 CEUR-WS</a> proceedings.</p>

<h2 id=""important"">Important</h2>

<p>Participants of this challenge will automatically be registered at CLEF 2020. In order to be compliant with the CLEF registration requirements, please edit your profile by providing the following additional information:</p>

<ul>
  <li>
    <p>First name</p>
  </li>
  <li>
    <p>Last name</p>
  </li>
  <li>
    <p>Affiliation</p>
  </li>
  <li>
    <p>Address</p>
  </li>
  <li>
    <p>City</p>
  </li>
  <li>
    <p>Country</p>
  </li>
  <li>
    <p><em>Regarding the username, please choose a name that represents your team.</em></p>
  </li>
</ul>

<p><em>This information will not be publicly visible and will be exclusively used to contact you and to send the registration data to CLEF, which is the main organizer of all CLEF labs</em></p>

<h2 id=""participating-as-an-individual-non-affiliated-researcher"">Participating as an individual (non affiliated) researcher</h2>

<p>We welcome individual researchers, i.e. not affiliated to any institution, to participate. We kindly ask you to provide us with a motivation letter containing the following information:</p>

<ul>
  <li>
    <p>the presentation of your most relevant research activities related to the task/tasks</p>
  </li>
  <li>
    <p>your motivation for participating in the task/tasks and how you want to exploit the results</p>
  </li>
  <li>
    <p>a list of the most relevant 5 publications (if applicable)</p>
  </li>
  <li>
    <p>the link to your personal webpage</p>
  </li>
</ul>

<p>The motivation letter should be directly concatenated to the End User Agreement document or sent as a PDF file to bionescu at imag dot pub dot ro. The request will be analyzed by the ImageCLEF organizing committee. We reserve the right to refuse any applicants whose experience in the field is too narrow, and would therefore most likely prevent them from being able to finish the task/tasks.</p>

<h1 id=""citations"">Citations</h1>

<p>Information will be posted after the challenge ends.</p>

<h2 id=""publication"">Publication</h2>

<p>ImageCLEF 2020 is an evaluation campaign that is being organized as part of the <a href=""http://clef2020.clef-initiative.eu/"" target=""_blank"">CLEF initiative</a> labs. The campaign offers several research tasks that welcome participation from teams around the world. The results of the campaign appear in the working notes proceedings, published by CEUR Workshop Proceedings (CEUR-WS.org). Selected contributions among the participants, will be invited for publication in the following year in the Springer Lecture Notes in Computer Science (LNCS) together with the annual lab overviews.</p>

<h1 id=""resources"">Resources</h1>

<h2 id=""contact-us"">Contact us</h2>

<p><em>Discussion Forum</em></p>

<ul>
  <li>You can ask questions related to this challenge on the Discussion Forum. Before asking a new question please make sure that question has not been asked before.</li>
  <li>Click on Discussion tab above or direct link: <a href=""https://discourse.aicrowd.com/c/imageclef-2020-vqa-med-vqg"" target=""_blank"">https://discourse.aicrowd.com/c/imageclef-2020-vqa-med-vqg</a></li>
</ul>

<p><em>Alternative channels</em></p>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

<ul>
  <li>asma[dot]benabacha[at]nih[dot]gov</li>
  <li>vivek[dot]datla[at]philips[dot]com</li>
  <li>sadidhasan[at]gmail[dot]com</li>
  <li>joey[dot]liu[at]philips[dot]com</li>
  <li>ddemner[at]mail[dot]nih[dot]gov</li>
  <li>henning[dot]mueller[at]hevs[dot]ch</li>
</ul>

<h2 id=""more-information"">More information</h2>

<p>You can find additional information on the challenge here:
<a href=""https://www.imageclef.org/2020/medical/vqa"" target=""_blank"">https://www.imageclef.org/2020/medical/vqa</a></p>

<p>Mailing list: https://groups.google.com/d/forum/imageclef-vqa-med</p>
"
9,"<blockquote class=""update"">
  <p>The Uber Prize tests your <strong>machine learning skills</strong> and <strong>urban planning abilities</strong> as you develop algorithms to find a <em>policy</em> that will best motivate sustainable, equitable, and reliable use of a large metropolitan transportation system.
Policies developed by contestants will be executed in a multiagent simulation environment, which will produce outputs that describe how the system reacts to the proposed changes. Contestants will be scored by how well they meet evaluation criteria. To get started right away, go to the Resources Section below.</p>
</blockquote>

<blockquote class=""update"">
  <p>Want to make your first submission ? Here is how : <a href=""https://gitlab.aicrowd.com/uber/uber-hackathon-sample-submission"">https://gitlab.aicrowd.com/uber/uber-hackathon-sample-submission</a></p>
</blockquote>

<p><img src=""https://s3.eu-central-1.amazonaws.com/aicrowd-static/uber/Poster_Hackathon.png"" alt=""  "" /></p>

<p>For this Hackathon (and Phase I of the Uber Prize), contestants are asked to <strong>optimize the transportation network for a sample of citizens from a mock city: Sioux Faux</strong> (a virtual instantiation of Sioux Falls, a real city in South Dakota). The city’s 157,000 citizens travel between activities using either their personal automobiles, buses provided via a public transit system, on-demand rides, walking, or a combination of multiple modes in accordance with their preferences. <br /> <br /> To participate, contestants will optimize a set of <strong>inputs</strong> represent possible city-wide policies regarding transportation: policies that control <em>operational</em> and <em>financial</em> aspects of mass transit as well as <em>incentives</em> that influence the use of mass transit and on-demand ride services. An agent-based simulation of the city takes in the contestant-defined inputs and produces a set of <strong>output</strong> metrics and <strong>score components</strong> that evaluate system-wide level of service, congestion, environmental sustainability, and the net costs of the policy interventions. <br /> <br /> The metrics evaluated from contestant submissions will be judged in comparison to the corresponding metrics from the <em>business as usual</em>, or <em>do nothing</em>, scenario. The <strong>total score</strong> for a contestant submission is computed as the average of the standardized score components.</p>

<h1 id=""resources"">Resources</h1>

<ol>
  <li>
    <p><strong>To get started</strong>, competitors can use the  <a href=""https://gitlab.aicrowd.com/uber/Uber-Prize-Starter-Kit""> <em>Uber Prize Starter Kit</em> </a>on GitLab. It will give them a quick overview of the transportation challenge of Sioux Faux, the technical knowledge needed to master the use of the BISTRO policy optimization framework as well as explanations on how to submit solutions.</p>
  </li>
  <li>
    <p><strong>To get a full description of the models of Sioux Faux</strong>, the typical control inputs at the disposal of the city (hence the competitors) and their effects on the city and the corresponding outputs, read <em>Part I</em> of the <a href=""https://s3.eu-central-1.amazonaws.com/aicrowd-static/uber/PS_SD_Uber_hackathon_2019.pdf""><em>the Problem Statement and Scoring document</em></a> are used in the scoring of the solutions of the competitors. Part I is important to understand how the Uber Prize will be of relevance for real-world planning. While it is recommended to read in its entirety, some parts can be skipped upon first reading, to get started faster.</p>
  </li>
  <li>
    <p><strong>Scoring and judging</strong> is explained in <em>Part II</em> of the <a href=""https://s3.eu-central-1.amazonaws.com/aicrowd-static/uber/PS_SD_Uber_hackathon_2019.pdf""><em>the Problem Statement and Scoring document</em></a> . This part provides several scenarios that can be used by the contestants as a starter point. In particular, a <em>business as usual</em> (BAU) scenario provides the current modus operandi of Sioux Faux, and can be considered as neutral. Solutions with scores better than BAU (which improve Sioux Faux)  are presented. In particular corresponding inputs and outputs are plotted, along with interpretations in terms of policies for the city, to help contestants gain insights on their solutions.</p>
  </li>
</ol>

<p><img src=""https://s3.eu-central-1.amazonaws.com/aicrowd-static/uber/bus_watercolor.png"" alt=""  "" /></p>

<h1 id=""evaluation-criteria"">Evaluation Criteria</h1>

<p>The quality of the new policy-based transportation system is evaluated based upon a <strong>comparison against the business as usual (BAU) scenario</strong>. The BAU scenario represents the baseline or “current” status-quo of the Sioux Faux transportation system. In other words, this is a “do-nothing” approach. This comparison answers the following question: <br />
 <strong>How will the new policy improve over the current state of the transportation system</strong>?</p>

<p>The performance of this new policy-based transportation system is measured by a <strong>scoring function</strong>, which computes the weighted sum of five groups of outputs:</p>

<ol>
  <li>How much <em>congestion</em> did the agents experience during the day?</li>
  <li>What <em>level of service</em> did the transportation system offer to agents?</li>
  <li>What were the <em>budgetary</em> constraints and impacts?</li>
  <li>How easily can the agents <em>access</em> opportunities or points of interest via available modes of travel?</li>
  <li>How <em>sustainable</em> is the transportation system?</li>
</ol>

<p>A total submission score smaller than 0 indicates that, under the evaluated policy portfolio, the system is performing better compared to the BAU scenario.</p>

<p><img src=""https://s3.eu-central-1.amazonaws.com/aicrowd-static/uber/B99745279Z.1_20190201062922_000_GA02C5C5F.2-0.jpg"" alt="""" /></p>

<h1 id=""business-rules"">Business rules</h1>

<p><strong>The hackathon will be judged in the <a href=""https://docs.google.com/document/d/1hQMwbbzKV1rqKpfSY30j0mTjh-lBEzbaJtYnx_EfcMY/edit"">following way</a></strong>. In order to win, the BISTRO solution provided by the competitors must adhere to the <em>business rules</em> outlined below. These are in place to make sure that the solutions provided by the competitors are compliant with common practices in policy and planning, as follows:</p>

<ul>
  <li>
    <p>The bus frequency cannot change more than four times across the day. More precisely, there can be no more than five distinct bus service periods (this mimics the typical delineation: am peak, midday, pm peak, evening, late night/early morning).</p>
  </li>
  <li>
    <p>Bus fares and mode incentives may not isolate a single age; fares
and incentives must be defined in bins no smaller than five years (or
$5,000, or other) in range.</p>
  </li>
  <li>
    <p>Bus route headways may be no more than 120 minutes and no less than 3 minutes.</p>
  </li>
</ul>

<h1 id=""submissions-and-judging-criteria"">Submissions and Judging Criteria</h1>

<h2 id=""by-march-11-2019-1159pm"">By March 11, 2019, 11:59pm</h2>

<ul>
  <li>
    <p><em>All code</em> used to generate the solution, graphics, visualization and outputs must be submitted along with the <em>solution (inputs)</em>, with <em>proper comments</em>.</p>
  </li>
  <li>
    <p>Along with their submission, the teams will submit a <em>report documenting their findings</em>, as well as their approaches, designs and conclusions.</p>
  </li>
</ul>

<p>The competitors with the top 10 scores on the <a href=""https://www.aicrowd.com/challenges/uber-2019-ml-hackathon/leaderboards"">AICrowd leaderboard</a> will become finalists <br /></p>

<h2 id=""on-march-13-2019"">On March 13, 2019</h2>

<ul>
  <li>
    <p>The 10 finalists will prepare <em>presentations</em> to be given during the final round of judging .</p>
  </li>
  <li>
    <p>After the presentations, the judges will confer and select <em>three winners</em> for the Hackathon from the 10 finalists. The three prizes will be determined using a final score comprised of a weighted sum along three axes of delineation:</p>
  </li>
</ul>

<ol>
  <li>Quantitative scores, <br /></li>
  <li>The quality of their supporting documents and presentation, and <br /></li>
  <li>Citizenship scores. <br /> <br /> 
Each prize will be determined using the same three axes, however the weight of each element’s contribution to the final score will change. For more details, refer to Section 6.2 of the Scoring Document.</li>
</ol>

<h1 id=""prizes"">Prizes</h1>

<p>There will be three prizes:</p>

<ul>
  <li>
    <p><strong>Best in Show:</strong>  Winning team will share a prize worth $10,000 and have dinner with Dara<br /></p>
  </li>
  <li>
    <p><strong>Most Creative:</strong> Winning team will join the dinner with Dara <br /></p>
  </li>
  <li>
    <p><strong>Best Citizen:</strong> Winning team will join the dinner with Dara</p>
  </li>
</ul>

<h1 id=""contact-us"">Contact us</h1>

<ul>
  <li>If you have any questions concerning the competition, please, review first the <a href=""https://gitlab.aicrowd.com/uber/Uber-Prize-Starter-Kit/blob/master/docs/FAQ.md"">FAQ page</a> of the Starter Kit. <br /></li>
  <li>If your question was note answered in the FAQ page, you can <strong>file an issue</strong> in the <strong><a href=""https://gitlab.aicrowd.com/uber/Uber-Prize-Starter-Kit/issues"">issue tracker of the Starter Kit</a></strong> or ask it in the <a href=""https://uchat.uberinternal.com/uber/channels/2019-uber-prize-ml-hackathon"">2019 Uber Prize Mal Hackathon Uchat channel</a>.<br /></li>
  <li>If your problem has still not been resolved, you can contact the Hackathon organizing team at <strong>ml-hackathon@uber.com</strong></li>
</ul>

<h1 id=""partners"">Partners</h1>

<p><img src=""https://s3.eu-central-1.amazonaws.com/aicrowd-static/uber/ITSlogo_long_blue_med.jpg"" alt="" "" />
<img src=""https://s3.eu-central-1.amazonaws.com/aicrowd-static/uber/client_2_s2_r4_v1506729386119_main.png"" alt="" "" class=""img-logo"" /></p>
"
68,"
<h1 id=""extract-roads-from-satellite-images"">Extract roads from satellite images</h1>

<p>For this problem, we provide a set of satellite/aerial images acquired from GoogleMaps. We also provide ground-truth images where each pixel is labeled as {road, background}. Your goal is to train a classifier to segment roads in these images, i.e. assign a label {road=1, background=0} to each pixel. Please see detailed instructions on the course github.</p>

<h1 id=""dataset"">Dataset</h1>

<h2 id=""file-descriptions--"">File descriptions -</h2>

<ul>
  <li>training.zip - the training set consisting of images with their ground truth</li>
  <li>test_set_images.zip - the test set</li>
  <li>sampleSubmission.csv - a sample submission file in the correct format</li>
  <li>mask_to_submission.py - script to make a submission file from a binary image</li>
  <li>submission_to_mask.py - script to reconstruct an image from the sample submission file</li>
</ul>

<p>The sample submission file contains two columns:</p>

<ul>
  <li>
    <p>The first column corresponds to the image id followed by the x and y top-left coordinate of the image patch (16x16 pixels)</p>
  </li>
  <li>
    <p>The second column is the label assigned to the image patch</p>
  </li>
</ul>

<h1 id=""evaluation"">Evaluation</h1>

<p>Your algorithm is evaluated according to the following criterion:</p>

<ul>
  <li><a href=""https://en.wikipedia.org/wiki/F1_score""> F1 score </a> (this combines the two numbers of precision and recall)</li>
</ul>

<h1 id=""rules"">Rules</h1>

<p>Each participant is allowed to make <strong>5 submissions per day</strong> (i.e. up to 15 submissions per team per day). Failed submissions (e.g. wrong submission file format) do not count.</p>
"
215,"<p><em>Note: ImageCLEF Lifelog 2020 is divided into 2 subtasks (challenges). This challenge is about Lifelog moment retrieval (<strong>LMRT</strong>). For information on the  Sport Performance Lifelog (<strong>SPLL</strong>) challenge click <a href=""/challenges/imageclef-2020-lifelog-spll"" target=""_blank"">here</a>. Both challenges share the same dataset, so registering for one of these challenges will automatically give you access to the other one.</em></p>

<p><em>Note: Do not forget to read the <strong>Rules</strong> section on this page. Pressing the red <strong>Participate button</strong> leads you to a page where you have to agree with those rules. You will not be able to submit any results before agreeing with the rules.</em></p>

<p><em>Note: Before trying to submit results, read the <strong>Submission instructions</strong> section on this page.</em></p>

<h1 id=""lifelog-schedule"">Lifelog Schedule</h1>
<ul>
  <li>10.02.2020: registration opens</li>
  <li>17.02.2020: development data released</li>
  <li>14.04.2020: test data release starts</li>
  <li>15.05.2020: deadline for submitting the participants runs</li>
  <li>18.05.2020: release of the processed results by the task organizers</li>
  <li>30.05.2020: deadline for submission of working notes papers by the participants</li>
  <li>15.06.2020: notification of acceptance of the working notes papers</li>
  <li>29.06.2020: camera ready working notes papers</li>
  <li>22-25.09.2020: CLEF 2020<a href=""https://clef2020.clef-initiative.eu/"">https://clef2020.clef-initiative.eu/</a>, Thessaloniki, Greece</li>
</ul>

<h1 id=""challenge-description"">Challenge description</h1>
<p>Lifelog Core Task: lifelog moment retrieval (LMRT, 4th edition). The participants are required to retrieve a number of specific predefined activities in a lifelogger’s life. For example, they are asked to return the relevant moments for the query “Find the moment(s) when the lifelogger was having an icecream on the beach”. Particular attention should be paid to the diversification of the selected moments with respect to the target scenario. Data. A new rich multimodal dataset will be used (e.g., about 4.5 months in total of data from three lifeloggers, 1,500-2,500 images per day, visual concepts, semantic content, biometrics information, music listening history, computer usage).
…</p>

<h1 id=""data"">Data</h1>
<p>The 4th edition of this task will come with new, enriched data, focused on daily living activities and the chronological order of the moments and a completely new task for assessing sport performance.
*****
<em>As soon as the data is released it will be available under the “Resources” tab.</em></p>

<hr />

<p>More information will follow soon.</p>

<h1 id=""submission-instructions"">Submission instructions</h1>

<hr />
<p><em>As soon as the submission is open, you will find a “Create Submission” button on this page (next to the tabs).</em></p>

<hr />
<p><em>Before being allowed to submit your results, you have to first press the red participate button, which leads you to a page where you have to accept the challenges rules.</em></p>

<hr />

<p>More information regarding the submission instructions will be released soon.</p>

<h1 id=""evaluation-criteria"">Evaluation criteria</h1>
<p>For assessing performance, classic metrics will be deployed. These metrics are:</p>

<p>Cluster Recall at X (CR@X) - a metric that assesses how many different clusters from the ground truth are represented among the top X results;
Precision at X (P@X) - measures the number of relevant photos among the top X results;
F1-measure at X (F1@X) - the harmonic mean of the previous two.
Various cut off points are to be considered, e.g., X=5, 10, 20, 30, 40, 50. Official ranking metrics this year will be the F1-measure@10, which gives equal importance to diversity (via CR@10) and relevance (via P@10).</p>

<p>Participants are allowed to undertake the sub-tasks in an interactive or automatic manner. For interactive submissions, a maximum of five minutes of search time is allowed per topic. In particular, the organizers would like to emphasize methods that allow interaction with real users (via Relevance Feedback (RF), for example), i.e., beside of the best performance, the way of interaction (like number of iterations using RF), or innovation level of the method (for example, new way to interact with real users) are encouraged.</p>

<h1 id=""rules"">Rules</h1>

<p><em>Note: In order to participate in this challenge you have to sign an End User Agreement (EUA). You will find more information on the ‘Resources’ tab.</em></p>

<p>ImageCLEF lab is part of the Conference and Labs of the Evaluation Forum: CLEF 2020. CLEF 2020 consists of independent peer-reviewed workshops on a broad range of challenges in the fields of multilingual and multimodal information access evaluation, and a set of benchmarking activities carried in various labs designed to test different aspects of mono and cross-language Information retrieval systems. More details about the conference can be found <a href=""http://clef2020.clef-initiative.eu/"" target=""_blank"">here</a> .</p>

<p>Submitting a working note with the full description of the methods used in each run is mandatory. Any run that could not be reproduced thanks to its description in the working notes might be removed from the official publication of the results. Working notes are published within CEUR-WS proceedings, resulting in an assignment of an individual DOI (URN) and an indexing by many bibliography systems including DBLP. According to the CEUR-WS policies, a light review of the working notes will be conducted by ImageCLEF organizing committee to ensure quality. As an illustration, ImageCLEF 2019 working notes (task overviews and participant working notes) can be found within <a href=""http://ceur-ws.org/Vol-2380/"" target=""_blank"">CLEF 2019 CEUR-WS</a> proceedings.</p>

<h2 id=""important"">Important</h2>

<p>Participants of this challenge will automatically be registered at CLEF 2020. In order to be compliant with the CLEF registration requirements, please edit your profile by providing the following additional information:</p>

<ul>
  <li>
    <p>First name</p>
  </li>
  <li>
    <p>Last name</p>
  </li>
  <li>
    <p>Affiliation</p>
  </li>
  <li>
    <p>Address</p>
  </li>
  <li>
    <p>City</p>
  </li>
  <li>
    <p>Country</p>
  </li>
  <li>
    <p><em>Regarding the username, please choose a name that represents your team.</em></p>
  </li>
</ul>

<p><em>This information will not be publicly visible and will be exclusively used to contact you and to send the registration data to CLEF, which is the main organizer of all CLEF labs</em></p>

<h2 id=""participating-as-an-individual-non-affiliated-researcher"">Participating as an individual (non affiliated) researcher</h2>

<p>We welcome individual researchers, i.e. not affiliated to any institution, to participate. We kindly ask you to provide us with a motivation letter containing the following information:</p>

<ul>
  <li>
    <p>the presentation of your most relevant research activities related to the task/tasks</p>
  </li>
  <li>
    <p>your motivation for participating in the task/tasks and how you want to exploit the results</p>
  </li>
  <li>
    <p>a list of the most relevant 5 publications (if applicable)</p>
  </li>
  <li>
    <p>the link to your personal webpage</p>
  </li>
</ul>

<p>The motivation letter should be directly concatenated to the End User Agreement document or sent as a PDF file to bionescu at imag dot pub dot ro. The request will be analyzed by the ImageCLEF organizing committee. We reserve the right to refuse any applicants whose experience in the field is too narrow, and would therefore most likely prevent them from being able to finish the task/tasks.</p>

<h1 id=""citations"">Citations</h1>

<p>Information will be posted after the challenge ends.</p>

<h1 id=""prizes"">Prizes</h1>

<h2 id=""publication"">Publication</h2>

<p>ImageCLEF 2020 is an evaluation campaign that is being organized as part of the <a href=""http://clef2020.clef-initiative.eu/"" target=""_blank"">CLEF initiative</a> labs. The campaign offers several research tasks that welcome participation from teams around the world. The results of the campaign appear in the working notes proceedings, published by CEUR Workshop Proceedings (CEUR-WS.org). Selected contributions among the participants, will be invited for publication in the following year in the Springer Lecture Notes in Computer Science (LNCS) together with the annual lab overviews.</p>

<h1 id=""resources"">Resources</h1>

<h2 id=""contact-us"">Contact us</h2>

<p><em>Discussion Forum</em>
- You can ask questions related to this challenge on the Discussion Forum. Before asking a new question please make sure that question has not been asked before.
- Click on Discussion tab above or direct link: <a href=""https://discourse.aicrowd.com/c/imageclef-2020-lifelog-lmrt"" target=""_blank"">https://discourse.aicrowd.com/c/imageclef-2020-lifelog-lmrt</a></p>

<p><em>Alternative channels</em></p>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

<ul>
  <li>ductien.dangnguyen[at]uib[dot]no</li>
  <li>zhou.liting2[at]mail[dot]dcu[dot]ie</li>
  <li>tu.ninhvan[at]adaptcentre[dot]ie</li>
  <li>tukhiem.le4[at]mail[dot]dcu[dot]ie</li>
  <li>luca.piras[at]diee[dot]unica[dot]it</li>
  <li>michael[at]simula[dot]no</li>
  <li>tmtriet[at]hcmus[dot]edu[dot]vn</li>
  <li>mlux[at]itec[dot]aau[dot]at</li>
  <li>cgurrin[at]computing[dot]dcu[dot]ie</li>
</ul>

<h2 id=""more-information"">More information</h2>

<p>You can find additional information on the challenge here:
<a href=""https://www.imageclef.org/2020/lifelog"" target=""_blank"">https://www.imageclef.org/2020/lifelog</a></p>

"
217,"<p><em>Note: ImageCLEF Coral 2020 is divided into 2 subtasks (challenges). This challenge is about <strong>Annotation and Localisation</strong>. For information on the <strong>Pixel-wise Parsing</strong> challenge click <a href=""/challenges/imageclef-2020-coral-pixel-wise-parsing"" target=""_blank"">here</a>. Both challenges share the same dataset, so registering for one of these challenges will automatically give you access to the other one.</em></p>

<p><em>Note: Do not forget to read the <strong>Rules</strong> section on this page. Pressing the red <strong>Participate button</strong> leads you to a page where you have to agree with those rules. You will not be able to submit any results before agreeing with the rules.</em></p>

<p><em>Note: Before trying to submit results, read the <strong>Submission instructions</strong> section on this page.</em></p>

<h1 id=""challenge-description"">Challenge description</h1>

<p>The increasing use of structure-from-motion photogrammetry for modelling large-scale environments from action cameras attached to drones has driven the next-generation of visualisation techniques that can be used in augmented and virtual reality headsets. It has also created a need to have such models labelled, with objects such as people, buildings, vehicles, terrain, etc. all essential for machine learning techniques to automatically identify as areas of interest and to label them appropriately. However, the complexity of the images makes impossible for human annotators to assess the contents of images on a large scale.
Advances in automatically annotating images for complexity and benthic composition have been promising, and we are interested in automatically identify areas of interest and to label them appropriately for monitoring coral reefs. Coral reefs are in danger of being lost within the next 30 years, and with them the ecosystems they support. This catastrophe will not only see the extinction of many marine species, but also create a humanitarian crisis on a global scale for the billions of humans who rely on reef services. By monitoring the changes and composition of coral reefs we can help prioritise conservation efforts.</p>

<p><strong>This task</strong> requires the participants to label the images with types of benthic substrate together with their bounding box in the image. Each image is provided with possible class types. For each image, participants will produce a set of bounding boxes, predicting the benthic substrate for each bounding box in the image.</p>

<h1 id=""data"">Data</h1>
<hr />
<p><em>As soon as the data is released it will be available under the “Resources” tab.</em></p>

<hr />

<p>More information will follow soon.</p>

<h1 id=""submission-instructions"">Submission instructions</h1>

<hr />
<p><em>As soon as the submission is open, you will find a “Create Submission” button on this page (next to the tabs).</em></p>

<hr />
<p><em>Before being allowed to submit your results, you have to first press the red participate button, which leads you to a page where you have to accept the challenges rules.</em></p>

<hr />

<p>Participants will be permitted to submit up to 10 runs. External training data is allowed and encouraged.
Each system run will consist of a single ASCII plain text file. The results of each test set should be given in separate lines in the text file. The format of the text file is as follows:</p>

<p>[image_ID/document_ID] [results]</p>

<p>The results of each test set image should be given in separate lines, each line providing only up to 500 localised substrates. The format has characters to separate the elements, semicolon ‘;’ for the substrates, colon ‘:’ for the confidence, comma ‘,’ to separate multiple bounding boxes, and ‘x’ and ‘+’ for the size-offset bounding box format, i.e.:</p>

<p>[image_ID];[substrate1] [[confidence1,1]:][width1,1]x[height1,1]+[xmin1,1]+[ymin1,1],[[confidence1,2]:][width1,2]x[height1,2]+[xmin1,2]+[ymin1,2],…;[substrate2] …</p>

<p>[confidence] are floating point values 0-1 for which a higher value means a higher score.</p>

<p>For example, in the development set format (notice that there are 2 bounding boxes for substrate c_soft_coral):</p>

<ul>
  <li>
    <p>2018_0714_112604_057 0 c_hard_coral_branching 1 891 540 1757 1143</p>
  </li>
  <li>
    <p>2018_0714_112604_057 3 c_soft_coral 1 2724 1368 2825 1507</p>
  </li>
  <li>
    <p>2018_0714_112604_057 4 c_soft_coral 1 2622 1576 2777 1731</p>
  </li>
</ul>

<p>In the submission format, it would be a line as:</p>

<ul>
  <li>2018_0714_112604_057;c_hard_coral_branching 0.6:867x 604+891+540;c_soft_coral 0.7:102x140+2724+2825,0.3:156x156+2622+1576</li>
</ul>

<h1 id=""evaluation-criteria"">Evaluation criteria</h1>

<p>More information regarding the evaluation criteria will be released soon.</p>

<h1 id=""rules"">Rules</h1>

<p><em>Note: In order to participate in this challenge you have to sign an End User Agreement (EUA). You will find more information on the ‘Resources’ tab.</em></p>

<p>ImageCLEF lab is part of the Conference and Labs of the Evaluation Forum: CLEF 2020. CLEF 2020 consists of independent peer-reviewed workshops on a broad range of challenges in the fields of multilingual and multimodal information access evaluation, and a set of benchmarking activities carried in various labs designed to test different aspects of mono and cross-language Information retrieval systems. More details about the conference can be found <a href=""http://clef2020.clef-initiative.eu/"" target=""_blank"">here</a> .</p>

<p>Submitting a working note with the full description of the methods used in each run is mandatory. Any run that could not be reproduced thanks to its description in the working notes might be removed from the official publication of the results. Working notes are published within CEUR-WS proceedings, resulting in an assignment of an individual DOI (URN) and an indexing by many bibliography systems including DBLP. According to the CEUR-WS policies, a light review of the working notes will be conducted by ImageCLEF organizing committee to ensure quality. As an illustration, ImageCLEF 2019 working notes (task overviews and participant working notes) can be found within <a href=""http://ceur-ws.org/Vol-2380/"" target=""_blank"">CLEF 2019 CEUR-WS</a> proceedings.</p>

<h2 id=""important"">Important</h2>

<p>Participants of this challenge will automatically be registered at CLEF 2020. In order to be compliant with the CLEF registration requirements, please edit your profile by providing the following additional information:</p>

<ul>
  <li>
    <p>First name</p>
  </li>
  <li>
    <p>Last name</p>
  </li>
  <li>
    <p>Affiliation</p>
  </li>
  <li>
    <p>Address</p>
  </li>
  <li>
    <p>City</p>
  </li>
  <li>
    <p>Country</p>
  </li>
  <li>
    <p><em>Regarding the username, please choose a name that represents your team.</em></p>
  </li>
</ul>

<p><em>This information will not be publicly visible and will be exclusively used to contact you and to send the registration data to CLEF, which is the main organizer of all CLEF labs</em></p>

<h2 id=""participating-as-an-individual-non-affiliated-researcher"">Participating as an individual (non affiliated) researcher</h2>

<p>We welcome individual researchers, i.e. not affiliated to any institution, to participate. We kindly ask you to provide us with a motivation letter containing the following information:</p>

<ul>
  <li>
    <p>the presentation of your most relevant research activities related to the task/tasks</p>
  </li>
  <li>
    <p>your motivation for participating in the task/tasks and how you want to exploit the results</p>
  </li>
  <li>
    <p>a list of the most relevant 5 publications (if applicable)</p>
  </li>
  <li>
    <p>the link to your personal webpage</p>
  </li>
</ul>

<p>The motivation letter should be directly concatenated to the End User Agreement document or sent as a PDF file to bionescu at imag dot pub dot ro. The request will be analyzed by the ImageCLEF organizing committee. We reserve the right to refuse any applicants whose experience in the field is too narrow, and would therefore most likely prevent them from being able to finish the task/tasks.</p>

<h1 id=""citations"">Citations</h1>

<p>Information will be posted after the challenge ends.</p>

<h1 id=""prizes"">Prizes</h1>

<h2 id=""publication"">Publication</h2>

<p>ImageCLEF 2020 is an evaluation campaign that is being organized as part of the <a href=""http://clef2020.clef-initiative.eu/"" target=""_blank"">CLEF initiative</a> labs. The campaign offers several research tasks that welcome participation from teams around the world. The results of the campaign appear in the working notes proceedings, published by CEUR Workshop Proceedings (CEUR-WS.org). Selected contributions among the participants, will be invited for publication in the following year in the Springer Lecture Notes in Computer Science (LNCS) together with the annual lab overviews.</p>

<h1 id=""resources"">Resources</h1>

<h2 id=""contact-us"">Contact us</h2>

<p><em>Discussion Forum</em></p>

<ul>
  <li>You can ask questions related to this challenge on the Discussion Forum. Before asking a new question please make sure that question has not been asked before.</li>
  <li>Click on Discussion tab above or direct link: <a href=""https://discourse.aicrowd.com/c/imageclef-2020-coral-annotation-and-localisation"" target=""_blank"">https://discourse.aicrowd.com/c/imageclef-2020-coral-annotation-and-localisation</a></li>
</ul>

<p><em>Alternative channels</em></p>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

<ul>
  <li>jchamb[at]essex[dot]ac[dot]uk</li>
  <li>alien[at]essex[dot]ac[dot]uk</li>
  <li>a[dot]campello[at]wellcome[dot]ac[dot]uk</li>
  <li>alba[dot]garcia[at]essex[dot]ac[dot]uk</li>
</ul>

<h2 id=""more-information"">More information</h2>

<p>You can find additional information on the challenge here:
<a href=""https://www.imageclef.org/2020/coral"" target=""_blank"">https://www.imageclef.org/2020/coral</a></p>

"
218,"<p><em>Note: ImageCLEF Coral 2020 is divided into 2 subtasks (challenges). This challenge is about <strong>Pixel-wise Parsing</strong>. For information on the <strong>Annotation and Localisation</strong> challenge click <a href=""/challenges/imageclef-2020-coral-annotation-and-localisation"" target=""_blank"">here</a>. Both challenges share the same dataset, so registering for one of these challenges will automatically give you access to the other one.</em></p>

<p><em>Note: Do not forget to read the <strong>Rules</strong> section on this page. Pressing the red <strong>Participate button</strong> leads you to a page where you have to agree with those rules. You will not be able to submit any results before agreeing with the rules.</em></p>

<p><em>Note: Before trying to submit results, read the <strong>Submission instructions</strong> section on this page.</em></p>

<h1 id=""challenge-description"">Challenge description</h1>

<p>The increasing use of structure-from-motion photogrammetry for modelling large-scale environments from action cameras attached to drones has driven the next-generation of visualisation techniques that can be used in augmented and virtual reality headsets. It has also created a need to have such models labelled, with objects such as people, buildings, vehicles, terrain, etc. all essential for machine learning techniques to automatically identify as areas of interest and to label them appropriately. However, the complexity of the images makes impossible for human annotators to assess the contents of images on a large scale.
Advances in automatically annotating images for complexity and benthic composition have been promising, and we are interested in automatically identify areas of interest and to label them appropriately for monitoring coral reefs. Coral reefs are in danger of being lost within the next 30 years, and with them the ecosystems they support. This catastrophe will not only see the extinction of many marine species, but also create a humanitarian crisis on a global scale for the billions of humans who rely on reef services. By monitoring the changes and composition of coral reefs we can help prioritise conservation efforts.</p>

<p><strong>This task</strong> requires the participants to segment and parse each coral reef image into different image regions associated with benthic substrate types. For each image, segmentation algorithms will produce a semantic segmentation mask, predicting the semantic category for each pixel in the image.</p>

<h1 id=""data"">Data</h1>
<hr />
<p><em>As soon as the data is released it will be available under the “Resources” tab.</em></p>

<hr />

<p>More information will follow soon.</p>

<h1 id=""submission-instructions"">Submission instructions</h1>

<hr />
<p><em>As soon as the submission is open, you will find a “Create Submission” button on this page (next to the tabs).</em></p>

<hr />
<p><em>Before being allowed to submit your results, you have to first press the red participate button, which leads you to a page where you have to accept the challenges rules.</em></p>

<hr />

<p>Participants will be permitted to submit up to 10 runs. External training data is allowed and encouraged.
Each system run will consist of a single ASCII plain text file. The results of each test set should be given in separate lines in the text file. The format of the text file is as follows:</p>

<p>[image_ID/document_ID] [results]</p>

<p>The results of each test set image should be given in separate lines, each line providing only up to 500 localised substrates, with up to 500 coordinate localisations of the same substrate expected. <strong>The bounding polygons should not self-intersect</strong> . The format has characters to separate the elements, semicolon ‘;’ for the substrates, colon ‘:’ for the confidence, comma ‘,’ to separate multiple bounding polygons, and ‘x’ and ‘+’ for the size-offset bounding polygon format, i.e.:</p>

<p>[image_ID];[substrate1][[confidence1,1]:][x1,1]+[y1,1]+[x2,1]+[y2,1]+….+[xn,1]+[yn,1],[[confidence1,2][x1,2]+[y1,2]+[x2,2]+[y2,2]+….+[xn,2]+[yn,2];[substrate2] …</p>

<p>[confidence] are floating point values 0-1 for which a higher value means a higher score and the [xi,yi] represents consecutive points.
For example, in the development set format (notice that there are 2 polygons for substrate c_soft_coral):</p>

<ul>
  <li>
    <p>2018_0714_112604_057 0 c_hard_coral_branching 1 1757 833 1645 705 1559 598 1442 540 1249 593 1121 679 1020 705 998 844 891 967 966 1122 1137 1143 1324 1122 1468 1074 1655 978</p>
  </li>
  <li>
    <p>2018_0714_112604_057 3 c_soft_coral 1 2804 1368 2745 1368 2724 1427 2729 1507 2809 1507 2825 1453</p>
  </li>
  <li>
    <p>2018_0714_112604_057 4 c_soft_coral 1 2697 1576 2638 1592 2638 1608 2622 1667 2654 1694 2713 1731 2777 1731 2777 1635</p>
  </li>
</ul>

<p>In the submission format, it would be a line as:</p>

<ul>
  <li>2018_0714_112604_057;c_hard_coral_branching 0.6:1757+833+1645+705+1559+598+1442+540+1249+593+1121+679+1020+705+998+844+891+967+966+1122+1137+1143+1324+1122+1468+1074+1655+978;c_soft_coral 0.7:2804+1368+2745+1368+2724+1427+2729+1507+2809+1507+2825+1453,0.3:2697+1576+2638+1592+2638+1608+2622+1667+2654+1694+2713+1731+2777+1731+2777+1635</li>
</ul>

<h1 id=""evaluation-criteria"">Evaluation criteria</h1>

<p>More information regarding the evaluation criteria will be released soon.</p>

<h1 id=""rules"">Rules</h1>

<p><em>Note: In order to participate in this challenge you have to sign an End User Agreement (EUA). You will find more information on the ‘Resources’ tab.</em></p>

<p>ImageCLEF lab is part of the Conference and Labs of the Evaluation Forum: CLEF 2020. CLEF 2020 consists of independent peer-reviewed workshops on a broad range of challenges in the fields of multilingual and multimodal information access evaluation, and a set of benchmarking activities carried in various labs designed to test different aspects of mono and cross-language Information retrieval systems. More details about the conference can be found <a href=""http://clef2020.clef-initiative.eu/"" target=""_blank"">here</a> .</p>

<p>Submitting a working note with the full description of the methods used in each run is mandatory. Any run that could not be reproduced thanks to its description in the working notes might be removed from the official publication of the results. Working notes are published within CEUR-WS proceedings, resulting in an assignment of an individual DOI (URN) and an indexing by many bibliography systems including DBLP. According to the CEUR-WS policies, a light review of the working notes will be conducted by ImageCLEF organizing committee to ensure quality. As an illustration, ImageCLEF 2019 working notes (task overviews and participant working notes) can be found within <a href=""http://ceur-ws.org/Vol-2380/"" target=""_blank"">CLEF 2019 CEUR-WS</a> proceedings.</p>

<h2 id=""important"">Important</h2>

<p>Participants of this challenge will automatically be registered at CLEF 2020. In order to be compliant with the CLEF registration requirements, please edit your profile by providing the following additional information:</p>

<ul>
  <li>
    <p>First name</p>
  </li>
  <li>
    <p>Last name</p>
  </li>
  <li>
    <p>Affiliation</p>
  </li>
  <li>
    <p>Address</p>
  </li>
  <li>
    <p>City</p>
  </li>
  <li>
    <p>Country</p>
  </li>
  <li>
    <p><em>Regarding the username, please choose a name that represents your team.</em></p>
  </li>
</ul>

<p><em>This information will not be publicly visible and will be exclusively used to contact you and to send the registration data to CLEF, which is the main organizer of all CLEF labs</em></p>

<h2 id=""participating-as-an-individual-non-affiliated-researcher"">Participating as an individual (non affiliated) researcher</h2>

<p>We welcome individual researchers, i.e. not affiliated to any institution, to participate. We kindly ask you to provide us with a motivation letter containing the following information:</p>

<ul>
  <li>
    <p>the presentation of your most relevant research activities related to the task/tasks</p>
  </li>
  <li>
    <p>your motivation for participating in the task/tasks and how you want to exploit the results</p>
  </li>
  <li>
    <p>a list of the most relevant 5 publications (if applicable)</p>
  </li>
  <li>
    <p>the link to your personal webpage</p>
  </li>
</ul>

<p>The motivation letter should be directly concatenated to the End User Agreement document or sent as a PDF file to bionescu at imag dot pub dot ro. The request will be analyzed by the ImageCLEF organizing committee. We reserve the right to refuse any applicants whose experience in the field is too narrow, and would therefore most likely prevent them from being able to finish the task/tasks.</p>

<h1 id=""citations"">Citations</h1>

<p>Information will be posted after the challenge ends.</p>

<h1 id=""prizes"">Prizes</h1>

<h2 id=""publication"">Publication</h2>

<p>ImageCLEF 2020 is an evaluation campaign that is being organized as part of the <a href=""http://clef2020.clef-initiative.eu/"" target=""_blank"">CLEF initiative</a> labs. The campaign offers several research tasks that welcome participation from teams around the world. The results of the campaign appear in the working notes proceedings, published by CEUR Workshop Proceedings (CEUR-WS.org). Selected contributions among the participants, will be invited for publication in the following year in the Springer Lecture Notes in Computer Science (LNCS) together with the annual lab overviews.
# Resources</p>

<h2 id=""contact-us"">Contact us</h2>

<p><em>Discussion Forum</em></p>

<ul>
  <li>You can ask questions related to this challenge on the Discussion Forum. Before asking a new question please make sure that question has not been asked before.</li>
  <li>Click on Discussion tab above or direct link: <a href=""https://discourse.aicrowd.com/c/imageclef-2020-coral-pixel-wise-parsing"" target=""_blank"">https://discourse.aicrowd.com/c/imageclef-2020-coral-pixel-wise-parsing</a></li>
</ul>

<p><em>Alternative channels</em></p>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

<ul>
  <li>jchamb[at]essex[dot]ac[dot]uk</li>
  <li>alien[at]essex[dot]ac[dot]uk</li>
  <li>a[dot]campello[at]wellcome[dot]ac[dot]uk</li>
  <li>alba[dot]garcia[at]essex[dot]ac[dot]uk</li>
</ul>

<h2 id=""more-information"">More information</h2>

<p>You can find additional information on the challenge here:
<a href=""https://www.imageclef.org/2020/coral"" target=""_blank"">https://www.imageclef.org/2020/coral</a></p>

"
104,"<h1 id=""What’s the Challenge?"" class=""mt-0"">What’s the Challenge?</h1>

<h2 id=""Background"" class=""mt-1"">Background</h2>

<p>Over the past 3 years, humanitarian information analysts have been using an open source platform called <a href=""https://www.thedeep.io"" title=""DEEP""><strong>DEEP</strong></a> to facilitate collaborative, and joint analysis of unstructured data. The aim of the platform is to provide insights from years of historical and in-crisis humanitarian text data. The platform allows users to upload documents and classify text snippets according to predefined humanitarian target labels, grouped into and referred to as analytical frameworks. DEEP is now successfully functional in several international humanitarian organizations and the United Nations across the globe.</p>

<p>While DEEP comes with a generic analytical framework, each organization may also create its own custom framework based on the specific needs of its domain. In fact, while there is a large conceptual overlap for humanitarian organizations, various domains define slightly different analytical frameworks to describe their specific concepts. These differences between the analytical frameworks in different domains can still contain various degrees of conceptual (semantic) linkages, for instance on sectors such as Food Security and Livelihoods, Health, Nutrition, and Protection.</p>

<h2 id=""Challenge?"" class=""mt-1"">Challenge</h2>

<p>Currently, the ML/NLP elements of DEEP are trained separately for each organization, using the annotated data provided by the organization. Clearly, for the organizations which start working with DEEP, especially the ones with own custom frameworks, due to the lack of sufficiently tagged data, the text classifier shows poor performance. For these organizations, DEEP faces a cold-start challenge.</p>

<p>This challenge is a unique opportunity to address this issue with a wide impact. It enables not only better text classification, but also showcases those conceptual semantic linkages between the sectors of various organizations, ultimately resulting in improved analysis of the humanitarian situation across domains. You will be provided with the data of four organizations, consisting of text snippets and their corresponding target sectors, where, three of the organizations has the same analytical frameworks (target labels), and one has a slightly different one.</p>

<p>The aim is to <strong>learn novel text classification models, able to transfer knowledge across organizations, and specifically improve the classification effectiveness of the organizations with smaller amount of available training data</strong>. Ideally, transfer and joint learning methods provide a robust solution for the lack of data in the data-sparse scenarios.</p>

<h2 id=""Societal Impact"" class=""mt-1"">Societal Impact</h2>

<p>The DEEP project provides effective solutions to analyze and harvest data from secondary sources such as news articles, social media, and reports that are used by responders and analysts in humanitarian crises. During crises, rapidly identifying important information from the constantly-increasing data is crucial to understand the needs of affected populations and to improve evidence-based decision making. Despite the general effectiveness of DEEP, its ML-based features (in particular the text classifier) lack efficient accuracy, especially in domains with little or no training data.</p>

<p>The benefits of the challenge would be immediately seen in helping to increase the quality of the humanitarian community’s secondary data analysis. As such, humanitarian analysts would be able to spend time doing what the human mind does best: subjective analysis of information. The legwork of the easier to automate tasks such as initial sourcing of data and extraction of potentially relevant information can be left to their android counterparts.
With these improvements, the time required to gain key insights in humanitarian situations will be greatly decreased, and valuable aid and assistance can be distributed in a more efficient and targeted manner, while bringing together both in-crisis information, crucial contextual information on socio-economic issues, human rights, peace missions etc. that are currently disjointed.</p>

<h1 id=""What should I know to get involved?"" class=""mt-0"">What should I know to get involved?</h1>

<p>The challenge is the classification of multilingual text snippets of 4 organizations into 12 sectors (labels). The data is provided in 4 sets, each one belongs to a humanitarian organization. The amount of the available data highly differs across the organizations. The first 3 organizations have used  the same set of sectors; the 4th is tagged based on a different set of sectors, however, its sectors have many semantic overlaps with the ones of the first three organizations. The success of the final classifiers is measured base on the average of the prediction accuracies of organizations.</p>

<h2 id=""Resources"" class=""mt-1"">Resources</h2>

<p>The data consists of 4 sets, belonging to 4 organizations (<em>org1</em> to <em>org4</em>), and each comes with a development set (<em>orgX_dev</em>), and a test set (<em>orgX_test</em>).</p>

<p>The development sets contain the following fields:</p>

<ul>
  <li><strong>id</strong>: the unique identifier of text snippet; a string value, created by concatenating the name of the organization with a distinct number, for example <em>org1_13005</em>.</li>
  <li><strong>entry_original</strong>: the original text of the snippet, provided in languages, such as English and Spanish.</li>
  <li><strong>language</strong>: the language of the text snippet.</li>
  <li><strong>entry_translated</strong>: the translation of the text snippet to English, done using Google Translator.</li>
  <li><strong>labels</strong>: the label identifiers of the sectors. <strong>Each entry can have several labels.</strong> These labels are separated with semicolons (;).</li>
</ul>

<p>The test sets contain the following fields:</p>

<ul>
  <li><strong>id</strong>: the unique identifier of text snippet.</li>
  <li><strong>entry_original</strong>: the original text of the snippet.</li>
  <li><strong>language</strong>: the language of the text snippet.</li>
  <li><strong>entry_translated</strong>: the translation of the text snippet to English, done using Google Translator.</li>
</ul>

<p><strong>Important:</strong> As mentioned before, the first three organizations have the same labels, but the fourth has a set of different ones. The sectors regarding each label identifier are provided in the <em>label_captions</em> file. Later in this section, you can find a detailed explanation of the meaning of these sectors, and their potential semantic relations.</p>

<h2 id=""Submissions"" class=""mt-1"">Submissions</h2>

<p>As mentioned above, each entry in train data can have one or more labels (sectors). However, for submission you should provide the prediction of only one label, namely the most probable one.</p>

<p>Given the test sets of the 4 organizations, the submissions should be provided in <strong>comma-separated (,) CSV format</strong>, containing the following two fields:</p>

<ul>
  <li><strong>id</strong>: the unique identifier of text snippets in the test sets</li>
  <li><strong>predicted_label</strong>: the unique identifier of ONE predicted label</li>
</ul>

<p>The submission file contains the predictions of all 4 organizations together. Here an example of a submission file:</p>

<p>```id,predicted_label</p>

<p>org1_8186,1</p>

<p>org1_11018,10</p>

<p>…</p>

<p>org2_3828,5</p>

<p>org2_5340,9</p>

<p>…</p>

<p>org3_2206,8</p>

<p>org3_1875,4</p>

<p>…</p>

<p>org4_75,107</p>

<p>org4_158,104</p>

<p>…```</p>

<h2 id=""Evaluation"" class=""mt-1"">Evaluation</h2>

<p>The evaluation is done based on the mean of accuracy values over the organizations: we first calculate the accuracy of the predictions of the test data of each organization, and then report the average of these 4 accuracy values. This measure is referred to as <em>Mean of Accuracies</em>.</p>

<p>Since the reference data, similar to train data, can assign one or more labels to each entry, we consider a prediction as <strong>correct</strong>, when at least one of the reference labels are predicted.</p>

<p>This evaluation measure gives the same weight to each organization, although each organization has a different number of test data. It incentivizes good performance on the organizations with smaller available training (and also test) data, as they have the same importance as the other ones.</p>

<p>To facilitate the development and test of the systems, we provide the evaluation script (<em>deep_evaluator.py</em>), available in Resources.</p>

<h2 id=""A Guidance through the Sectors"" class=""mt-1"">A Guidance through the Sectors</h2>

<p>Humanitarian response is organised in thematic clusters. Clusters are groups of humanitarian organizations, both UN and non-UN, in each of the main sectors of humanitarian action, e.g. water, health and logistics. Those serve as global organizing principle to coordinate humanitarian response.</p>

<p>Sectors for the first, second, and third organization:</p>

<ul>
  <li>(1) Agriculture</li>
  <li>(2) Cross: short form of Cross-sectoral; areas of humanitarian response that require action in more than one sector. For example malnutrition requires humanitarian interventions in health, access to food, access to basic hygiene items and clean water, and access to non-food items such as bottles to feed infants.</li>
  <li>(3) Education</li>
  <li>(4) Food</li>
  <li>(5) Health</li>
  <li>(6) Livelihood: Access to employment and income</li>
  <li>(7) Logistics: Any logistical support needed to carry out humanitarian activities e.g. air transport, satellite phone connection etc.</li>
  <li>(8) NFI: Non-food items needed in daily life that are not food such as bedding, mattrassess, jerrycans, coal or oil for heating</li>
  <li>(9) Nutrition</li>
  <li>(10) Protection</li>
  <li>(11) Shelter</li>
  <li>(12) WASH (Water, Sanitation and Hygiene)</li>
</ul>

<p>Sectors for the fourth organization:</p>

<ul>
  <li>(101) Child Protection</li>
  <li>(102) Early Recovery and Livelihoods</li>
  <li>(103) Education</li>
  <li>(104) Food</li>
  <li>(105) GBV: Gender Based Violence</li>
  <li>(106) Health</li>
  <li>(107) Logistics</li>
  <li>(108) Mine Action</li>
  <li>(109) Nutrition</li>
  <li>(110) Protection</li>
  <li>(111) Shelter and NFIs</li>
  <li>(112) WASH</li>
</ul>
"
3,"<blockquote class=""update"">
  <p><strong>Starter Kit</strong> : <a href=""https://github.com/AIcrowd/snake-species-identification-challenge-starter-kit"">https://github.com/AIcrowd/snake-species-identification-challenge-starter-kit</a></p>
</blockquote>

<p>Snakebite is the most deadly neglected tropical disease (NTD), being responsible for <a href=""http://www.who.int/snakebites/en/"" target=""_blank""> a dramatic humanitarian crisis in global health </a></p>

<p>Snakebite causes over 100,000 human deaths and 400,000 victims of disability and disfigurement globally every year. It affects poor and rural communities in developing countries, which host the highest venomous snake diversity and the highest burden of snakebite <a href=""https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(18)31224-8/fulltext"" target=""_blank""> due to limited medical expertise and access to antivenoms </a></p>

<p>Antivenoms can be life‐saving when correctly administered but this depends first on the correct taxonomic identification (i.e. family, genus, species) of the biting snake. Snake identification is challenging due to:</p>

<ul>
  <li>their high diversity</li>
  <li>the incomplete or misleading information provided by snakebite victims</li>
  <li>the lack of knowledge or resources in herpetology that healthcare professionals have</li>
</ul>

<p>In this challenge we want to explore how Machine Learning can help with snake identification, in order to potentially reduce erroneous and delayed healthcare actions.</p>

<p><img src=""https://pbs.twimg.com/media/DLtBJ-xW0AA94lh.jpg"" alt=""image4_challenge_dataset.png"" />
Species richness of reptiles worldwide</p>

<h1 id=""task"">Task</h1>

<p>In this challenge you will be provided with a dataset of RGB images of snakes, and their corresponding species (class) and geographic location (continent, country). The goal is to train a classification model.</p>

<p>The difficulty of the challenge relies on the dataset characteristics, as there might be a high intraclass variance for certain classes and a low interclass variance among others, as shown in the examples from the Datasets section. Also, the distribution of images between class is not equal for all classes: the class with the most images has 17,749, while the class with the fewest images has 552.</p>

<p>For now, we would like to make the barrier to entry much lower and demonstrate that an approach works well on 85 species and 187,720 images. The idea would be then to renew the challenge every 4 months in order to get closer to our final goal, which is to build an algorithm which best predicts which antivenin should be given (if any) when given a specific image.</p>

<p><img src=""https://i.imgur.com/7wR9zXc.png"" alt=""image5_challenge_dataset.png"" />
Number of images per species in the dataset</p>

<h1 id=""datasets"">Datasets</h1>

<p>Snakes are extremely diverse, and snake biologists continue to document &amp; describe snake diversity, with an average of 30 new species described per year since the year 2000. Although most people probably think of snakes as a single “kind” of animal, humans are as evolutionarily close to whales as pythons are to rattlesnakes, so snakes in fact are very diverse! Taxonomically speaking, 
 <a href=""http://reptile-database.reptarium.cz/advanced_search?taxon=serpentes&amp;submit=Search"" target=""_blank"">snakes are classified into 24 families, containing 528 genera and 3,709 species.   </a></p>

<p><img src=""https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/836c44312cef907354e41d7e5c84bd3c_image1_challenge_dataset.png"" alt=""image1_challenge_dataset.png"" /></p>

<p>You can download the datasets in the Datasets Section. You are provided with a Train.tar.gz, file composed of 187,720 RGB images of varying size, split into 85 species.</p>

<p>Several aspects of snake morphology make this challenge more challenging:</p>

<ul>
  <li>
    <p>Some species have patterns that vary depending on their age</p>
  </li>
  <li>
    <p>Some species have patterns that vary depending on their location</p>
  </li>
  <li>
    <p>Two species might look very similar, with one being venomous and the other not</p>
  </li>
</ul>

<p>The first iteration of the data set contains few such species, but we will add in more later.</p>

<p><img src=""https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/f4e927cb3680ceb410ff825a8c0a53c4_picture2_challenge_datasets.png"" alt=""picture2_challenge_datasets.png"" /></p>

<p><img src=""https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/94e519cc1e7fabdd92f535897bbd7356_picture3_challenge_datasets.png"" alt=""picture3_challenge_datasets.png"" /></p>

<h1 id=""prizes"">Prizes</h1>

<ul>
  <li>1 travel grant to <a href=""http://ghf2020.g2hp.net/"" target=""_blank"">Geneva Health Forum 2020</a></li>
  <li>1 co-authored paper releasing the dataset and describing the top solution(s) as baselines</li>
</ul>

<h1 id=""timeline"">Timeline</h1>
<p>This is the very first <strong>benchmarking</strong> challenge, meaning that it has no end date, but it will be updated every 3 months. Here are the first deadlines:</p>

<p><strong>January 21, 2019</strong><br />
Competition Open</p>

<p><strong>May 31, 2019</strong><br />
Qualifying Round 1 submission deadline</p>

<p><strong>July 31, 2019</strong><br />
Qualifying Round 2 submission deadline</p>

<p><strong>January 17, 2020</strong><br />
Qualifying Round 3 submission deadline</p>

<p><img src=""https://i.imgur.com/w2LzVat.jpg"" alt=""image_challenge_dataset.png"" />
A puff adder, one of the most dangerous snakes in Africa</p>
"
220,"<p><em>Note: ImageCLEF 2020 VQA-Med includes 2 tasks. This page is about the Visual Question Answering (<strong>VQA</strong>) task. For information about the Visual Question Generation (<strong>VQG</strong>) task click <a href=""/challenges/imageclef-2020-vqa-med-vqg"" target=""_blank"">here</a>. Both challenges share the same dataset, so registering for one of these challenges will automatically give you access to the other one.</em></p>

<hr />

<p><em>Note: Please do not forget to read the <strong>Rules</strong> section on this page. Pressing the red <strong>Participate button</strong> leads you to a page where you have to agree with those rules. You will not be able to submit any results before agreeing with the rules.</em></p>

<p><em>Note: Before trying to submit results, please read the <strong>Submission instructions</strong> section on this page.</em></p>

<h1 id=""vqa-task-description"">VQA task description</h1>

<p>In continuation of the two previous editions, this year’s task on visual question answering (VQA) consists in answering natural language questions from the visual content of associated radiology images. This year, we will focus particularly on questions about abnormalities.</p>

<h1 id=""data"">Data</h1>

<hr />
<p><em>As soon as the data is released it will be available under the “Resources” tab.</em></p>

<hr />

<h1 id=""schedule"">Schedule</h1>

<ul>
  <li>25/02/2020: Release of the training and validation datasets</li>
  <li>10/04/2020: Release of the test set</li>
  <li>10/05/2020: Run submission deadline</li>
</ul>

<h1 id=""submission-instructions"">Submission instructions</h1>

<hr />
<p><em>As soon as the submission is open, you will find a “Create Submission” button on this page (next to the tabs).</em></p>

<hr />
<p><em>Before being allowed to submit your results, you have to first press the red participate button, which leads you to a page where you have to accept the challenges rules.</em></p>

<hr />

<p>More information regarding the submission instructions will be released soon.</p>

<h1 id=""evaluation-criteria"">Evaluation criteria</h1>

<p>More information regarding the evaluation criteria will be released soon.</p>

<h1 id=""rules"">Rules</h1>

<p><em>Note: In order to participate in this challenge you have to sign an End User Agreement (EUA). You will find more information on the ‘Resources’ tab.</em></p>

<p>ImageCLEF lab is part of the Conference and Labs of the Evaluation Forum: CLEF 2020. CLEF 2020 consists of independent peer-reviewed workshops on a broad range of challenges in the fields of multilingual and multimodal information access evaluation, and a set of benchmarking activities carried in various labs designed to test different aspects of mono and cross-language Information retrieval systems. More details about the conference can be found <a href=""http://clef2020.clef-initiative.eu/"" target=""_blank"">here</a> .</p>

<p>Submitting a working note with the full description of the methods used in each run is mandatory. Any run that could not be reproduced thanks to its description in the working notes might be removed from the official publication of the results. Working notes are published within CEUR-WS proceedings, resulting in an assignment of an individual DOI (URN) and an indexing by many bibliography systems including DBLP. According to the CEUR-WS policies, a light review of the working notes will be conducted by ImageCLEF organizing committee to ensure quality. As an illustration, ImageCLEF 2019 working notes (task overviews and participant working notes) can be found within <a href=""http://ceur-ws.org/Vol-2380/"" target=""_blank"">CLEF 2019 CEUR-WS</a> proceedings.</p>

<h2 id=""important"">Important</h2>

<p>Participants of this challenge will automatically be registered at CLEF 2020. In order to be compliant with the CLEF registration requirements, please edit your profile by providing the following additional information:</p>

<ul>
  <li>
    <p>First name</p>
  </li>
  <li>
    <p>Last name</p>
  </li>
  <li>
    <p>Affiliation</p>
  </li>
  <li>
    <p>Address</p>
  </li>
  <li>
    <p>City</p>
  </li>
  <li>
    <p>Country</p>
  </li>
  <li>
    <p><em>Regarding the username, please choose a name that represents your team.</em></p>
  </li>
</ul>

<p><em>This information will not be publicly visible and will be exclusively used to contact you and to send the registration data to CLEF, which is the main organizer of all CLEF labs</em></p>

<h2 id=""participating-as-an-individual-non-affiliated-researcher"">Participating as an individual (non affiliated) researcher</h2>

<p>We welcome individual researchers, i.e. not affiliated to any institution, to participate. We kindly ask you to provide us with a motivation letter containing the following information:</p>

<ul>
  <li>
    <p>the presentation of your most relevant research activities related to the task/tasks</p>
  </li>
  <li>
    <p>your motivation for participating in the task/tasks and how you want to exploit the results</p>
  </li>
  <li>
    <p>a list of the most relevant 5 publications (if applicable)</p>
  </li>
  <li>
    <p>the link to your personal webpage</p>
  </li>
</ul>

<p>The motivation letter should be directly concatenated to the End User Agreement document or sent as a PDF file to bionescu at imag dot pub dot ro. The request will be analyzed by the ImageCLEF organizing committee. We reserve the right to refuse any applicants whose experience in the field is too narrow, and would therefore most likely prevent them from being able to finish the task/tasks.</p>

<h1 id=""citations"">Citations</h1>

<p>Information will be posted after the challenge ends.</p>

<h2 id=""publication"">Publication</h2>

<p>ImageCLEF 2020 is an evaluation campaign that is being organized as part of the <a href=""http://clef2020.clef-initiative.eu/"" target=""_blank"">CLEF initiative</a> labs. The campaign offers several research tasks that welcome participation from teams around the world. The results of the campaign appear in the working notes proceedings, published by CEUR Workshop Proceedings (CEUR-WS.org). Selected contributions among the participants, will be invited for publication in the following year in the Springer Lecture Notes in Computer Science (LNCS) together with the annual lab overviews.</p>

<h1 id=""resources"">Resources</h1>

<h2 id=""contact-us"">Contact us</h2>

<p><em>Discussion Forum</em></p>

<ul>
  <li>You can ask questions related to this challenge on the Discussion Forum. Before asking a new question please make sure that question has not been asked before.</li>
  <li>Click on Discussion tab above or direct link: <a href=""https://discourse.aicrowd.com/c/imageclef-2020-vqa-med-vqg"" target=""_blank"">https://discourse.aicrowd.com/c/imageclef-2020-vqa-med-vqg</a></li>
</ul>

<p><em>Alternative channels</em></p>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

<ul>
  <li>asma[dot]benabacha[at]nih[dot]gov</li>
  <li>vivek[dot]datla[at]philips[dot]com</li>
  <li>sadidhasan[at]gmail[dot]com</li>
  <li>joey[dot]liu[at]philips[dot]com</li>
  <li>ddemner[at]mail[dot]nih[dot]gov</li>
  <li>henning[dot]mueller[at]hevs[dot]ch</li>
</ul>

<h2 id=""more-information"">More information</h2>

<p>You can find additional information on the challenge here:
<a href=""https://www.imageclef.org/2020/medical/vqa"" target=""_blank"">https://www.imageclef.org/2020/medical/vqa</a></p>

<p>Mailing list: https://groups.google.com/d/forum/imageclef-vqa-med</p>
"
10,"<p>Anonymisation of data is a long term scientific question that is not yet solved. With the rising of open data, we’re in need of good anonymisation algorithms. This is why we held the first international Data Anonymisation and Re-identification Competition (DARC).</p>

<p>This competition take place in two rounds :
    - The first round is dedicated to the anonymization of the ground truth data.
    - The second to the re-identification of other player’s anonymized data.</p>

<h2 id=""dataset"">Dataset</h2>
<p>You can download the datasets in the <a href=""https://www.aicrowd.com/challenges/data-anonymization-and-re-identification-competition-darc/dataset_files"">Datasets Section</a>. You will find <code class=""highlighter-rouge"">ground_truth.csv</code>, which is the transactional histroy of 4000 users from the UCI dataset <code class=""highlighter-rouge"">Online Retail</code>.</p>

<h2 id=""evaluation-criteria"">Evaluation Criteria</h2>

<p>You will be evaluated on two aspects : <strong>utility</strong> and <strong>re-identification rate</strong>.</p>

<p><strong>Utility</strong> is computed by taking the maximum of 6 metrics (named $E_i$). Each metrics return a score between 0 and 1, 0 being the best score.</p>

<p><strong>Re-identification</strong>  is computed by taking the maximum of 6 metrics (named $S_i$). Each metrics return a score between 0 and 1, 0 being the best score.</p>

<p>For more information about the metrics , please read the <a href=""https://zenodo.org/record/2640922/files/rulesDARC2018_ver1.3.pdf?download=1"">rules</a></p>

<p>The score of a file is computed as follow : $\frac{Utility + Reidentification}{2}$</p>

<h2 id=""rules"">Rules</h2>

<p>The competition take place in two rounds :</p>

<ul>
  <li>
    <p><strong>Round1</strong> : it’s the anonymisation period. Players submits a csv file which is the anonymized version of the ground truth.</p>
  </li>
  <li>
    <p><strong>Round</strong> : the attack period. Players can attack other player anonymization file during this period. They submit a tarball containing the $\hat{F}$ and a Json file containing information about the team attacked and the file attacked.</p>
  </li>
</ul>

"
175,"<p><img src=""https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/97818f3831eff469165ba01ae8f79860_Capture%20d%E2%80%99e%CC%81cran%202017-01-10%20a%CC%80%2010.22.24.png"" alt="" location-based-species-recommendation "" /></p>

<p><em>Note: Do not forget to read the <strong>Rules</strong> section on this page</em></p>

<h3 id=""usage-scenario"">Usage scenario</h3>
<p>Automatically predicting the list of species that are the most likely to be observed at a given location is useful for many scenarios in biodiversity informatics. First of all, it could improve species identification processes and tools by reducing the list of candidate species that are observable at a given location (be they automated, semi-automated or based on classical field guides or flora). More generally, it could facilitate biodiversity inventories through the development of location-based recommendation services (typically on mobile phones) as well as the involvement of non-expert nature observers. Last but not least, it might serve educational purposes thanks to biodiversity discovery applications providing functionalities such as contextualized educational pathways.</p>

<h3 id=""challenge-description"">Challenge description</h3>

<p>The aim of the challenge is to predict the list of species that are the most likely to be observed at a given location. Therefore, we will provide a large training set of species occurrences, each occurrence being associated to a multi-channel image characterizing the local environment. Indeed, it is usually not possible to learn a species distribution model directly from spatial positions because of the limited number of occurrences and the sampling bias. What is usually done in ecology is to predict the distribution on the basis of a representation in the environmental space, typically a feature vector composed of climatic variables (average temperature at that location, precipitation, etc.) and other variables such as soil type, land cover, distance to water, etc. The originality of GeoLifeCLEF is to generalize such niche modeling approach to the use of an image-based environmental representation space. Instead of learning a model from environmental feature vectors, the goal of the task will be to learn a model from k-dimensional image patches, each patch representing the value of an environmental variable in the neighborhood of the occurrence (see figure below for an illustration). From a machine learning point of view, the challenge will thus be treatable as an image classification task.</p>

<p><img src=""https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/05115f1f63ee37eb96761cd0b40744a2_figure-1.png"" alt=""  "" /></p>

<h3 id=""data"">Data</h3>

<p>A detailed description of the data is provided in the Dataset section. In a nutshell, the dataset was built from occurrence data of the Global Biodiversity Information Facility (GBIF), the world’s largest open data infrastructure in this domain, funded by governments. It is composed of 261,176 occurrences of 3,203 plant species observed on the French territory between 1835 and 2017. Each occurrence is characterized by 33 local environmental images of 64x64 pixels (encoded as tif images with 33 channels). These environmental images were constructed from various open datasets including Chelsea Climate [1], ESDB soil pedology data [2,3,4], Corine Land Cover 2012 soil occupation data, CGIAR-CSI evapotranspiration data [5,6], USGS Elevation data (Data available from the U.S. Geological Survey.) and BD Carthage hydrologic data. This dataset is split in 3/4 for training and 1/4 for testing.</p>

<p><strong>External data</strong></p>

<p>Participants are allowed to use other external training data but at the condition that (i) the experiment is entirely re-produceable, i.e. that the used external ressource is clearly referenced and accessible to any other research group in the world, (ii) participants submit at least one run without external training data so that we can study the contribution of such ressources, (iii) the additional ressource does not contain any of the test observations.</p>

<h3 id=""submission-instructions"">Submission instructions</h3>

<p>Each team is allowed to submit 10 runs maximum. 
A <strong>run</strong> is a .csv file with 4 columns separated by “;” and containing :</p>

<p>patch_id ; species_glc_id ; probability ; rank</p>

<p>Here is an example :</p>

<p>69;42;0.1415;1<br />
69;25;0.0001;2<br />
70;42;0.9265;1<br />
70;3;0.01;2<br />
70;23;0.001;3<br /></p>

<p>Please watch your <strong>runs</strong> format. patch_id, species_glc_id and rank should be integers, and probability a float. For a patch_id, one can give up to 100 species, which must be distinct and their ranks strictly consecutive.</p>

<p><strong>WARNING</strong>: <strong>Even though a run induces an error, it is counted among the 10 allowed</strong>.</p>

<p>Remark : There is no leaderboard for this task. We removed it to maximise the independence between submitted algorithms and test data, and the usefulness of GeoLifeClef results for research.</p>

<p><em>As soon as the submission is open, you will find a “Create Submission” button on this page (just next to the tabs)</em></p>

<h3 id=""results"">Results</h3>
<p>The following table and graph sum up the MRR results on the test set per participant :</p>

<p><img src=""https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/f7eda598fbb2c2cb65de982322bd98ca_results_table.png"" alt=""  "" /></p>

<p><img src=""https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/009263e5a4ea173ebd0c6adacf1949e9_result_graph_2.png"" alt=""  "" /></p>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>
<p>The used metric will be the Mean Reciprocal Rank (MRR). The MRR is a statistic measure for evaluating any process that produces a list of possible responses to a sample of queries ordered by probability of correctness. The reciprocal rank of a query response is the multiplicative inverse of the rank of the first correct answer. The MRR is the average of the reciprocal ranks for the whole test set:
<img src=""https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/4b2b4e82a9d495d540ac7b4d7c3db95f_mrr-crowdai2.png"" alt=""mrr-crowdai2.png"" /></p>

<table>
  <tbody>
    <tr>
      <td>where</td>
      <td>Q</td>
      <td>is the total number of query occurrences in the test set.</td>
    </tr>
  </tbody>
</table>

<h3 id=""resources"">Resources</h3>

<h3 id=""contact-us"">Contact us</h3>

<ul>
  <li>Technical issues : <a href=""https://gitter.im/crowdAI/lifeclef-2018-geo"" target=""_blank""> https://gitter.im/crowdAI/lifeclef-2018-geo </a></li>
  <li>Discussion Forum : <a href=""https://www.crowdai.org/challenges/lifeclef-2018-geo/topics"" target=""_blank""> https://www.crowdai.org/challenges/lifeclef-2018-geo/topics </a></li>
</ul>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

<ul>
  <li>Sharada Prasanna Mohanty: sharada.mohanty@epfl.ch</li>
  <li>Christophe Botella: christophe[DOT]botella[AT]gmail[DOT]com</li>
  <li>Alexis Joly: alexis[DOT]joly[AT]inria[DOT]fr</li>
  <li>Ivan Eggel: ivan[DOT]eggel[AT]hevs[DOT]ch</li>
</ul>

<h3 id=""more-information"">More information</h3>

<p>You can find additional information on the challenge here:
<a href=""http://imageclef.org/node/229"" target=""_blank""> http://imageclef.org/node/229 </a></p>

<h3 id=""prizes"">Prizes</h3>

<p>LifeCLEF 2018 is an evaluation campaign that is being organized as part of the <a href=""http://clef2018.clef-initiative.eu/"" target=""_blank""> CLEF initiative </a> labs. The campaign offers several research tasks that welcome participation from teams around the world. The results of the campaign appear in the working notes proceedings, published by CEUR Workshop Proceedings (CEUR-WS.org). Selected contributions among the participants, will be invited for publication in the following year in the Springer Lecture Notes in Computer Science (LNCS) together with the annual lab overviews.</p>

<h3 id=""datasets-license"">Datasets License</h3>

"
15,"<p><strong>NEWS:</strong> Deadlines <a href=""http://www.cs.ox.ac.uk/isg/challenges/sem-tab/"">updated</a>. Please join our <a href=""https://groups.google.com/d/forum/sem-tab-challenge"">discussion group</a>.</p>

<p>This is a task of <a href=""https://iswc2019.semanticweb.org/challenges/"">ISWC 2019</a> “Semantic Web Challenge on Tabular Data to Knowledge Graph Matching”. It’s to annotate an entity column (i.e., a column composed of phrases) in a table with classes of DBpedia Ontology. <a href=""http://www.cs.ox.ac.uk/isg/challenges/sem-tab/"">Click here</a> for the official challenge website.</p>

<h1 id=""task-description"">Task Description</h1>

<p>The task is to annotate each of the given entity columns with classes of DBpedia ontology. The annotation class should come from <a href=""http://mappings.dbpedia.org/server/ontology/classes/"">DBpedia ontology classes</a> (excluding owl:Thing and owl:Agent). Each column can be annotated by multiple classes: the one that is as fine grained as possible and correct to all its cells, is regarded as a <strong>perfect annotation</strong>; the one that is the ancestor of the perfect annotation is regarded as an <strong>okay annotation</strong>; others are regarded as <strong>wrong annotations</strong>. Case is NOT sensitive.</p>

<p>Each submission should be a CSV file. Each line should include a column identified by table id and column id and its class annotations. It means one line should include three fields: “Table ID”, “Column ID” and “DBpedia classes”. The headers should be excluded from the submission file. Annotation classes should be separated by space, and their order does not matter. Here is one line example:
“9206866_1_8114610355671172497”,”0”,”http://dbpedia.org/ontology/Country http://dbpedia.org/ontology/PopulatedPlace http://dbpedia.org/ontology/Place”</p>

<p>Notes:</p>

<p>1) Table ID does not include the file name extension; make sure you remove the .csv extension from the filename.</p>

<p>2) Column ID is the position of the column in the input, starting from 0, i.e., first column’s ID is 0.</p>

<p>3) In Round 1, only perfect annotations score; in Round 2, both perfect annotations and okay annotations score.</p>

<p>4) One submission file should have NO duplicate lines (annotations) for one target column.</p>

<p>5) Annotations for columns out of the target columns are ignored.</p>

<h1 id=""datasets"">Datasets</h1>

<p>Table set for Round #1: <a href=""https://www.cs.ox.ac.uk/isg/challenges/sem-tab/data/CTA_Round1.tar.gz"">Tables</a>, <a href=""https://www.cs.ox.ac.uk/isg/challenges/sem-tab/data/CTA_Round1_Targets.csv"">Target Columns</a></p>

<p>Table set for Round #2: <a href=""https://www.cs.ox.ac.uk/isg/challenges/sem-tab/data/Tables_Round2.tar.gz"">Tables</a>, <a href=""https://www.cs.ox.ac.uk/isg/challenges/sem-tab/data/CTA_Round2_Targets.csv"">Target Columns</a></p>

<p>Table set for Round #3: <a href=""https://www.cs.ox.ac.uk/isg/challenges/sem-tab/data/Tables_Round3.tar.gz"">Tables</a>, <a href=""https://www.cs.ox.ac.uk/isg/challenges/sem-tab/data/CTA_Round3_Targets.csv"">Target Columns</a></p>

<p>Data Description: One table is stored in one CSV file. Each line corresponds to a table row. Note that the first row may either be the table header or content. The target columns for annotation are saved in a CSV file.</p>

<h1 id=""evaluation-criteria-round-1"">Evaluation Criteria [Round 1]</h1>

<p>Precision, Recall and F1 Score will be calculated for ranking:</p>

<p>Precision = (# perfect annotations) / (# submitted annotations)</p>

<p>Recall = (# perfect  annotations) / (# ground truth annotations)</p>

<p>F1 Score = (2 * Precision * Recall) / (Precision + Recall)</p>

<p>Note:</p>

<p>1)  # denotes the number.</p>

<p>2) In the ground truth file, one specified column is exactly annotated by one perfect class: # ground truth annotations = # target columns.</p>

<p>2) F1 Score is used as the primary score, Precision is used as the secondary score.</p>

<h1 id=""evaluation-criteria-round-2"">Evaluation Criteria [Round 2]</h1>

<p>The following metrics named Average Hierarchical Score (AH-Score) and Average Perfect Score (AP-Score) are calculated for ranking:</p>

<p>AH-Score = (1 * (# perfect annotations) + 0.5 * (# okay annotations) - 1 * (# wrong annotations)) / (# target columns)</p>

<p>AP-Score = (# perfect annotations) / (# total annotated classes)</p>

<p>Notes:</p>

<p>1) # denotes the number</p>

<p>2) AH-Score is used as the primary score; AP-Score is used as the secondary score.</p>

<h1 id=""evaluation-criteria-round-3"">Evaluation Criteria [Round 3]</h1>

<p>The same as Round 2.</p>

<h1 id=""evaluation-criteria-round-4"">Evaluation Criteria [Round 4]</h1>

<p>The same as Round 2.</p>

<h1 id=""prizes"">Prizes</h1>

<p>SIRIUS and IBM Research sponsor the prizes for the best systems.</p>

<h1 id=""rules"">Rules</h1>

<ol>
  <li>
    <p>Selected systems with the best results in Round 1 and 2 will be invited to present their results during the <a href=""https://iswc2019.semanticweb.org/"">ISWC conference</a> and the <a href=""http://om2019.ontologymatching.org/"">Ontology Matching workshop</a>.</p>
  </li>
  <li>
    <p>The prize winners will be announced during the <a href=""https://iswc2019.semanticweb.org/"">ISWC conference</a> (on October 30, 2019). We will take into account all evaluation rounds specially the ones running till the conference dates.</p>
  </li>
  <li>
    <p>Participants are encouraged to submit a system paper describing their tool and the obtained results. Papers will be published online as a volume of <a href=""http://ceur-ws.org/"">CEUR-WS</a> as well as indexed on <a href=""https://dblp.uni-trier.de/"">DBLP</a>. By submitting a paper, the authors accept the CEUR-WS and DBLP publishing rules.</p>
  </li>
  <li>
    <p>Please see additional information at our <a href=""http://www.cs.ox.ac.uk/isg/challenges/sem-tab/"">official website</a></p>
  </li>
</ol>

"
156,"<h3 id=""motivation"">Motivation</h3>

<p>Interpreting and summarizing the insights gained from medical images such as radiology output is a time-consuming task that involves highly trained experts and often represents a bottleneck in clinical diagnosis pipelines.</p>

<p>Consequently, there is a considerable need for automatic methods that can approximate this mapping from visual information to condensed textual descriptions. The more image characteristics are known, the more structured are the radiology scans and hence, the more efficient are the radiologists regarding interpretation. We work on the basis of a large-scale collection of figures from open access biomedical journal articles (PubMed Central). All images in the training data are accompanied by UMLS concepts extracted from the original image caption.</p>

<p>Lessons learned:</p>

<ul>
  <li>
    <p>In the first and second editions of this task, held at ImageCLEF 2017 and ImageCLEF 2018, participants noted a broad variety of content and situation among training images. For this year, the training data is reduced solely to radiology images</p>
  </li>
  <li>
    <p>A large number of concepts was used in the previous years. This year, the captions are first processed before concept extraction, hence leading to a reduced number of concepts</p>
  </li>
  <li>
    <p>As uncertainty regarding additional source was noted, we will clearly separate systems using exclusively the official training data from those that incorporate additional sources of evidence</p>
  </li>
</ul>

<h3 id=""challenge-description"">Challenge description</h3>

<p>The first step to automatic image captioning and scene understanding is identifying the presence and location of relevant concepts in a large corpus of medical images. Based on the visual image content, this subtask provides the building blocks for the scene understanding step by identifying the individual components from which captions are composed. The concepts can be further applied for context-based image and information retrieval purposes.</p>

<p>Evaluation is conducted in terms of set coverage metrics such as precision, recall, and combinations thereof. This task will be run using a subset of the Radiology Objects in COntext (<a href=""https://www.springerprofessional.de/radiology-objects-in-context-roco-a-multimodal-image-dataset/16204278"" target=""_blank""> ROCO </a>) dataset [1].</p>

<h3 id=""data"">Data</h3>

<p>From the PubMed Open Access subset containing 1,828,575 archives, a total number of 6,031,814
image - caption pairs were extracted. To focus on radiology images and non-compound figures, automatic filtering with deep learning systems as well as manual revisions were applied, reducing the dataset to 72,187 radiology images of several medical imaging modalities.</p>

<p>NOTE: If the usage of an additional source for training is intended, it should not be a subset of PubMed Central Open Access (archiving date: 01.02.2018 - 01.02.2019), to avoid an overlap with the test data.</p>

<h3 id=""submission-instructions"">Submission instructions</h3>

<p>As soon as the submission is open, you will find a “Create Submission” button on this page (just next to the tabs)</p>

<p>For the submission we expect the following format:</p>

<ul>
  <li>Figure-ID TAB Concept-ID-1;Concept-ID-2;Concept-ID-n
e.g.:</li>
  <li>
    <p>ROCO_41341 C0033785;C0035561</p>
  </li>
  <li>ROCO_07563 C0043299;C1306645;C1548003;C1962945</li>
</ul>

<p>You need to respect the following constraints:</p>

<ul>
  <li>The separator between the figure ID and the concepts has to be a tabular whitespace</li>
  <li>The separator between the UMLS concepts has to be a semicolon (;)</li>
  <li>Each figure ID of the test set must be included in the submitted file exactly once (even if there 
 are not concepts)</li>
  <li>The same concept cannot be specified more than once for a given figure ID</li>
</ul>

<h3 id=""acknowledgements"">Acknowledgements</h3>

<p><a href=""http://www.ncbi.nlm.nih.gov/pmc/"" target=""_blank""> PubMed Central </a></p>

<h3 id=""references"">References</h3>
<p>[1] O. Pelka, S. Koitka, J. Rückert, F. Nensa und C. M. Friedrich „Radiology Objects in COntext (ROCO): A Multimodal Image Dataset“, Proceedings of the MICCAI Workshop on Large-scale Annotation of Biomedical data and Expert Label Synthesis (MICCAI LABELS 2018), Granada, Spain, September 16, 2018, Lecture Notes in Computer Science (LNCS) Volume 11043, Page 180-189, DOI: 10.1007/978-3-030-01364-6_20, Springer Verlag, 2018.</p>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>
<p>Evaluation is conducted in terms of <strong>F1 scores</strong> between system predicted and ground truth concepts, using the following methodology and parameters:</p>

<ul>
  <li>
    <p>The default implementation of the Python scikit-learn (v0.17.1-2) F1 scoring method is used. It is documented here.</p>
  </li>
  <li>
    <p>A Python (3.x) script loads the candidate run file, as well as the ground truth (GT) file, and processes each candidate-GT concept sets</p>
  </li>
  <li>
    <p>For each candidate-GT concept set, the <strong>y_pred</strong> and <strong>y_true</strong> arrays are generated. They are binary arrays indicating for each concept contained in both candidate and GT set if it is present (1) or not (0).</p>
  </li>
  <li>
    <p>The F1 score is then calculated. The default ‘binary’ averaging method is used.</p>
  </li>
  <li>
    <p>All F1 scores are summed and averaged over the number of elements in the test set (10’000), giving the final score.</p>
  </li>
</ul>

<p>The ground truth for the test set was generated based on the <a href=""https://download.nlm.nih.gov/umls/kss/2018AB/umls-2016AB-full.zip"" target=""_blank""> UMLS Full Release 2018AB </a>.</p>

<p><strong>NOTE</strong> : The source code of the evaluation tool is available here. It <strong>must</strong> be executed using Python <strong>3.x</strong>, on a system where the scikit-learn (<strong>&gt;= v0.17.1-2</strong>) Python library is installed. The script should be run like this:</p>

<p><code class=""highlighter-rouge"">
/path/to/python3 evaluate-f1.py /path/to/candidate/file /path/to/ground-truth/file
</code></p>

<hr />
<p><em>The leaderboard will be visible from 01.05.2019 (official deadline) on. The submission system will remain open a few more days. Results submitted after the deadline will not be part of the official results.</em></p>

<hr />

<h3 id=""resources"">Resources</h3>

<h3 id=""contact-us"">Contact us</h3>

<ul>
  <li>Technical issues : <a href=""https://gitter.im/crowdAI/imageclef-2019-caption-concept-detection"" target=""_blank""> https://gitter.im/crowdAI/imageclef-2019-caption-concept-detection </a></li>
  <li>Discussion Forum : <a href=""https://www.crowdai.org/challenges/imageclef-2019-caption-concept-detection/topics"" target=""_blank""> https://www.crowdai.org/challenges/imageclef-2019-caption-concept-detection/topics </a></li>
</ul>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

<ul>
  <li>Obioma Pelka: obioma[DOT]pelka[AT]fh-dortmund[DOT]de</li>
  <li>Christoph M. Friedrich: christoph[DOT]friedrich[AT]fh-dortmund[DOT]de</li>
  <li>Alba Garcia Seco de Herrera: alba[DOT]garcia[AT]essex[DOT]ac[DOT]uk</li>
  <li>Henning Müller: henning[DOT]mueller[AT]hevs[DOT]ch</li>
</ul>

<h3 id=""more-information"">More information</h3>

<p>You can find additional information on the challenge here:
<a href=""https://www.imageclef.org/2019/medical/caption"" target=""_blank""> http://imageclef.org/2019/caption </a></p>

<h3 id=""prizes"">Prizes</h3>

<p>ImageCLEF 2019 is an evaluation campaign that is being organized as part of the <a href=""http://clef2019.clef-initiative.eu/"" target=""_blank""> CLEF initiative </a> labs. The campaign offers several research tasks that welcome participation from teams around the world. The results of the campaign appear in the working notes proceedings, published by CEUR Workshop Proceedings (CEUR-WS.org). Selected contributions among the participants, will be invited for publication in the following year in the Springer Lecture Notes in Computer Science (LNCS) together with the annual lab overviews.</p>

<h3 id=""datasets-license"">Datasets License</h3>

"
176,"<p><em>Note: This challenge is one of the two subtasks of the LifeCLEF <a href=""http://www.imageclef.org/node/230"" target=""_blank""> Bird identification challenge </a> 2018. For more information about the  other subtask click <a href=""/challenges/lifeclef-2018-bird-soundscape"" target=""_blank""> here </a>. Both challenges share the same training dataset.</em></p>

<h3 id=""challenge-description"">Challenge description</h3>

<p>The goal of the task is to identify the species of the most audible bird (i.e. the one that was intended to be recorded) in each of the provided test recordings. Therefore, the evaluated systems have to return a ranked list of possible species for each of the 12,347 test recordings. Each prediction item (i.e. each line of the file to be submitted) has to respect the following format:
&lt; MediaId;ClassId;Probability;Rank&gt;</p>

<p>Here is a short fake run example respecting this format on only 3 test MediaId:
<a href=""https://crowdai-prd.s3.eu-central-1.amazonaws.com/task_dataset_files/clef_task_5/bfb5b15f-ef02-4aa1-82fb-864acd6d00a1_monophone_fake_run.csv?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAILFF3ZEGG7Y4HXEQ%2F20180503%2Feu-central-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20180503T122734Z&amp;X-Amz-Expires=604800&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Signature=e007999e7066658183047a2cd88f9dc25256f306eea25b8bdac45c53813bd01d"">monophone_fake_run</a></p>

<p>Each participating group is allowed to submit up to 4 runs built from different methods. Semi-supervised, interactive or crowdsourced approaches are allowed but will be compared independently from fully automatic methods. Any human assistance in the processing of the test queries has therefore to be signaled in the submitted runs.</p>

<p>Participants are allowed to use any of the provided metadata complementary to the audio content (.wav 44.1, 48 kHz or 96 kHz sampling rate), and will also be allowed to use any external training data but at the condition that (i) the experiment is entirely re-producible, i.e. that the used external resource is clearly referenced and accessible to any other research group in the world, (ii) participants submit at least one run without external training data so that we can study the contribution of such resources, (iii) the additional resource does not contain any of the test observations. It is in particular strictly forbidden to crawl training data from: <a href=""http://www.xeno-canto.org/"" target=""_blank""> www.xeno-canto.org </a></p>

<h3 id=""data"">Data</h3>

<p>The data collection will be the same as the one used in BirdCLEF 2017, mostly based on the contributions of the <a href=""http://www.xeno-canto.org/"" target=""_blank""> Xeno-Canto </a> network. The training set contains 36,496 recordings covering 1500 species of central and south America (the largest bioacoustic dataset in the literature). It has a massive class imbalance with a minimum of four recordings for Laniocera rufescens and a maximum of 160 recordings for Henicorhina leucophrys. Recordings are associated to various metadata such as the type of sound (call, song, alarm, flight, etc.), the date, the location, textual comments of the authors, multilingual common names and collaborative quality ratings. The test set contains 12,347 recordings of the same type (mono-phone recordings). More details about that data can be found in the <a href=""http://ceur-ws.org/Vol-1866/invited_paper_8.pdf"" target=""_blank""> overview working note of BirdCLEF 2017 </a>.</p>

<h3 id=""submission-instructions"">Submission instructions</h3>

<hr />
<p><em>As soon as the submission is open, you will find a “Create Submission” button on this page (just next to the tabs)</em></p>

<hr />

<p>More information will be available soon.</p>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>
<p>The used metric will be the Mean Reciprocal Rank (MRR). The MRR is a statistic measure for evaluating any process that produces a list of possible responses to a sample of queries ordered by probability of correctness. The reciprocal rank of a query response is the multiplicative inverse of the rank of the first correct answer. The MRR is the average of the reciprocal ranks for the whole test set:
<img src=""https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/4b2b4e82a9d495d540ac7b4d7c3db95f_mrr-crowdai2.png"" alt=""mrr-crowdai2.png"" /></p>

<table>
  <tbody>
    <tr>
      <td>where</td>
      <td>Q</td>
      <td>is the total number of query occurrences in the test set.</td>
    </tr>
  </tbody>
</table>

<h3 id=""resources"">Resources</h3>

<h3 id=""contact-us"">Contact us</h3>

<ul>
  <li>Technical issues : <a href=""https://gitter.im/crowdAI/lifeclef-2018-bird-monophone"" target=""_blank""> https://gitter.im/crowdAI/lifeclef-2018-bird-monophone </a></li>
  <li>Discussion Forum : <a href=""https://www.crowdai.org/challenges/lifeclef-2018-bird-monophone/topics"" target=""_blank""> https://www.crowdai.org/challenges/lifeclef-2018-bird-monophone/topics </a></li>
</ul>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

<ul>
  <li>Sharada Prasanna Mohanty: sharada.mohanty@epfl.ch</li>
  <li>Hervé Glotin: glotin[AT]univ-tln[DOT]fr</li>
  <li>Hervé Goëau: herve[DOT]goeau[AT]cirad[DOT]fr</li>
  <li>Alexis Joly: alexis[DOT]joly[AT]inria[DOT]fr</li>
  <li>Ivan Eggel: ivan[DOT]eggel[AT]hevs[DOT]ch</li>
</ul>

<h3 id=""more-information"">More information</h3>

<p>You can find additional information on the challenge here:
<a href=""http://imageclef.org/node/230"" target=""_blank""> http://imageclef.org/node/230 </a></p>

<h3 id=""baseline-repository"">Baseline Repository</h3>

<p>You can find a baseline system and a continuative tutorial can be found here: <a href=""link_url"" target=""_blank""> https://github.com/kahst/BirdCLEF-Baseline </a></p>

<p>We encourage all participants of the challenge to build upon the provided code base and share the results for future reference.</p>

<h3 id=""results-tables-and-figures"">Results (tables and figures)</h3>
<p><a href=""http://www.imageclef.org/node/230"" target=""_blank"">(Official round during the LifeCLEF 2018 campaign)</a></p>

<h3 id=""prizes"">Prizes</h3>

<p>LifeCLEF 2018 is an evaluation campaign that is being organized as part of the <a href=""http://clef2018.clef-initiative.eu/"" target=""_blank""> CLEF initiative </a> labs. The campaign offers several research tasks that welcome participation from teams around the world. The results of the campaign appear in the working notes proceedings, published by CEUR Workshop Proceedings (CEUR-WS.org). Selected contributions among the participants, will be invited for publication in the following year in the Springer Lecture Notes in Computer Science (LNCS) together with the annual lab overviews.</p>

<h3 id=""datasets-license"">Datasets License</h3>

"
157,"<hr />
<p><strong>Important note:</strong></p>

<p><em>The <strong>ImageCLEF Lifelog - Activities of Daily Living understanding (ADLT) challenge has officially ended</strong> and we would like to thank everybody for their participation. You can find the official results at <a href=""http://imageclef.org/2018/lifelog"">http://imageclef.org/2018/lifelog</a>.</em></p>

<p><em>Post-challenge submissions and the leaderboard will remain enabled  for a few weeks so you will still be able to submit result files and have them continuously evaluated during a limited period. 
Please consider that in order to see the version of the leaderboard with the post-challenge submissions integrated, you have to turn on the switch <strong>Show post-challenge submission</strong> right below the leaderboard.</em></p>

<p><em>At the same time we’d like to encourage you to submit a <a href=""http://clef2018.clef-initiative.eu/index.php?page=Pages/InstructionsforCLEF2018WorkingNotes.html"">CLEF Working notes paper</a> until the end of May.</em></p>

<p><em>Please also note that participants registering from now on will not be
automatically registered with CLEF anymore.</em></p>

<hr />

<p><em>Note: ImageCLEF Lifelog 2018 is divided into 2 subtasks (challenges). This challenge is about Activities of Daily Living understanding (<strong>ADLT</strong>). For information on the Lifelog moment retrieval (<strong>LMRT</strong>) challenge click <a href=""/challenges/imageclef-2018-lifelog-lmrt"" target=""_blank""> here </a>. Both challenges share the same dataset, so registering for one of these challenges will automatically give you access to the other one.</em></p>

<p><em>Note: Do not forget to read the <strong>Rules</strong> section on this page</em></p>

<h3 id=""motivation"">Motivation</h3>

<p>An increasingly wide range of personal devices, such as smartphones, video cameras as well as wearable devices that allow capturing pictures, videos, and audio clips in every moment of our life are becoming available. Considering the huge volume of data created, commonly referred to as lifelogs, there is a need for systems that can automatically analyse the data in order to categorize, summarize and also query to retrieve the information the user may need.</p>

<p>Despite the increasing number of successful related workshops and panels ( <a href=""http://www.jcdl.org/archived-conf-sites/jcdl2015/www.jcdl2015.org/panels.html"" target=""_blank""> JCDL 2015  </a>, <a href=""http://irlld2016.computing.dcu.ie/index.html"" target=""_blank""> iConf 2016 </a> , <a href=""http://lta2016.computing.dcu.ie/styled/index.html"" target=""_blank""> ACM MM 2016 </a> ,  <a href=""http://lta2017.computing.dcu.ie/"" target=""_blank""> ACM MM 2017 </a> ) lifelogging has seldom been the subject of a rigorous comparative benchmarking exercise as, for example, the new lifelog evaluation task at <a href=""http://ntcir-lifelog.computing.dcu.ie/"" target=""_blank""> NTCIR-13 </a> or the last year edition of the <a href=""http://www.imageclef.org/2017/lifelog"" target=""_blank""> ImageCLEFlifelog </a> task. In this edition of this task we aim to bring the attention of lifelogging to an as wide as possible audience and to promote research into some of the key challenges of the coming years.</p>

<h3 id=""challenge-description"">Challenge description</h3>

<p>Given a period of time, e.g., “From 13 August to 16 August” or “Every Saturday”, the participants should analyse the lifelog data and provide a summarisation based on the selected concepts (provided by the task organizers) of Activities of Daily Living (ADL) and the environmental settings / contexts in which these activities take place.</p>

<p>Some examples of ADL concepts: “Commuting (to work or another common venue)”, “Travelling (to a destination other than work, home or another common social event)”, “Preparing meals (include making tea or coffee)”, “Eating/drinking”, and contexts: “In an office environment”, “In a home”, “In an open space”. The summarisation should be described as the frequency and spending time for ADL concepts and total time for contexts concepts. For example:</p>

<ul>
  <li>
    <p>ADL: “Eating/drinking: 6 times, 90 minutes”, “Travelling: 1 time, 60 minutes”;</p>
  </li>
  <li>
    <p>Context: “In an office environment: 500 minutes”, “In a church: 30 minutes”.</p>
  </li>
</ul>

<h3 id=""data"">Data</h3>

<p>The task will be split into two related subtasks using a completely new multimodal dataset which consists of 50 days of data from a lifelogger, namely: images (1,500-2,500 per day from wearable cameras), visual concepts (automatically extracted visual concepts with varying rates of accuracy), semantic content (semantic locations, semantic activities) based on sensor readings (via the Moves App) on mobile devices, biometrics information (heart rate, galvanic skin response, calorie burn, steps, etc.), music listening history. The dataset is built based on the data available for the <a href=""http://ntcir-lifelog.computing.dcu.ie/"" target=""_blank""> NTCIR-13 - Lifelog 2 task </a>.</p>

<h3 id=""format-of-the-metadata"">Format of the metadata</h3>
<p>The metadata is stored in an .xml file, which is a simple aggregation of all users data. It is structured as follows:</p>

<p>The root node of the data is the USERS tag. Each user element contains all the data of that user (u1 or u2). Each user has a tag USER that contains the user ID as an attribute, example: [user id=”u1”]. For this year, only user u1 is considered. Inside the USER element, is his/her data:</p>

<p>Following that there is a tag DAYS, this tag contains the lifelogging information of that user organised per day, each day is included in a tag DAY that has the data (a tag DATA), the relative path to the directory that contains the images captured in that particular day (the tag IMAGES-DIRECTORY), then the minutes of of that day under a root tag called MINUTES.</p>

<p>At the start of each day there is a set of daily metatdata for that user. This data is of three forms; BIOMETRICS, ACTIVITIES &amp; PERSONAL LOGS. The biometrics contains WEIGHT, FAT MASS, HEART RATE, SYSTOLIC blood pressure &amp; DIASTOLIC blood pressure, which were readings taken after waking up each day. The activities contains summary activities: STEPS taken that day, DISTANCE walked in metres that day &amp; ELEVATION climbed in metres that day. The personal logs contain HEALTH LOGS, including the TIME of reading, GLU Glucose levels in the blood, BP Blood Pressure, HR Heart Rate, MOOD manually logged every morning and sometimes a COMMENT, as well as DRINK LOGS and FOOD LOGS which were manually logged throughout the dat.</p>

<p>Following that, the day’s data is organised into minutes. The MINUTES element, contains exactly 1440 child elements (called MINUTE), each child has an ID (example: [minute id=“0”], [minute id=“1”], [minute id=“2”]… etc), and it represent one minute in the day ordered from 0 = 12:00 AM, to 1439 = 23:59PM.</p>

<p>Each minute contains: 0 or 1 location information (LOCATION tag), 0 or one activity information (ACTIVITY tag), biometrics, 0 or more captured images (IMAGES tag with IMAGE child element (each element has has a relative path to the image and a unique image ID), and 0 or 1 MUSIC tag giving details of the music listened to at that point in time.</p>

<p>-The location information is captured by Moves app (https://www.moves-app.com/), and they represent to semantic locations (Home, Work, DCU Computing building, GYM, Name of a Store, etc…), or to landmark locations registered by Moves. This tag can contain information in several languages. For locations that are not (HOME) or (WORK), the GPS locations are provided.</p>

<h3 id=""submission-instructions"">Submission instructions</h3>

<hr />
<p><em>As soon as the submission is open, you will find a “Create Submission” button on this page (just next to the tabs)</em></p>

<hr />

<p>A submitted run for the ADLT sub-task must be in the form of a CSV file in the following format:</p>

<p>[topic id, number of times, number of minutes]</p>

<p>Where:
- topic id: Number of the queried topic, e.g., from 1 to 10 for the development set.
- number of times, number of minutes: positive integer numbers.</p>

<p>Then comes the optional information in the following format, starts with a line of 6 asterisk characters ‘<strong>**</strong>’, and the optional information:</p>

<p>[topic id, time id, image id]</p>

<p>Where:
- topic id: Number of the queried topic, e.g., from 1 to 10 for the development set.
- image id: ID of a relevant image.
- time id: the occasion id for each time the ADL occurs, counted from 1.</p>

<p>Sample:</p>

<p>1, 3, 300</p>

<p>2, 4, 25</p>

<p>3, 5, 200</p>

<p>…</p>

<p>10, 3, 400</p>

<p>******</p>

<p>1, 1, u1_2015-08-01_145314_1</p>

<p>1, 1, u1_2015-08-01_145345_2</p>

<p>1, 2, u1_2015-08-01_145531_1</p>

<p>…
// should have about 300 lines for topic id 1, then 25 lines for topic id 2, and so on</p>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>
<p>### Metrics</p>

<p>The final score is computed as the average of the times and minutes, as follows:</p>

<p>$ADL_{score} = \frac{1}{2} \left(max(0, 1 - \frac{abs(n - n_{gt})}{n_{gt}}) + max(0, 1 - \frac{abs(m - m_{gt})}{m_{gt}})\right)$</p>

<p>where $n, n_{gt}$ are the submitted and ground-truth values for how many times the events occurred, respectively, and $m, m_{gt}$ are the submitted and ground-truth values for how long (in minutes) the events happened, respectively.</p>

<h3 id=""resources"">Resources</h3>

<h3 id=""recommanded-reading"">Recommanded Reading</h3>

<p>[1] Duc-Tien Dang-Nguyen, Luca Piras, Michael Riegler, Giulia Boato, Liting Zhou, Cathal Gurrin, <a href=""https://pralab.diee.unica.it/sites/default/files/invited_paper_10.pdf"" target=""_blank""> “Overview of ImageCLEFlifelog 2017: Lifelog Retrieval and Summarization” </a>, CLEF2017 Working Notes, Dublin, Ireland, 2017, vol 1866.</p>

<p>[2] Cathal Gurrin, Xavier Giro-i-Nieto, Petia Radeva, Mariella Dimiccoli, Håvard Johansen, Hideo Joho, Vivek K Singh, <a href=""http://dl.acm.org/citation.cfm?id=2980534"" target=""_blank""> “LTA 2016: The First Workshop on Lifelogging Tools and Applications” </a>, ACM Multimedia, Amsterdam, The Netherlands, 2016.</p>

<p>[3] Cathal Gurrin, Xavier Giro-i-Nieto, Petia Radeva, Mariella Dimiccoli, Duc Tien Dang Nguyen, Hideo Joho, <a href=""http://doras.dcu.ie/22031/1/cGurrina.pdf"" target=""_blank""> “LTA 2017: The Second Workshop on Lifelogging Tools and Applications” </a>, ACM Multimedia, Mountain View, CA USA, 2017.</p>

<p>[4] Cathal Gurrin, Hideo Joho, Frank Hopfgartner, Liting Zhou, Rami Albatal, <a href=""http://research.nii.ac.jp/ntcir/workshop/OnlineProceedings12/pdf/ntcir/OVERVIEW/01-NTCIR12-OV-LIFELOG-GurrinC.pdf"" target=""_blank""> “Overview of NTCIR-12 Lifelog Task” </a>, Proceedings of the 12th NTCIR Conference on Evaluation of Information Access Technologies, Tokyo, Japan, 2016.</p>

<p>[5] Duc-Tien Dang-Nguyen, Luca Piras, Giorgio Giacinto, Giulia Boato, Francesco GB De Natale, <a href=""https://dl.acm.org/citation.cfm?id=3103613"" target=""_blank""> “Multimodal Retrieval with Diversification and Relevance Feedback for Tourist Attraction Images” </a>, ACM Transactions on Multimedia Computing, Communications, and Applications, vol 13, n° 4, 2017.</p>

<p>[6] Duc-Tien Dang-Nguyen, Luca Piras, Giorgio Giacinto, Giulia Boato, Francesco GB De Natale, <a href=""http://ieeexplore.ieee.org/document/7177486/?arnumber=7177486&amp;tag=1"" target=""_blank""> “A hybrid approach for retrieving diverse social images of landmarks” </a>, IEEE International Conference on Multimedia and Expo (ICME), Turin, Italy, 2015.</p>

<p>[7] <a href=""http://ceur-ws.org/Vol-1436/"" target=""_blank""> Working notes of the 2015 MediaEval Retrieving Diverse Social Images task </a>, CEUR-WS.org, Vol. 1436, ISSN: 1613-0073.</p>

<p>[8] B. Ionescu, A.L. Gînscă, B. Boteanu, M. Lupu, A. Popescu,H. Müller, <a href=""http://imag.pub.ro/~bionescu/index_files/DivTask_MMSys2016.pdf"" target=""_blank""> “Div150Multi: A Social Image Retrieval Result Diversification Dataset with Multi-topic Queries” </a>, ACM MMSys, Klagenfurt, Austria, 2016.</p>

<h3 id=""helpful-tools-and-resources"">Helpful tools and resources</h3>

<ul>
  <li>
    <p><a href=""http://eyeaware.computing.dcu.ie/"" target=""_blank""> Eyeaware lifelogging framework </a></p>
  </li>
  <li>
    <p><a href=""http://opencv.org/"" target=""_blank""> OpenCV – Open Source Computer Vision </a></p>
  </li>
  <li>
    <p><a href=""http://www.lire-project.net/"" target=""_blank""> LIRE: Lucence Image Retrieval </a></p>
  </li>
  <li>
    <p><a href=""http://trec.nist.gov/trec_eval/index.html"" target=""_blank""> trec_eval scoring software </a></p>
  </li>
  <li>
    <p><a href=""http://www.imageclef.org/"" target=""_blank""> ImageCLEF - Image Retrieval in CLEF </a></p>
  </li>
  <li>
    <p><a href=""http://www.cs.waikato.ac.nz/ml/weka/"" target=""_blank""> Weka Data Mining Software </a></p>
  </li>
  <li>
    <p><a href=""https://developer.nvidia.com/digits"" target=""_blank""> Nvidia DIGITS </a></p>
  </li>
  <li>
    <p><a href=""http://caffe.berkeleyvision.org/"" target=""_blank""> Caffee deep learning framework </a></p>
  </li>
  <li>
    <p><a href=""http://creativecommons.org/"" target=""_blank""> Creative Commons </a></p>
  </li>
</ul>

<h3 id=""contact-us"">Contact us</h3>

<ul>
  <li>Technical issues : <a href=""https://gitter.im/crowdAI/imageclef-2018-lifelog-adlt"" target=""_blank""> https://gitter.im/crowdAI/imageclef-2018-lifelog-adlt </a></li>
  <li>Discussion Forum : <a href=""https://www.crowdai.org/challenges/imageclef-2018-lifelog-adlt/topics"" target=""_blank""> https://www.crowdai.org/challenges/imageclef-2018-lifelog-adlt/topics </a></li>
</ul>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

<ul>
  <li>Sharada Prasanna Mohanty: sharada.mohanty@epfl.ch</li>
  <li>Duc-Tien Dang-Nguyen: duc-tien[DOT]dang-nguyen[AT]dcu[DOT]ie</li>
  <li>Luca Piras: luca[DOT]piras[AT]diee[DOT]unica[DOT]it</li>
  <li>Ivan Eggel: ivan[DOT]eggel[AT]hevs[DOT]ch</li>
</ul>

<h3 id=""more-information"">More information</h3>

<p>You can find additional information on the challenge here:
<a href=""http://imageclef.org/2018/lifelog"" target=""_blank""> http://imageclef.org/2018/lifelog </a></p>

<h3 id=""prizes"">Prizes</h3>

<p>ImageCLEF 2018 is an evaluation campaign that is being organized as part of the <a href=""http://clef2018.clef-initiative.eu/"" target=""_blank""> CLEF initiative </a> labs. The campaign offers several research tasks that welcome participation from teams around the world. The results of the campaign appear in the working notes proceedings, published by CEUR Workshop Proceedings (CEUR-WS.org). Selected contributions among the participants, will be invited for publication in the following year in the Springer Lecture Notes in Computer Science (LNCS) together with the annual lab overviews.</p>

<h3 id=""datasets-license"">Datasets License</h3>

"
177,"<p><img src=""https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/c87aa80b0c17962f6a7eacd3d35e38ff_manmachine.png"" alt="" expert"" /></p>

<p><em>Note: Do not forget to read the <strong>Rules</strong> section on this page</em></p>

<h3 id=""usage-scenario"">Usage scenario</h3>

<p>Automated identification of plants and animals has improved considerably in the last few years. In the scope of LifeCLEF 2017 in particular, we measured impressive identification performance achieved thanks to recent deep learning models (e.g. up to 90% classification accuracy over 10K species). This raises the question of how far automated systems are from the human expertise and of whether there is a upper bound that can not be exceeded. A picture actually contains only a partial information about the observed plant and it is often not sufficient to determine the right species with certainty. For instance, a decisive organ such as the flower or the fruit, might not be visible at the time a plant was observed. Or some of the discriminant patterns might be very hard or unlikely to be observed in a picture such as the presence of pills or latex, or the morphology of the root. As a consequence, even the best experts can be confused and/or disagree between each others when attempting to identify a plant from a set of pictures. Similar issues arise for most living organisms including fishes, birds, insects, etc. Quantifying this intrinsic data uncertainty and comparing it to the performance of the best automated systems is of high interest for both computer scientists and expert naturalists.</p>

<h3 id=""challenge-description"">Challenge description</h3>

<p>The goal of the task will be to return the most likely species for each observation of the test set. More practically, the run file to be submitted has to contain as much lines as the number of predictions, each prediction being composed of an ObservationId (the identifier of a specimen that can be itself composed of several images), a ClassId, a Probability and a Rank (used in case of equal probabilities). Each line should have the following format:
&lt;ObservationId;ClassId;Probability;Rank&gt;</p>

<p>Here is a short fake run example respecting this format for only 3 observations:
<a href=""https://crowdai-prd.s3.eu-central-1.amazonaws.com/task_dataset_files/clef_task_6/3d3559de-b5d9-459d-9d76-3781fb508c14_myrun1exp.csv?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAILFF3ZEGG7Y4HXEQ%2F20180503%2Feu-central-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20180503T120421Z&amp;X-Amz-Expires=604800&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Signature=d8a0f08510f4656207b8ba47dba9e727ddc13445a959dbb7f60d61fda910951c"">fake_run</a></p>

<p>The small fraction of the test set identified by the pool of experts will then be used to conduct the experts vs. machines evaluation.</p>

<h3 id=""data"">Data</h3>

<p>To conduct a valuable experts vs. machines experiment, we collected image-based identifications from the best experts in the plant domain. Therefore, we created sets of observations that were identified in the field by other experts (in order to have a near-perfect golden standard). These pictures will be immersed in a much larger test set that will have to be processed by the participating systems. As for training data, the datasets of the previous LifeCLEF campaigns will be made available to the participants and might be extended with new contents. It will contain between 1M and 2M pictures.</p>

<h3 id=""submission-instructions"">Submission instructions</h3>

<hr />
<p><em>As soon as the submission is open, you will find a “Create Submission” button on this page (just next to the tabs)</em></p>

<hr />

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>
<p>The two main evaluation metrics will be the top-1 accuracy on 1) the fraction of the test set identified by the pool of experts, 2) on the whole test set.</p>

<h3 id=""resources"">Resources</h3>

<h3 id=""contact-us"">Contact us</h3>

<ul>
  <li>Technical issues : <a href=""https://gitter.im/crowdAI/lifeclef-2018-expert"" target=""_blank""> https://gitter.im/crowdAI/lifeclef-2018-expert </a></li>
  <li>Discussion Forum : <a href=""https://www.crowdai.org/challenges/lifeclef-2018-expert/topics"" target=""_blank""> https://www.crowdai.org/challenges/lifeclef-2018-expert/topics </a></li>
</ul>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

<ul>
  <li>Sharada Prasanna Mohanty: sharada.mohanty@epfl.ch</li>
  <li>Hervé Goëau: herve[DOT]goeau[AT]cirad[DOT]fr</li>
  <li>Alexis Joly: alexis[DOT]joly[AT]inria[DOT]fr</li>
  <li>Ivan Eggel: ivan[DOT]eggel[AT]hevs[DOT]ch</li>
</ul>

<h3 id=""more-information"">More information</h3>

<p>You can find additional information on the challenge here:
<a href=""http://imageclef.org/node/231"" target=""_blank""> http://imageclef.org/node/231 </a></p>

<h3 id=""results-tables-and-figures"">Results (tables and figures)</h3>
<p><a href=""http://www.imageclef.org/node/231"" target=""_blank"">(Official round during the LifeCLEF 2018 campaign)</a></p>

<h3 id=""prizes"">Prizes</h3>

<p>LifeCLEF 2018 is an evaluation campaign that is being organized as part of the <a href=""http://clef2018.clef-initiative.eu/"" target=""_blank""> CLEF initiative </a> labs. The campaign offers several research tasks that welcome participation from teams around the world. The results of the campaign appear in the working notes proceedings, published by CEUR Workshop Proceedings (CEUR-WS.org). Selected contributions among the participants, will be invited for publication in the following year in the Springer Lecture Notes in Computer Science (LNCS) together with the annual lab overviews.</p>

<h3 id=""datasets-license"">Datasets License</h3>

"
5,"<p>The <a href=""https://sites.google.com/view/mediqa2019"" target=""_blank""> MEDIQA </a> challenge is an  <a href=""https://aclweb.org/aclwiki/BioNLP_Workshop"" target=""_blank""> ACL-BioNLP </a> 2019 shared task aiming to attract further research efforts in Natural Language Inference (NLI), Recognizing Question Entailment (RQE), and their applications in medical Question Answering (QA).</p>

<h1 id=""natural-language-inference-task-nli"">Natural Language Inference Task (NLI)</h1>
<p>The objective of this task is to identify three inference relations between two sentences: Entailment, Neutral and Contradiction.</p>

<h1 id=""datasets"">Datasets</h1>

<p>Training set: <a href=""https://jgc128.github.io/mednli/"" target=""_blank"">  https://jgc128.github.io/mednli/ </a> 
Participants will have to obtain access to MIMIC in order to access <strong>MedNLI</strong> and the <strong>test set</strong>.</p>

<h1 id=""timeline"">Timeline</h1>

<ul>
  <li>April 15, 2019: Release of the test sets for the 3 tasks.</li>
  <li>April 30, 2019: Run submission deadline. Participants’ results will be available on AIcrowd.</li>
  <li>May 15, 2019: Paper submission deadline.</li>
  <li>August 1, 2019: BioNLP workshop, ACL 2019, Florence, Italy.</li>
</ul>

<h1 id=""evaluation-criteria"">Evaluation criteria</h1>

<p>For the NLI and RQE tasks:</p>

<ul>
  <li>The evaluation will be based on <em>Accuracy</em>.</li>
  <li>For the result submission file, we expect a csv file with the following header:
<strong>pair_id,label</strong> (cf. <a href=""https://www.aicrowd.com/challenges/mediqa-2019-recognizing-question-entailment-rqe/dataset_files"" target=""_blank""> ValSet_Baseline.csv </a> of the RQE task)</li>
</ul>

<h1 id=""rules"">Rules</h1>

<p>1) Each team is allowed to submit a maximum of 5 <strong>runs</strong>.</p>

<p>2) Please choose a <strong>username</strong> that represents your team, and <strong>update your profile</strong> with the following information: First name, Last nam, Affiliation, Address, City, Country.</p>

<p>3) For each run submission, it is mandatory to fill in the <strong>submission description</strong> field of the submission form with a short description of the methods, tools and resources used for that run.</p>

<p>4) The final results will not be considered official until a <strong>working notes paper</strong> with the full description of the methods is submitted.</p>

<h1 id=""contact-us"">Contact us</h1>

<ul>
  <li>We strongly encourage you to use our <strong>mailing list</strong> for communications between the participants and the organizers: <a href=""https://groups.google.com/d/forum/bionlp-mediqa"" target=""_blank""> https://groups.google.com/d/forum/bionlp-mediqa </a></li>
  <li>In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at: <a href=""asma.benabacha@nih.gov"" target=""_blank""> asma.benabacha@nih.gov </a></li>
</ul>
"
185,"<p>Using the provided data sets of financial predictors and semi-annual returns, participants are challenged to develop a model that will help identify the best-performing stocks in each time-period.</p>

<p>Research Question: <strong><em>Which stocks will experience the highest and lowest returns during the next six months?</em></strong></p>

<p>Out of the thousands of stocks in the market, small groups will experience exceptionally high or low returns. Considering the distribution of stock returns, a portfolio manager must buy the stocks in the right tail of the distribution and avoid the stocks in the left tail. The performance of an entire equity portfolio is often driven by these key investment decisions. The goal of this challenge is to explore methodology that will increase the probability that portfolio managers identify these stocks with extreme positive or negative returns.</p>

<p>Each team must create a model that ranks a set of stocks based on the expected return over a forward 6-month window. This model can be a risk factor-based strategy (multi-factor model), predictive model, or any other data-based heuristic. There are many ways to approach this task and creative, non-traditional solutions are strongly encouraged. The final model will be tested on each 6-month period from 2002 to 2017.</p>

<h2 id=""motivation"">Motivation</h2>

<p>Analysts rely on a mix of quantitative and qualitative methodology to help investors consistently outperform the market. It’s not enough to be investment experts. Having the right data at the right time plays a critical role in successfully anticipating economic and environmental changes that may impact investment performance. Personalized solutions can be designed to provide a tailored mix of risk and return. Current baseline solutions rely on simple regressions and/or random forest solutions. Current approaches have high explanatory value and low predictive value. Improved solutions would increase predictive accuracy.</p>

<h2 id=""dataset"">Dataset</h2>

<p>Teams are provided with predictors and semi-annual returns for a group of stocks from <code class=""highlighter-rouge"">1996</code> to <code class=""highlighter-rouge"">2017</code>. This span of <strong>21 years</strong> is represented as <strong>42 non-overlapping 6-month periods</strong>. In each of the <code class=""highlighter-rouge"">42 time periods</code>, roughly <strong>900 stocks</strong> with the largest market capitalization (i.e., total market value in USD) were selected. Therefore, the selected set of stocks at each time period changes as companies increase or decrease in value. <strong>All stock identifiers have been removed</strong> and <strong>all numeric variables have been anonymized and normalized</strong>. Training and test datasets were created by selecting a <strong>random sample of stocks</strong> at each time period. <code class=""highlighter-rouge"">60%</code> of stocks were sampled into the training set and the remaining <code class=""highlighter-rouge"">40%</code> created the test set. Finally, all data from the second half of 2017 was allocated to the test set. This 6-month period will provide a final out-of-sample test of a model’s performance.</p>

<p><strong>Note</strong> : Please refer to the <a href=""https://github.com/crowdAI/ieee_investment_ranking_challenge-starter-kit"">starter-kit</a> to quickly get started with the dataset, train a simple Random Forest based model, and make an example submission to crowdai.</p>

<h2 id=""evaluation"">Evaluation</h2>

<p>Consistent performance over time and through varying market conditions is crucial for any financial model. Each team must test their model using an expanding window procedure. For a given time period, <script type=""math/tex"">T</script>, an expanding window test allows the model to incorporate all available information up to time <script type=""math/tex"">T</script>, to generate predictions for time <script type=""math/tex"">T+1</script>. For example, when predicting the stock rankings in the first half of 2016, the model can include all data from 1996 to no later than year-end 2015. Predictions for the second half of 2016 could then include all the data from the first half of 2016. The quality of the predicted rankings at each time period will be evaluated in two ways, described below.</p>

<ul>
  <li>
    <p><strong>Spearman correlation</strong>: This metric will describe the overall relationship between the actual rankings and the predicted rankings from the model. Higher values indicate better performance.</p>
  </li>
  <li>
    <p><strong>Normalized Discounted Cumulative Gain of Top 20%</strong>: In reality, analysts and portfolio managers are not concerned with the entire distribution of stocks. They will instead focus on identifying and buying the best-ranking stocks. Normalized Discounted Cumulative Gain (NDCG) is a metric from the information retrieval domain that considers the relevance and confidence (rank position) to describe a model’s rank quality.</p>
  </li>
</ul>

<h2 id=""technical-details"">Technical Details</h2>

<h3 id=""spearman-correlation"">Spearman correlation</h3>

<p>Spearman correlation describes how well a model is ranking the stocks at a given time period. Spearman correlation is calculated using the formula below.</p>

<script type=""math/tex; mode=display"">r_s = 1 - \frac{6 \sum_i{}{d_i^2}}{n(n^2 - 1)}</script>

<p>Where $d_i$ is the difference between the predicted and actual ranking of stock <strong><em>i</em></strong>.</p>

<p>Spearman correlation has a range from -1 to 1. Models that rank stocks more accurately will produce higher Spearman correlation values. Correlation values will be averaged across all time periods.</p>

<p><a href=""https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient"" target=""""> Click here for more details on Spearman correlation. </a></p>

<h3 id=""normalized-discounted-cumulative-gain-of-top-20"">Normalized Discounted Cumulative Gain of Top 20%</h3>

<p>Normalized Discounted Cumulative Gain (NDCG) is the ratio between the Discounted Cumulative Gain (DCG) and Ideal Discounted Cumulative Gain (IDCG), shown below.</p>

<script type=""math/tex; mode=display"">NDCG = \frac{DCG}{IDCG}</script>

<p>Where:</p>

<script type=""math/tex; mode=display"">DCG = \sum_{i=1}^n{\frac{n}{n+i} Z_i}</script>

<p><script type=""math/tex"">Z_i</script> represents the normalized future 6-month return (Norm_Ret_F6M in the dataset) of the <script type=""math/tex"">i^{th}</script> ranked stock. With this formula, stocks with better (lower) predicted ranks will have more influence on the ranking quality than stocks with higher predicted ranks. IDCG is the maximum possible DCG, which gives the NDCG score an upper bound of 1. The NDCG will be calculated for each individual 6-month period and then averaged across all periods.</p>

<p>Note that the NDCG is calculated using only the top 20% of a model’s predicted rankings. Therefore, NDCG rewards correctly identifying stocks in the top 20% and ranking them in the correct order. This aligns with the viewpoint of a ‘long-only’ portfolio manager who will focus on buying the best stocks and ignore stocks outside the top 20%.</p>

<p><a href=""https://en.wikipedia.org/wiki/Discounted_cumulative_gain"" target=""""> A more detailed description of NDCG can be found here. </a> This challenge uses a modified formulation of DCG that is tailored to investment ranking.</p>

<p><strong>Update: The evaluation script was incorrectly calculating NDCG as of the challenge launch. This was fixed 03/28. Solutions submitted prior to this date would have provided incorrect results.</strong></p>

<h1 id=""testing-your-solution"">Testing your solution</h1>

<p>Throughout the competition, teams will be given the opportunity to evaluate their models on the test dataset. Teams can sign in and upload their predictions up to 5 times per day. This will provide an estimate for out-of-sample performance during the competition. Teams should rely on internal model validation procedures and be careful not to optimize results to this one small section of the test dataset.</p>

<h1 id=""timeline"">Timeline</h1>

<ul>
  <li>
    <p><strong>March 26</strong> : Challenge launch and start of Round 1 - contestants create models and upload predictions to crowdAI.</p>
  </li>
  <li>
    <p><strong>April 30</strong> : Deadline for Round 1. All solutions must be submitted by 11:59 GMT; Top solutions from leader board invited to Round 2.</p>
  </li>
  <li>
    <p><strong>May 1</strong> : Start of Round 2 - Contestants explain their methods, results, and conclusions in short paper. Contestants also package code of submitted solution using Docker for testing and evaluation.</p>
  </li>
  <li>
    <p><strong>May 20</strong> : Deadline for Round 2. All solutions must be submitted by 11:59 GMT.</p>
  </li>
  <li>
    <p><strong>May 21</strong> :  Top 6 solutions selected; Winners provided travel stipend (maximum USD 1000) and invitation to present at the IEEE Data Science Workshop in Lausanne, Switzerland June 4 - June 6.</p>
  </li>
</ul>

<h1 id=""details-for-round-2"">Details for Round 2:</h1>

<p>Round 2 is open to all challenge participants. Round 1 focused on prototyping models that maximized statistical measures and Round 2 will enhance this with a deeper dive into your methodology and a new set of holdout data from 2017. To compete, all participants must submit the following items:</p>

<ul>
  <li>
    <p>Final predictions for all time periods</p>
  </li>
  <li>
    <p>A brief written solution using the <a href=""https://www.ieee.org/conferences/publishing/templates.html"" target="""">IEEE template for conference proceedings.</a> MS Word and LaTeX are both acceptable. At a minimum, the document should include an introduction, description of your methodology, results, and any other information needed to understand your solution and its merit. Tables, charts, and other visuals are highly encouraged.</p>
  </li>
  <li>
    <p>All code and files needed to reproduce your results uploaded to Gitlab. (More details on this soon)</p>
  </li>
</ul>

<p>The top 6 solutions will be selected based on their statistical performance, calculated in the following manner:</p>

<p>Final score = (A+B+C+D)/4</p>

<p>Where:</p>

<ul>
  <li>
    <p>A = Rank of spearman correlation on holdout data from 2002 – 2016</p>
  </li>
  <li>
    <p>B = Rank of NDCG score on holdout data from 2002 – 2016</p>
  </li>
  <li>
    <p>C = Rank of spearman correlation on holdout data from 2017</p>
  </li>
  <li>
    <p>D = Rank of NDCG score on holdout data from 2017</p>
  </li>
</ul>

<p>All ranks will be determined using 3 significant digits. Performance on the data from 2017 will be used as a tiebreaker if needed.</p>

<h2 id=""round-2-submission-details"">Round 2 Submission Details</h2>
<p><strong>UPDATE</strong></p>

<p>The Round-2 of the IEEE Investment Ranking Challenge is now accepting submissions.
Please remember to update your crowdai client and follow the instructions <a href=""https://github.com/crowdAI/ieee_investment_ranking_challenge-starter-kit#submission-of-predicted-file-to-crowdai"">here</a> before making a submission.</p>

<p>We will accept the submissions until 21st of May, and to be eligible for the final leaderboard, you will also have to upload your code as a private repository to <a href=""https://gitlab.crowdai.org/"">gitlab.crowdai.org</a>.</p>

<p>Please add the following users as Members  of your private repository : benharlander, spMohanty</p>

<p>Apart from your code, please include a description of your approach using <a href=""https://www.acm.org/publications/proceedings-template"">this template</a></p>

<p>The leaderboard of this challenge will be shown only at the end of the Round on May 21st, but the grading status of your submissions can still be checked under the Submissions Tab.</p>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>

<h3 id=""resources"">Resources</h3>

<h3 id=""starter-kit"">Starter Kit</h3>

<p>A starter kit has been prepared which explains how to get access to the dataset, parse it, train a simple random forest based method, and make a submission.
It can be accessed at : <a href=""https://github.com/crowdAI/ieee_investment_ranking_challenge-starter-kit"">https://github.com/crowdAI/ieee_investment_ranking_challenge-starter-kit</a></p>

<h3 id=""contact-us"">Contact Us</h3>

<ul>
  <li>Gitter Channel : <a href=""https://gitter.im/crowdAI/ieee-investment-ranking-challenge"">crowdAI/ieee-investment-ranking-challenge</a></li>
  <li>Technical issues : <a href=""https://github.com/crowdAI/ieee_investment_ranking_challenge-starter-kit/issues"" target=""_blank"">https://github.com/crowdAI/ieee_investment_ranking_challenge-starter-kit/issues </a></li>
  <li>Discussion Forum : <a href=""https://www.crowdai.org/challenges/ieee-investment-ranking-challenge/topics"">https://www.crowdai.org/challenges/ieee-investment-ranking-challenge/topics</a></li>
</ul>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organisers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

<ul>
  <li><a href=""mailto:Harlander.Benjamin@principal.com"" target=""_blank""> Harlander.Benjamin@principal.com </a></li>
  <li><a href=""mailto:sharada.mohanty@epfl.ch"" target=""_blank""> sharada.mohanty@epfl.ch
 </a></li>
</ul>

<h3 id=""prizes"">Prizes</h3>

<p>Top-6 participants on the leaderboard (except the organizers) will be invited (maximum USD 1000 travel stipend, provided by Principal Financial Group) to present at the <a href=""https://2018.ieeedatascience.org/"" target=""_blank"">IEEE Data Science Workshop</a> in Lausanne, June 4-6, 2018.</p>

<h3 id=""datasets-license"">Datasets License</h3>

"
7,"<p>The <a href=""https://sites.google.com/view/mediqa2019"" target=""_blank""> MEDIQA </a> challenge is an  <a href=""https://aclweb.org/aclwiki/BioNLP_Workshop"" target=""_blank""> ACL-BioNLP </a> 2019 shared task aiming to attract further research efforts in Natural Language Inference (NLI), Recognizing Question Entailment (RQE), and their applications in medical Question Answering (QA).</p>

<h1 id=""question-answering-task-qa"">Question Answering Task (QA)</h1>

<p>The objective of this task is to <strong>filter</strong> and <strong>improve the ranking</strong> of automatically retrieved answers. The input ranks are generated by the medical QA system <a href=""https://chiqa.nlm.nih.gov/"" target=""_blank"">CHiQA</a>.</p>

<p>We highly recommend the reuse of RQE and/or NLI systems (first tasks) in the QA task.</p>

<h1 id=""datasets"">Datasets</h1>

<p><strong>Training, validation and test sets are available here:</strong> <a href=""https://github.com/abachaa/MEDIQA2019/tree/master/MEDIQA_Task3_QA"" target=""_blank""> https://github.com/abachaa/MEDIQA2019/tree/master/MEDIQA_Task3_QA </a></p>

<p>In addition, the <a href=""https://github.com/abachaa/MedQuAD"" target=""_blank""> MedQuAD </a> dataset can be used to retrieve answered questions that are entailed from the original questions. [1]</p>

<blockquote>
  <p><em>[1] A. Ben Abacha &amp; D. Demner-Fushman. “A Question-Entailment Approach to Question Answering”. arXiv:1901.08079 [cs.CL], January 2019. <a href=""https://arxiv.org/abs/1901.08079"" target=""_blank""> Link </a></em></p>
</blockquote>

<h1 id=""timeline"">Timeline</h1>

<ul>
  <li>March 19, 2019: Release of the validation set for the QA task.</li>
  <li>April 15, 2019: Release of the test sets for the 3 tasks.</li>
  <li>April 30, 2019: Run submission deadline. Participants’ results will be available on AIcrowd.</li>
  <li>May 15, 2019: Paper submission deadline.</li>
  <li>August 1, 2019: BioNLP workshop, ACL 2019, Florence, Italy.</li>
</ul>

<p>You can download the datasets in the <a href=""https://www.aicrowd.com/challenges/mediqa-2019-question-answering-qa/dataset_files"" target=""_blank"">Resources Section</a>.</p>

<h1 id=""evaluation-criteria"">Evaluation Criteria</h1>

<p>The evaluation of the QA task will be based on the Accuracy, Mean Reciprocal Rank (MRR), Precision, and Spearman’s Rank Correlation Coefficient.</p>

<h1 id=""submission-format"">Submission format</h1>

<p>1) Each line should have the following format: QuestionID,AnswerID,Label.</p>

<p>Label = 0 (incorrect answer) 
  Label = 1 (correct answer)</p>

<p>2) The line number should correspond to the rank of the answer.
Incorrect answers (label values of 0) will be used to compute accuracy. 
For rank-based measures, incorrect answers will be filtered out automatically by our evaluation script.</p>

<p>– No header in the submission file.</p>

<h1 id=""example"">Example</h1>

<p>Test question Q1 with 5 answers: A11, A12, A13, A14 and A15 (systemRanks)</p>

<p>A submission file with 3 correct answers ranked: A13, A11, A15 and 2 incorrect answers: A12 and A15, should look like:</p>

<ul>
  <li>Q1,A13,1</li>
  <li>Q1,A11,1</li>
  <li>Q1,A15,1</li>
  <li>Q1,A12,0</li>
  <li>Q1,A15,0</li>
</ul>

<h1 id=""rules"">Rules</h1>

<p>1) Each team is allowed to submit a maximum of 5 <strong>runs</strong>.</p>

<p>2) Please choose a <strong>username</strong> that represents your team, and <strong>update your profile</strong> with the following information: First name, Last nam, Affiliation, Address, City, Country.</p>

<p>3) For each run submission, it is mandatory to fill in the <strong>submission description</strong> field of the submission form with a short description of the methods, tools and resources used for that run.</p>

<p>4) The final results will not be considered official until a <strong>working notes paper</strong> with the full description of the methods is submitted.</p>

<h1 id=""contact-us"">Contact us</h1>

<ul>
  <li>We strongly encourage you to use our <strong>mailing list</strong> for communications between the participants and the organizers: <a href=""https://groups.google.com/d/forum/bionlp-mediqa"" target=""_blank""> https://groups.google.com/d/forum/bionlp-mediqa </a></li>
  <li>In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at: <a href=""asma.benabacha@nih.gov"" target=""_blank""> asma.benabacha@nih.gov </a></li>
</ul>
"
158,"<hr />
<p><strong>Important note:</strong></p>

<p><em>The <strong>ImageCLEF Lifelog - Lifelog Moment Retrieval (LMRT) challenge has officially ended</strong> and we would like to thank everybody for their participation. You can find the official results at <a href=""http://imageclef.org/2018/lifelog"">http://imageclef.org/2018/lifelog</a>.</em></p>

<p><em>Post-challenge submissions and the leaderboard will remain enabled  for a few weeks so you will still be able to submit result files and have them continuously evaluated during a limited period. 
Please consider that in order to see the version of the leaderboard with the post-challenge submissions integrated, you have to turn on the switch <strong>Show post-challenge submission</strong> right below the leaderboard.</em></p>

<p><em>At the same time we’d like to encourage you to submit a <a href=""http://clef2018.clef-initiative.eu/index.php?page=Pages/InstructionsforCLEF2018WorkingNotes.html"">CLEF Working notes paper</a> until the end of May.</em></p>

<p><em>Please also note that participants registering from now on will not be
automatically registered with CLEF anymore.</em></p>

<hr />

<p><em>Note: ImageCLEF Lifelog 2018 is divided into 2 subtasks (challenges). This challenge is about Lifelog moment retrieval (<strong>LMRT</strong>). For information on the Activities of Daily Living understanding (<strong>ADLT</strong>) challenge click <a href=""/challenges/imageclef-2018-lifelog-adlt"" target=""_blank""> here </a>. Both challenges share the same dataset, so registering for one of these challenges will automatically give you access to the other one.</em></p>

<p><em>Note: Do not forget to read the <strong>Rules</strong> section on this page</em></p>

<h3 id=""motivation"">Motivation</h3>

<p>An increasingly wide range of personal devices, such as smartphones, video cameras as well as wearable devices that allow capturing pictures, videos, and audio clips in every moment of our life are becoming available. Considering the huge volume of data created, commonly referred to as lifelogs, there is a need for systems that can automatically analyse the data in order to categorize, summarize and also query to retrieve the information the user may need.</p>

<p>Despite the increasing number of successful related workshops and panels ( <a href=""http://www.jcdl.org/archived-conf-sites/jcdl2015/www.jcdl2015.org/panels.html"" target=""_blank""> JCDL 2015  </a>, <a href=""http://irlld2016.computing.dcu.ie/index.html"" target=""_blank""> iConf 2016 </a> , <a href=""http://lta2016.computing.dcu.ie/styled/index.html"" target=""_blank""> ACM MM 2016 </a> ,  <a href=""http://lta2017.computing.dcu.ie/"" target=""_blank""> ACM MM 2017 </a> ) lifelogging has seldom been the subject of a rigorous comparative benchmarking exercise as, for example, the new lifelog evaluation task at <a href=""http://ntcir-lifelog.computing.dcu.ie/"" target=""_blank""> NTCIR-13 </a> or the last year edition of the <a href=""http://www.imageclef.org/2017/lifelog"" target=""_blank""> ImageCLEFlifelog </a> task. In this edition of this task we aim to bring the attention of lifelogging to an as wide as possible audience and to promote research into some of the key challenges of the coming years.</p>

<h3 id=""challenge-description"">Challenge description</h3>

<p>The participants have to retrieve a number of specific moments in a lifelogger’s life. We define moments as semantic events, or activities that happened throughout the day. For example, they should return the relevant moments for the query “Find the moment(s) when I was shopping for wine in the supermarket.” Particular attention should be paid to the diversification of the selected moments with respect to the target scenario.</p>

<p>The ground truth for this subtask was created using manual annotation.</p>

<h3 id=""data"">Data</h3>

<p>The task will be split into two related subtasks using a completely new multimodal dataset which consists of 50 days of data from a lifelogger, namely: images (1,500-2,500 per day from wearable cameras), visual concepts (automatically extracted visual concepts with varying rates of accuracy), semantic content (semantic locations, semantic activities) based on sensor readings (via the Moves App) on mobile devices, biometrics information (heart rate, galvanic skin response, calorie burn, steps, etc.), music listening history. The dataset is built based on the data available for the <a href=""http://ntcir-lifelog.computing.dcu.ie/"" target=""_blank""> NTCIR-13 - Lifelog 2 task </a>.</p>

<h3 id=""format-of-the-metadata"">Format of the metadata</h3>
<p>The metadata is stored in an .xml file, which is a simple aggregation of all users data. It is structured as follows:</p>

<p>The root node of the data is the USERS tag. Each user element contains all the data of that user (u1 or u2). Each user has a tag USER that contains the user ID as an attribute, example: [user id=”u1”]. For this year, only user u1 is considered. Inside the USER element, is his/her data:</p>

<p>Following that there is a tag DAYS, this tag contains the lifelogging information of that user organised per day, each day is included in a tag DAY that has the data (a tag DATA), the relative path to the directory that contains the images captured in that particular day (the tag IMAGES-DIRECTORY), then the minutes of of that day under a root tag called MINUTES.</p>

<p>At the start of each day there is a set of daily metatdata for that user. This data is of three forms; BIOMETRICS, ACTIVITIES &amp; PERSONAL LOGS. The biometrics contains WEIGHT, FAT MASS, HEART RATE, SYSTOLIC blood pressure &amp; DIASTOLIC blood pressure, which were readings taken after waking up each day. The activities contains summary activities: STEPS taken that day, DISTANCE walked in metres that day &amp; ELEVATION climbed in metres that day. The personal logs contain HEALTH LOGS, including the TIME of reading, GLU Glucose levels in the blood, BP Blood Pressure, HR Heart Rate, MOOD manually logged every morning and sometimes a COMMENT, as well as DRINK LOGS and FOOD LOGS which were manually logged throughout the dat.</p>

<p>Following that, the day’s data is organised into minutes. The MINUTES element, contains exactly 1440 child elements (called MINUTE), each child has an ID (example: [minute id=“0”], [minute id=“1”], [minute id=“2”]… etc), and it represent one minute in the day ordered from 0 = 12:00 AM, to 1439 = 23:59PM.</p>

<p>Each minute contains: 0 or 1 location information (LOCATION tag), 0 or one activity information (ACTIVITY tag), biometrics, 0 or more captured images (IMAGES tag with IMAGE child element (each element has has a relative path to the image and a unique image ID), and 0 or 1 MUSIC tag giving details of the music listened to at that point in time.</p>

<p>-The location information is captured by Moves app (https://www.moves-app.com/), and they represent to semantic locations (Home, Work, DCU Computing building, GYM, Name of a Store, etc…), or to landmark locations registered by Moves. This tag can contain information in several languages. For locations that are not (HOME) or (WORK), the GPS locations are provided.</p>

<h3 id=""submission-instructions"">Submission instructions</h3>

<hr />
<p><em>As soon as the submission is open, you will find a “Create Submission” button on this page (just next to the tabs)</em></p>

<hr />

<p>A submitted run for the LST sub-task must be in the form of a CSV file in the following format:</p>

<p>[topic id, image id, confidence score]</p>

<p>Where:
- topic id: Number of the queried topic, e.g., from 1 to 5 for the development set.
- image id: ID of a relevant image.
- confidence score: from 0 to 1.</p>

<p>The CSV file should contain a diversified summarization in 50 images for each query.</p>

<p>Sample:</p>

<p>1, u1_2016-08-26_095916_1, 1.00</p>

<p>1, u1_2016-08-26_095950_2, 0.95</p>

<p>1, u1_2016-08-26_100028_1, 0.92</p>

<p>…</p>

<p>10, u1_2016-09-01_145314_1, 1.00</p>

<p>10, u1_2016-09-01_145345_2, 0.89</p>

<p>10, u1_2016-09-01_145531_1, 0.86</p>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>
<p>### Metrics</p>

<p>For assessing performance, classic metrics will be deployed. These metrics are:</p>

<ul>
  <li>
    <p>Cluster Recall at X (CR@X) - a metric that assesses how many different clusters from the ground truth are represented among the top X results;</p>
  </li>
  <li>
    <p>Precision at X (P@X) - measures the number of relevant photos among the top X results;</p>
  </li>
  <li>
    <p>F1-measure at X (F1@X) - the harmonic mean of the previous two.</p>
  </li>
</ul>

<p>Various cut off points are to be considered, e.g., X=5, 10, 20, 30, 40, 50. Official ranking metrics this year will be the <strong>F1-measure@10</strong>, which gives equal importance to diversity (via CR@10) and relevance (via P@10).</p>

<p>Participants are allowed to undertake the sub-tasks in an interactive or automatic manner. For interactive submissions, a maximum of five minutes of search time is allowed per topic. In particular, the organizers would like to emphasize methods that allow interaction with real users (via Relevance Feedback (RF), for example), i.e., beside of the best performance, the way of interaction (like number of iterations using RF), or innovation level of the method (for example, new way to interact with real users) are encouraged.</p>

<h3 id=""ground-truth-format"">Ground truth format</h3>
<p>Ground truth is provided in two individual txt files: one file for the cluster ground truth and one file for the relevant image ground truth.</p>

<p>In the cluster ground-truth file each line corresponds to a cluster where the first value is the topic id, followed by cluster id number, followed by the cluster user tag separated by comma. Lines are separated by an end-of-line character (carriage return). An example is presented below:</p>

<p>1, 1, Badger &amp; Dodo Cafe</p>

<p>1, 2, Costa coffee</p>

<p>…</p>

<p>2, 1, Airport Restaurant</p>

<p>2, 2, Arnotts Department Store</p>

<p>…</p>

<p>In the relevant ground-truth file the first value on each line is the topic id, followed by a unique photo id, and then followed by the cluster id number (that corresponds to the values in the cluster ground-truth file) separated by comma. Each line corresponds to the ground truth of one image and lines are separated by an end-of-line character (carriage return). An example is presented below:</p>

<p>1, u1_2016-09-17_124915_1, 1</p>

<p>1, u1_2016-09-17_125300_1, 1</p>

<p>1, u1_2016-09-17_125332_2, 1</p>

<p>1, u1_2016-08-27_070424_1, 2</p>

<p>1, u1_2016-08-27_070456_2, 2</p>

<p>1, u1_2016-08-27_070528_1, 2</p>

<p>…</p>

<p>2, u1_2016-08-27_133126_1, 1</p>

<p>2, u1_2016-08-27_133158_2, 1</p>

<p>2, u1_2016-08-27_133230_1, 1</p>

<p>2, u1_2016-08-17_121617_1, 2</p>

<p>2, u1_2016-08-17_121704_1, 2</p>

<p>…</p>

<h3 id=""resources"">Resources</h3>

<h3 id=""recommanded-reading"">Recommanded Reading</h3>

<p>[1] Duc-Tien Dang-Nguyen, Luca Piras, Michael Riegler, Giulia Boato, Liting Zhou, Cathal Gurrin, <a href=""https://pralab.diee.unica.it/sites/default/files/invited_paper_10.pdf"" target=""_blank""> “Overview of ImageCLEFlifelog 2017: Lifelog Retrieval and Summarization” </a>, CLEF2017 Working Notes, Dublin, Ireland, 2017, vol 1866.</p>

<p>[2] Cathal Gurrin, Xavier Giro-i-Nieto, Petia Radeva, Mariella Dimiccoli, Håvard Johansen, Hideo Joho, Vivek K Singh, <a href=""http://dl.acm.org/citation.cfm?id=2980534"" target=""_blank""> “LTA 2016: The First Workshop on Lifelogging Tools and Applications” </a>, ACM Multimedia, Amsterdam, The Netherlands, 2016.</p>

<p>[3] Cathal Gurrin, Xavier Giro-i-Nieto, Petia Radeva, Mariella Dimiccoli, Duc Tien Dang Nguyen, Hideo Joho, <a href=""http://doras.dcu.ie/22031/1/cGurrina.pdf"" target=""_blank""> “LTA 2017: The Second Workshop on Lifelogging Tools and Applications” </a>, ACM Multimedia, Mountain View, CA USA, 2017.</p>

<p>[4] Cathal Gurrin, Hideo Joho, Frank Hopfgartner, Liting Zhou, Rami Albatal, <a href=""http://research.nii.ac.jp/ntcir/workshop/OnlineProceedings12/pdf/ntcir/OVERVIEW/01-NTCIR12-OV-LIFELOG-GurrinC.pdf"" target=""_blank""> “Overview of NTCIR-12 Lifelog Task” </a>, Proceedings of the 12th NTCIR Conference on Evaluation of Information Access Technologies, Tokyo, Japan, 2016.</p>

<p>[5] Duc-Tien Dang-Nguyen, Luca Piras, Giorgio Giacinto, Giulia Boato, Francesco GB De Natale, <a href=""https://dl.acm.org/citation.cfm?id=3103613"" target=""_blank""> “Multimodal Retrieval with Diversification and Relevance Feedback for Tourist Attraction Images” </a>, ACM Transactions on Multimedia Computing, Communications, and Applications, vol 13, n° 4, 2017.</p>

<p>[6] Duc-Tien Dang-Nguyen, Luca Piras, Giorgio Giacinto, Giulia Boato, Francesco GB De Natale, <a href=""http://ieeexplore.ieee.org/document/7177486/?arnumber=7177486&amp;tag=1"" target=""_blank""> “A hybrid approach for retrieving diverse social images of landmarks” </a>, IEEE International Conference on Multimedia and Expo (ICME), Turin, Italy, 2015.</p>

<p>[7] <a href=""http://ceur-ws.org/Vol-1436/"" target=""_blank""> Working notes of the 2015 MediaEval Retrieving Diverse Social Images task </a>, CEUR-WS.org, Vol. 1436, ISSN: 1613-0073.</p>

<p>[8] B. Ionescu, A.L. Gînscă, B. Boteanu, M. Lupu, A. Popescu,H. Müller, <a href=""http://imag.pub.ro/~bionescu/index_files/DivTask_MMSys2016.pdf"" target=""_blank""> “Div150Multi: A Social Image Retrieval Result Diversification Dataset with Multi-topic Queries” </a>, ACM MMSys, Klagenfurt, Austria, 2016.</p>

<h3 id=""helpful-tools-and-resources"">Helpful tools and resources</h3>

<ul>
  <li>
    <p><a href=""http://eyeaware.computing.dcu.ie/"" target=""_blank""> Eyeaware lifelogging framework </a></p>
  </li>
  <li>
    <p><a href=""http://opencv.org/"" target=""_blank""> OpenCV – Open Source Computer Vision </a></p>
  </li>
  <li>
    <p><a href=""http://www.lire-project.net/"" target=""_blank""> LIRE: Lucence Image Retrieval </a></p>
  </li>
  <li>
    <p><a href=""http://trec.nist.gov/trec_eval/index.html"" target=""_blank""> trec_eval scoring software </a></p>
  </li>
  <li>
    <p><a href=""http://www.imageclef.org/"" target=""_blank""> ImageCLEF - Image Retrieval in CLEF </a></p>
  </li>
  <li>
    <p><a href=""http://www.cs.waikato.ac.nz/ml/weka/"" target=""_blank""> Weka Data Mining Software </a></p>
  </li>
  <li>
    <p><a href=""https://developer.nvidia.com/digits"" target=""_blank""> Nvidia DIGITS </a></p>
  </li>
  <li>
    <p><a href=""http://caffe.berkeleyvision.org/"" target=""_blank""> Caffee deep learning framework </a></p>
  </li>
  <li>
    <p><a href=""http://creativecommons.org/"" target=""_blank""> Creative Commons </a></p>
  </li>
</ul>

<h3 id=""contact-us"">Contact us</h3>

<ul>
  <li>Technical issues : <a href=""https://gitter.im/crowdAI/imageclef-2018-lifelog-lmrt"" target=""_blank""> https://gitter.im/crowdAI/imageclef-2018-lifelog-lmrt </a></li>
  <li>Discussion Forum : <a href=""https://www.crowdai.org/challenges/imageclef-2018-lifelog-lmrt/topics"" target=""_blank""> https://www.crowdai.org/challenges/imageclef-2018-lifelog-lmrt/topics </a></li>
</ul>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

<ul>
  <li>Sharada Prasanna Mohanty: sharada.mohanty@epfl.ch</li>
  <li>Duc-Tien Dang-Nguyen: duc-tien[DOT]dang-nguyen[AT]dcu[DOT]ie</li>
  <li>Luca Piras: luca[DOT]piras[AT]diee[DOT]unica[DOT]it</li>
  <li>Ivan Eggel: ivan[DOT]eggel[AT]hevs[DOT]ch</li>
</ul>

<h3 id=""more-information"">More information</h3>

<p>You can find additional information on the challenge here:
<a href=""http://imageclef.org/2018/lifelog"" target=""_blank""> http://imageclef.org/2018/lifelog </a></p>

<h3 id=""prizes"">Prizes</h3>

<p>ImageCLEF 2018 is an evaluation campaign that is being organized as part of the <a href=""http://clef2018.clef-initiative.eu/"" target=""_blank""> CLEF initiative </a> labs. The campaign offers several research tasks that welcome participation from teams around the world. The results of the campaign appear in the working notes proceedings, published by CEUR Workshop Proceedings (CEUR-WS.org). Selected contributions among the participants, will be invited for publication in the following year in the Springer Lecture Notes in Computer Science (LNCS) together with the annual lab overviews.</p>

<h3 id=""datasets-license"">Datasets License</h3>

"
179,"
<p><em>Note: Do not forget to read the <strong>Rules</strong> section on this page</em></p>

<h3 id=""usage-scenario"">Usage scenario</h3>

<p>Automated identification of plants has improved considerably in the last few years. In the scope of LifeCLEF 2017 and 2018 in particular, we measured impressive identification performance over 10K species. However, these 10K species, mostly living in Europe and North America, only represent the tip of the iceberg. The vast majority of the species in the world (~369K species) actually lives in data deficient regions and the performance of state-of-the-art machine learning algorithms on these species is unknown and presumably much lower because of the weak amount of training data. Thus, the main focus of the 2019 edition of PlantCLEF will be to evaluate automated identification on the flora of such data deficient regions.</p>

<h3 id=""challenge-description"">Challenge description</h3>
<p>he goal of the task is return the most likely species for each observation of the test set (an observation being a set of images of the same individual plant and the associated metadata such as date, gps, author). A small part of the observations in the test set will be re-annotated by several experts so as to allow comparing the performance of the evaluated systems with the one of highly skilled experts.</p>

<h3 id=""data"">Data</h3>
<p>We provide a new dataset of 10K species mainly focused on the Guiana shield and the Amazon rainforest (known to be the largest collection of living plants and animal species in the world). The average number of images per species in that new dataset will be much lower than the dataset used in the previous editions of PlantCLEF (about 10 vs. 100). Many species will contain only a few images and some of them might even contain only 1 image.</p>

<p>The training data is now available (see the “Dataset” tab). The test set to be predicted will be delivered around the 1st of March 2019.</p>

<p>Participants are allowed to use complementary training data (e.g. for pre-training purposes) but at the condition that (i) the experiment is entirely re-produceable, i.e. that the used external resource is clearly referenced and accessible to any other research group in the world, (ii) the use of external training data or not is mentioned for each run, and (iii) the additional resource does not contain any of the test observations.</p>

<h3 id=""submission-instructions"">Submission instructions</h3>

<p>More practically, the run file to be submitted has to contain as much lines as the number of predictions, each prediction being composed of an ObservationId (the identifier of a specimen that can be itself composed of several images), a ClassId, a Probability and a Rank (used in case of equal probabilities). Each line should have the following format:
&lt;ObservationId;ClassId;Probability;Rank&gt;</p>

<p>Here is a short fake run example respecting this format for only 3 observations:
<a href=""https://www.imageclef.org/system/files/fakerun.txt"">fake_run</a></p>

<p>As soon as the submission is open, you will find a “Create Submission” button on this page (just next to the tabs).</p>

<h3 id=""comparison-with-plantclef-2018"">Comparison with PlantCLEF 2018</h3>
<p>Before the opening of the PlantCLEF 2019 submissions, we encourage participants to train their system and submit runs to the challenge of last year (dealing with 10K species of data-abundant regions). We therefore re-open a new submission round and leaderboard on the challenge page: 
<a href=""https://www.crowdai.org/challenges/lifeclef-2018-expert"">ExpertCLEF2018</a>. Note that the best performing models of last year have been shared by CVUT at the following URL: <a href=""http://ptak.felk.cvut.cz/personal/sulcmila/models/LifeCLEF2018/"" target=""_blank""> http://ptak.felk.cvut.cz/personal/sulcmila/models/LifeCLEF2018/ </a></p>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>
<p>The two main evaluation metrics will be the top-1 accuracy on 1) the fraction of the test set identified by the pool of experts, 2) on the whole test set.</p>

<h3 id=""resources"">Resources</h3>

<h3 id=""pre-trained-plant-identification-models-from-expertlifeclef-2018"">Pre-trained plant identification models (from ExpertLifeCLEF 2018)</h3>
<p>In order to support research in fine-grained plant classification, CVUT shares the pre-trained Inception-v4 and Inception-ResNet-v2 CNN models from their winning submission to the ExpertLifeCLEF 2018 Plant identification task. The pre-trained models may be a good starting point for the participants to LifeCLEF 2019:
<a href=""http://ptak.felk.cvut.cz/personal/sulcmila/models/LifeCLEF2018/"" target=""_blank""> http://ptak.felk.cvut.cz/personal/sulcmila/models/LifeCLEF2018/ </a></p>

<h3 id=""contact-us"">Contact us</h3>

<ul>
  <li>Discussion Forum : <a href=""https://www.crowdai.org/challenges/lifeclef-2019-plant/topics"" target=""_blank""> https://www.crowdai.org/challenges/lifeclef-2019-plant/topics </a></li>
</ul>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at lifeclef-org[AT]inria[DOT]fr</p>

<h3 id=""more-information"">More information</h3>

<p>You can find additional information on the challenge here:
<a href=""https://www.imageclef.org/PlantCLEF2019"" target=""_blank""> https://www.imageclef.org/PlantCLEF2019 </a></p>

<h3 id=""results-tables-and-figures"">Results (tables and figures)</h3>
<p><a href=""https://www.imageclef.org/PlantCLEF2019"" target=""_blank"">(Official round during the LifeCLEF 2019 campaign)</a></p>

<h3 id=""prizes"">Prizes</h3>

<p>LifeCLEF 2019 is an evaluation campaign that is being organized as part of the <a href=""http://clef2019.clef-initiative.eu/"" target=""_blank""> CLEF initiative </a> labs. The campaign offers several research tasks that welcome participation from teams around the world. The results of the campaign appear in the working notes proceedings, published by CEUR Workshop Proceedings (CEUR-WS.org). Selected contributions among the participants, will be invited for publication in the following year in the Springer Lecture Notes in Computer Science (LNCS) together with the annual lab overviews.</p>

<h3 id=""datasets-license"">Datasets License</h3>

"
6,"<p>The <a href=""https://sites.google.com/view/mediqa2019"" target=""_blank""> MEDIQA </a> challenge is an  <a href=""https://aclweb.org/aclwiki/BioNLP_Workshop"" target=""_blank""> ACL-BioNLP </a> 2019 shared task aiming to attract further research efforts in Natural Language Inference (NLI), Recognizing Question Entailment (RQE), and their applications in medical Question Answering (QA).</p>

<h1 id=""recognizing-question-entailment-task-rqe"">Recognizing Question Entailment Task (RQE)</h1>

<p>The objective of this task is to identify entailment between two questions in the context of QA. We use the following definition of question entailment: “a question A entails a question B if every answer to B is also a complete or partial answer to A” [1]</p>

<blockquote>
  <p><em>[1] A. Ben Abacha &amp; D. Demner-Fushman. “Recognizing Question Entailment for Medical Question Answering”. AMIA 2016. <a href=""https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5333286/"" target=""_blank""> Link </a></em></p>
</blockquote>

<h1 id=""datasets"">Datasets</h1>

<p>Training set: <a href=""https://github.com/abachaa/RQE_Data_AMIA2016/blob/master/RQE_Train_8588_AMIA2016.xml"" target=""_blank""> RQE_Train_8588_AMIA2016.xml </a> (<a href=""https://github.com/abachaa/RQE_Data_AMIA2016"" target=""_blank""> RQE_Data_AMIA2016 </a></p>

<p>You can download the validation and test datasets in the <a href=""https://www.aicrowd.com/challenges/mediqa-2019-recognizing-question-entailment-rqe/dataset_files"" target=""_blank"">Resources Section</a>.</p>

<p>– Please note that the validation set is the same as <a href=""https://github.com/abachaa/RQE_Data_AMIA2016/blob/master/RQE_Test_302_pairs_AMIA2016.xml"" target=""_blank""> AMIA2016_302_pairs_Test_Set  </a>.</p>

<p>– Training, validation and test sets are also available here:
https://github.com/abachaa/MEDIQA2019/tree/master/MEDIQA_Task2_RQE</p>

<h1 id=""timeline"">Timeline</h1>

<ul>
  <li>February 28, 2019: Release of the validation set for the RQE task.</li>
  <li>April 15, 2019: Release of the test sets for the 3 tasks.</li>
  <li>April 30, 2019: Run submission deadline. Participants’ results will be available on AIcrowd.</li>
  <li>May 15, 2019: Paper submission deadline.</li>
  <li>August 1, 2019: BioNLP workshop, ACL 2019, Florence, Italy.</li>
</ul>

<h1 id=""evaluation-criteria"">Evaluation Criteria</h1>

<p>For the RQE and NLI tasks:</p>

<ul>
  <li>The evaluation will be based on <em>Accuracy</em>.</li>
  <li>For the result submission file, we expect a csv file with the following header:<br />
<em>pair_id,label</em> (cf. <a href=""https://www.aicrowd.com/challenges/mediqa-2019-recognizing-question-entailment-rqe/dataset_files"" target=""_blank""> ValSet_Baseline.csv </a>)</li>
</ul>

<h1 id=""rules"">Rules</h1>

<p>1) Each team is allowed to submit a maximum of 5 <strong>runs</strong>.</p>

<p>2) Please choose a <strong>username</strong> that represents your team, and <strong>update your profile</strong> with the following information: First name, Last nam, Affiliation, Address, City, Country.</p>

<p>3) For each run submission, it is mandatory to fill in the <strong>submission description</strong> field of the submission form with a short description of the methods, tools and resources used for that run.</p>

<p>4) The final results will not be considered official until a <strong>working notes paper</strong> with the full description of the methods is submitted.</p>

<h1 id=""contact-us"">Contact us</h1>

<ul>
  <li>We strongly encourage you to use our <strong>mailing list</strong> for communications between the participants and the organizers: <a href=""https://groups.google.com/d/forum/bionlp-mediqa"" target=""_blank""> https://groups.google.com/d/forum/bionlp-mediqa </a></li>
  <li>In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at: <a href=""asma.benabacha@nih.gov"" target=""_blank""> asma.benabacha@nih.gov </a></li>
</ul>
"
159,"<hr />
<p><strong>Important note:</strong></p>

<p><em>The <strong>ImageCLEF Tuberculosis - MDR Detection challenge has officially ended</strong> and we would like to thank everybody for their participation. You can find the official results at <a href=""http://imageclef.org/2018/tuberculosis"">http://imageclef.org/2018/tuberculosis </a>.</em></p>

<p><em>Post-challenge submissions and the leaderboard will remain enabled  for a few weeks so you will still be able to submit result files and have them continuously evaluated during a limited period. 
Please consider that in order to see the version of the leaderboard with the post-challenge submissions integrated, you have to turn on the switch <strong>Show post-challenge submission</strong> right below the leaderboard.</em></p>

<p><em>At the same time we’d like to encourage you to submit a <a href=""http://clef2018.clef-initiative.eu/index.php?page=Pages/InstructionsforCLEF2018WorkingNotes.html"">CLEF Working notes paper</a> until the end of May.</em></p>

<p><em>Please also note that participants registering from now on will not be
automatically registered with CLEF anymore.</em></p>

<hr />

<p><em>Note: ImageCLEF Tuberculosis 2018 is divided into 3 subtasks (challenges). This challenge is about <strong>MDR (multi-drug-resistance) Detection</strong>. For information on the <strong>TBT (tuberculosis type) Classfication</strong> challenge click <a href=""/challenges/imageclef-2018-tuberculosis-tbt-classification"" target=""_blank""> here </a>. For information on the <strong>Severity Scoring</strong> challenge click <a href=""/challenges/imageclef-2018-tuberculosis-severity-scoring"" target=""_blank""> here </a>. All of these challenges share the same dataset, so registering for one of these challenges will automatically give you access to the other ones.</em></p>

<p><em>Note: Do not forget to read the <strong>Rules</strong> section on this page</em></p>

<h3 id=""motivation"">Motivation</h3>

<p>About 130 years after the discovery of Mycobacterium tuberculosis, the disease remains a persistent threat and a leading cause of death worldwide.</p>

<p>The greatest disaster that can happen to a patient with tuberculosis (TB) is that the organisms become resistant to two or more of the standard drugs. In contrast to drug sensitive (DS) tuberculosis, its multi-drug resistant (MDR) form is much more difficult and expensive to recover from. Thus, early detection of the drug resistance (DR) status is of great importance for effective treatment. The most commonly used methods of DR detection are either expensive or take too much time (up to several month). Therefore there is a need for quick and at the same time cheap methods of DR detection. One of the possible approaches for this task is based on Computed Tomography (CT) image analysis. Another challenging task is automatic detection of TB types (TBT) using CT volumes.</p>

<p><em>Differences compared to 2017</em>: Scoring the severity of TB cases based on chest CT images is another task compared to both tuberculosis-related subtasks considered in 2017. There are no direct links between them. Note only that original CT image datasets used in 2017 and in 2018 may slightly overlap.</p>

<h3 id=""challenge-description"">Challenge description</h3>

<p>The goal of this challenge is to assess the probability of a TB patient having resistant form of tuberculosis based on the analysis of chest CT scan.
More information will follow soon.</p>

<h3 id=""data"">Data</h3>

<p>For this task, a dataset of 3D CT images is used along with a set of clinically relevant metadata. The dataset includes only HIV-negative patients with no relapses and having one of the two forms of tuberculosis: drug sensitive (DS) or multi-drug resistant (MDR). The MDR class includes patients with extensively drug-resistant (XDR) tuberculosis.</p>

<table>
  <thead>
    <tr>
      <th>Num. Patients</th>
      <th style=""text-align: center"">Train</th>
      <th style=""text-align: center"">Test</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>DS</td>
      <td style=""text-align: center"">134</td>
      <td style=""text-align: center"">99</td>
    </tr>
    <tr>
      <td>MDR</td>
      <td style=""text-align: center"">125</td>
      <td style=""text-align: center"">137</td>
    </tr>
    <tr>
      <td><strong>Total patients</strong></td>
      <td style=""text-align: center""><strong>259</strong></td>
      <td style=""text-align: center""><strong>236</strong></td>
    </tr>
  </tbody>
</table>

<p>We provide 3D CT images with slice size of 512*512 pixels and number of slices varying from about 50 to 400. All the CT images are stored in NIFTI file format with .nii.gz file extension (g-zipped .nii files). This file format stores raw voxel intensities in Hounsfield units (HU) as well the corresponding image metadata such as image dimensions, voxel size in physical units, slice thickness, etc. A freely-available tool called <a href=""https://www.creatis.insa-lyon.fr/rio/vv"" target=""_blank""> “VV” </a> can be used for viewing image files. Currently, there are various tools available for reading and writing NIFTI files. Among them there are <a href=""https://de.mathworks.com/matlabcentral/fileexchange/8797-tools-for-nifti-and-analyze-image/content/load_nii.m"" target=""_blank""> load_nii </a> and <a href=""https://de.mathworks.com/matlabcentral/fileexchange/8797-tools-for-nifti-and-analyze-image/content/save_nii.m"" target=""_blank""> save_nii </a> functions for Matlab and <a href=""http://niftilib.sourceforge.net/"" target=""_blank""> Niftilib </a> library for C, Java, Matlab and Python.</p>

<p>We also provide automatic extracted masks of the lungs. This material can be downloaded together with the patients CT images. The details of this segmentation can be found <a href=""http://publications.hevs.ch/index.php/publications/show/1871"" target=""_blank""> here </a>.
In case the participants use these masks in their experiments, please refer to the section “Citations” to find the appropriate citation for this lung segmentation technique.</p>

<h3 id=""submission-instructions"">Submission instructions</h3>

<hr />
<p><em>As soon as the submission is open, you will find a “Create Submission” button on this page (just next to the tabs)</em></p>

<hr />

<p>Submit a plain text file named with the prefix <strong>MDR</strong> (e.g. MDRfree-text.txt) with the following format:</p>

<p>&lt;Patient-ID&gt;,&lt;Probability of MDR&gt;</p>

<p>e.g.:</p>

<div class=""highlighter-rouge""><div class=""highlight""><pre class=""highlight""><code>MDR_TST_001,0.1
MDR_TST_002,1
MDR_TST_003,0.56
MDR_TST_004,0.02
</code></pre></div></div>

<p><strong>Please use a score between 0 and 1 to indicate the probability of the patient having MDR.</strong></p>

<p><strong>You need to respect the following constraints:</strong></p>

<ul>
  <li>Patient-IDs must be part of the predefined Patient-IDs</li>
  <li>All patient-IDs must be present in the runfiles</li>
  <li>Only use numbers between 0 and 1 for the score. Use the dot (.) as a decimal point (no commas accepted)</li>
</ul>

<h3 id=""citations"">Citations</h3>

<p>Information will be posted after the challenge ends.</p>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>
<p>The results will be evaluated using ROC-curves produced from the probabilities provided by participants.</p>

<p>The leaderboard will be visible from the 01.05.2018. However, the submission system will remain open few more days.</p>

<h3 id=""resources"">Resources</h3>

<h3 id=""contact-us"">Contact us</h3>

<ul>
  <li>Technical issues : <a href=""https://gitter.im/crowdAI/imageclef-2018-tuberculosis-mdr-detection"" target=""_blank""> https://gitter.im/crowdAI/imageclef-2018-tuberculosis-mdr-detection </a></li>
  <li>Discussion Forum : <a href=""https://www.crowdai.org/challenges/imageclef-2018-tuberculosis-mdr-detection/topics"" target=""_blank""> https://www.crowdai.org/challenges/imageclef-2018-tuberculosis-mdr-detection/topics </a></li>
</ul>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

<ul>
  <li>Sharada Prasanna Mohanty: sharada.mohanty@epfl.ch</li>
  <li>Yashin Dicente Cid: yashin[DOT]dicente[AT]hevs[DOT]ch</li>
  <li>Henning Müller: henning[DOT]mueller[AT]hevs[DOT]ch</li>
  <li>Ivan Eggel: ivan[DOT]eggel[AT]hevs[DOT]ch</li>
</ul>

<h3 id=""more-information"">More information</h3>

<p>You can find additional information on the challenge here:
<a href=""http://imageclef.org/2018/tuberculosis""> http://imageclef.org/2018/tuberculosis </a></p>

<h3 id=""prizes"">Prizes</h3>

<p>ImageCLEF 2018 is an evaluation campaign that is being organized as part of the <a href=""http://clef2018.clef-initiative.eu/"" target=""_blank""> CLEF initiative </a> labs. The campaign offers several research tasks that welcome participation from teams around the world. The results of the campaign appear in the working notes proceedings, published by CEUR Workshop Proceedings (CEUR-WS.org). Selected contributions among the participants, will be invited for publication in the following year in the Springer Lecture Notes in Computer Science (LNCS) together with the annual lab overviews.</p>

<h3 id=""datasets-license"">Datasets License</h3>

"
181,"<p><em>Note: Do not forget to read the <strong>Rules</strong> section on this page</em></p>

<p><strong>The test data is available. Participants can now submit their runs.</strong></p>

<h3 id=""challenge-description"">Challenge description</h3>

<p>The goal of the challenge is to detect and classify all audible bird vocalizations within the provided soundscape recordings. Each soundscape is divided into segments of 5 seconds. Participants should submit a list of species associated with probability scores for each segment.</p>

<h3 id=""data"">Data</h3>

<p>The <strong>training data</strong> contains ~50,000 recordings taken from <a href=""https://www.xeno-canto.org"" target=""_blank"">xeno-canto.org</a> and covers 659 common species from North and South America. We limited the available amount of recordings to a minimum of 15 and a maximum of 100 audio files per species (each file contains a variable number of calls). All recordings vary in quality, sampling rate and encoding. The training data also includes extensive metadata (JSON files) for each recording, providing information on recording location, type of vocalization, presence/absence of background species, etc.</p>

<p>The <strong>test data</strong> contains soundscape recordings from two regions: (i) ~280h of data recorded in Ithaca, NY, United States and (ii) ~4h of data recorded in Bolívar, Colombia. Each test recording contains multiple species and overlapping vocalizations. Some recordings (especially nighttime recordings) might not contain any bird vocalization.</p>

<p>The <strong>validation data</strong> will be provided as a complementary resource in addition to the training data and can be used to locally validate the system performance before the test data is released. The validation data includes (i) ~72h of data recorded in Ithaca, NY, United States and (ii) ~25min of data recorded in Bolívar, Colombia. The selection of the validation recordings can be considered representative of the test data and features metadata that provides detailed information on the recording settings. However, not every species from the test data might be present in the validation data.</p>

<p>The test and validation data includes a total of ~80,000 expert annotations provided by the <a href=""https://www.birds.cornell.edu/home/"" target=""_blank""> Cornell Lab of Ornithology </a> and  <a href=""https://www.researchgate.net/profile/Paula_Caycedo"" target=""_blank"">Paula Caycedo</a>.</p>

<h3 id=""submission-instructions"">Submission instructions</h3>

<p>Valid submissions have to include all test recordings and the corresponding predictions independent of the recording location. Each line should contain one prediction item. Each prediction item has to follow this format:</p>

<p><em>&lt; MediaId;TC1-TC2;ClassId;probability&gt;</em></p>

<p><strong>MediaId:</strong> ID of each test recording; can be found in XML metadata.</p>

<p><strong>TC1-TC2:</strong> Timecode interval with the format of hh:mm:ss and a length of exactly 5 seconds starting with zero at the beginning of each file (e.g.: 00:00:00-00:00:05, then 00:00:05-00:00:10).</p>

<p><strong>ClassId:</strong> ID of each species class; can be found in metadata and subfolder names.</p>

<p><strong>Probability:</strong> Between 0 and 1; decreasing with the confidence in the prediction.</p>

<p>A valid submission item would look like this:</p>

<p><em>SSW003;00:13:25-00:13:30;cedwax;0.87345</em></p>

<p>You can find a sample run as part of the validation data.</p>

<p>Each participating group is allowed to submit up to 10 runs obtained from different methods. Semi-supervised, interactive or crowdsourced approaches are allowed, but will be compared independently from fully automatic methods. Any human assistance in the processing of the test queries needs to be clearly indicated in the submitted runs.</p>

<p>Participants are allowed to use any of the provided complementary metadata in addition to the audio data. Participants are also allowed to use additional external training data, but at the conditions that</p>

<p>(i) the experiment is entirely re-producible, i.e. that the used external resource is clearly referenced and accessible to any other research group in the world and</p>

<p>(ii) participants submit at least one run without external training data so that we can study the contribution of these additional resources.</p>

<p>Participants are also allowed to use the provided validation data for training under the conditions that</p>

<p>(i) participants submit at least one run that only uses training data and</p>

<p>(ii) clearly state when validation data was used for training in any of the additional runs.</p>

<hr />
<p><em>As soon as the submission is open, you will find a “Create Submission” button on this page (next to the tabs)</em></p>

<hr />

<h3 id=""citations"">Citations</h3>

<p>Information will be posted after the challenge ends.</p>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>
<p>The used metric will be the classification mean Average Precision (c-mAP), considering each class c of the ground truth as a query. This means that for each class c, we will extract from the run file all predictions with ClassId=c, rank them by decreasing probability and compute the average precision for that class. We will then take the mean across all classes. More formally:</p>

<p><img src=""https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/dff548c8fe3b97e373cc344242d125eb_cmap.png"" alt=""cmap.png"" /></p>

<p>where C is the number of species in the ground truth and AveP(c) is the average precision for a given species c computed as:</p>

<p><img src=""https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/59cd979a30b172d90dc6cd307a63ce3d_AveP.png"" alt=""AveP.png"" /></p>

<p>where k is the rank of an item in the list of the predicted segments containing c, n is the total number of predicted segments containing c, P(k) is the precision at cut-off k in the list, rel(k) is an indicator function equaling 1 if the segment at rank k is a relevant one (i.e. is labeled as containing c in the ground truth) and nrel is the total number of relevant segments for c.</p>

<h3 id=""resources"">Resources</h3>

<h3 id=""contact-us"">Contact us</h3>

<ul>
  <li>Discussion Forum : <a href=""https://www.crowdai.org/challenges/lifeclef-2019-bird-soundscape/topics"" target=""_blank""> https://www.crowdai.org/challenges/lifeclef-2019-bird-soundscape/topics </a></li>
</ul>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

<ul>
  <li>Stefan Kahl: stefan[DOT]kahl[AT]informatik[DOT]tu-chemnitz[DOT]de</li>
  <li>Alexis Joly: alexis[DOT]joly[AT]inria[DOT]fr</li>
  <li>Hervé Goëau: herve[DOT]goeau[AT]cirad[DOT]fr</li>
</ul>

<h3 id=""more-information"">More information</h3>

<p>You can find additional information on the challenge here:
<a href=""https://www.imageclef.org/BirdCLEF2019"" target=""_blank""> https://www.imageclef.org/BirdCLEF2019 </a></p>

<h3 id=""prizes"">Prizes</h3>

<p>LifeCLEF 2019 is an evaluation campaign that is being organized as part of the <a href=""http://clef2019.clef-initiative.eu/"" target=""_blank""> CLEF initiative </a> labs. The campaign offers several research tasks that welcome participation from teams around the world. The results of the campaign appear in the working notes proceedings, published by CEUR Workshop Proceedings (CEUR-WS.org). Selected contributions among the participants, will be invited for publication in the following year in the Springer Lecture Notes in Computer Science (LNCS) together with the annual lab overviews.</p>

<h3 id=""datasets-license"">Datasets License</h3>

"
204,"<blockquote>
  <p>Update 08 Jan 2019: The submission system is now live on <a href=""https://easychair.org/conferences/?conf=wsdmcup2019ssspc"" target=""_blank""> EasyChair </a>. We hope to see many of you submit reports on your work for this challenge.</p>
</blockquote>

<blockquote>
  <p>Update 07 Jan 2019: The final results are now available. We will be contacting the teams to confirm that their code is open sourced and can be verified, but a provisional congratulations to the winning teams, and thank you all for participating in this challenge. We look forward to reading and hearing about your insights.</p>
</blockquote>

<blockquote>
  <p>Update 04 Jan 2019: Good luck to all contestants in the final hours of the challenge! Once the submission period has concluded we will begin the final leaderboard evaluations. Additionally, we are in the process of finalizing the paper submission system for the WSDM Cup workshop, the deadline will be January 11, 2019. In the meantime, see the Call for Papers section here on the challenge overview page for information. Please note that submitting a paper is mandatory in order to be considered for the winning leaderboard positions, and in order to be eligible for the prizes.</p>
</blockquote>

<blockquote>
  <p>Update 12 Dec 2018: We would like to make several announcements: (1) We are happy to share that Google have kindly offered to sponsor coupons for google cloud compute resources for participants of this challenge. Please see the ‘Google Sponsored Computational Resources’ section of the overview page for further details. (2) We have released the call for papers for the WSDM Cup Workshop day. Please see the ‘Rules’ and ‘Call for Papers’ sections of the overview page for further details. (3) We are now providing the training set split into 10 files to make it easier for participants with slow connections to download the training set. Please see the Training_Set_Split_Download.txt file under the Dataset tab for the download links. (4) There was some ambiguity in the description of the challenge metric which has now been clarified, see the ‘Evaluation’ section of the overview page for further details. Please note that the metric is unchanged we have simply clarified the terminology.</p>
</blockquote>

<blockquote>
  <p>Update 20 Nov 2018: Unfortunately we have had to make some changes to the challenge dataset. More specifically, we have had to remove some features from the track features table (the updated Dataset Description file outlines the new track features schema). Please note that the other parts of the dataset all remain unchanged, except the track features table in the mini version of the dataset which was changed correspondingly. We apologize for any inconvenience caused by this change. If your work on the challenge is affected, we would appreciate if you email us at <a href=""wsdm-cup-2019@spotify.com"" target=""_blank""> wsdm-cup-2019@spotify.com </a> so that we can better understand any potential impact on participants.</p>
</blockquote>

<p>Spotify is an online music streaming service with over 190 million active users interacting with a library of over 40 million tracks. A central challenge for Spotify is to recommend the right music to each user. While there is a large related body of work on recommender systems, there is very little work, or data, describing how users sequentially interact with the streamed content they are presented with. In particular within music, the question of if, and when, a user skips a track is an important implicit feedback signal.</p>

<p>We release this dataset and challenge in the hope of spurring research on this important and understudied problem in streaming. Our challenge focuses on the task of session-based sequential skip prediction,  i.e. predicting whether users will skip tracks, given their immediately preceding interactions in their listening session.</p>

<p>The organization of this challenge is a joint effort of <a href=""https://www.spotify.com"" target=""_blank""> Spotify </a>, <a href=""http://www.wsdm-conference.org/2019/"" target=""_blank""> WSDM </a>, and <a href=""https://www.crowdai.org/"" target=""_blank""> CrowdAI </a>.</p>

<h2 id=""dataset"">Dataset</h2>

<p>The public part of the dataset consists of roughly 130 million listening sessions with associated user interactions on the Spotify service. In addition to the public part of the dataset, approximately 30 million listening sessions are used for the challenge leaderboard. For these leaderboard sessions the participant is provided all the user interaction features for the first half of the session, but only the track id’s for the second half. In total, users interacted with almost 4 million tracks during these sessions, and the dataset includes acoustic features and metadata for all of these tracks.</p>

<p>If you use this dataset in an academic publication, please cite the following paper:</p>

<p>@inproceedings{brost2019music,
  title={The Music Streaming Sessions Dataset},
  author={Brost, Brian and Mehrotra, Rishabh and Jehan, Tristan},
  booktitle={Proceedings of the 2019 Web Conference},
  year={2019},
  organization={ACM}
}</p>

<h2 id=""challenge"">Challenge</h2>

<p>The task is to predict whether individual tracks encountered in a listening session will be skipped by a particular user. In order to do this, complete information about the first half of a user’s listening session is provided, while the prediction is to be carried out on the second half. Participants have access to metadata, as well as acoustic descriptors, for all the tracks encountered in listening sessions.</p>

<p>The output of a prediction is a binary variable for each track in the second half of the session indicating if it was skipped or not, with a 1 indicating that the track skipped, and a 0 indicating that the track was not skipped. For this challenge we use the skip_2 field of the session logs as our ground truth.</p>

<p>There will be a workshop at WSDM where selected or top performing teams will be invited to present their work on this challenge. The paper submission deadline will be January 11, 2019, and the workshop will be held on February 15, 2019, as part of WSDM in Melbourne, Australia</p>

<h2 id=""how-to-generate-submissions"">How to generate submissions</h2>

<p>The test set sessions are always split between two files. Each session is partly contained in a prehistory file, and a corresponding input file. The full interaction feature set  for the first half of the session is contained in the prehistory file, and the track id’s for which you need to make a prediction are contained in the input file. For each test set session a row of 1’s and 0’s of the same length as the input part of the session must then be generated. Sample submissions are contained in the Sample_Submissions.tar.gz file under the Dataset tab, and code for generating a random submission is contained in the <a href=""https://github.com/crowdAI/skip-prediction-challenge-starter-kit"" target=""_blank""> Starter Kit </a>.</p>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>
<p>Accurate skip prediction can enable us to avoid recommending a potential track to the user, based on the user’s immediately preceding interactions. At a given moment in time, it is therefore most important to predict if the next immediate track is going to be skipped, but it would also be useful to predict if the tracks further into the session will be skipped. This motivates our use of Mean Average Accuracy as the primary metric for the challenge, with the average accuracy defined by</p>

<script type=""math/tex; mode=display"">\begin{align*}
AA = \frac{\sum_{i=1}^T{A(i)L(i)}}{T}
\end{align*}</script>

<p>where :</p>

<ul>
  <li><script type=""math/tex"">T</script> is the number of tracks to be predicted for the given session</li>
  <li><script type=""math/tex"">A(i)</script> is the accuracy at position <script type=""math/tex"">i</script> of the sequence</li>
  <li><script type=""math/tex"">L(i)</script> is the boolean indicator for if the <script type=""math/tex"">i</script>‘th prediction was correct.</li>
</ul>

<p>We will use the accuracy at predicting the first interaction in the second half of the session as a tie breaking secondary metric.</p>

<h3 id=""resources"">Resources</h3>

<p>A starter kit for participants to familiarize themselves with the dataset and challenge mechanics is provided at: <a href=""https://github.com/crowdAI/skip-prediction-challenge-starter-kit"" target=""_blank""> Starter Kit </a></p>

<p>Information about the Spotify API is provided at: <a href=""https://developer.spotify.com/documentation/web-api/"" target=""_blank""> Spotify API </a></p>

<p>For an introduction to some of the factors that affect user skip behaviour, see the following blog entry from Paul Lamere: <a href=""https://musicmachinery.com/2014/05/02/the-skip/"" target=""_blank""> MusicMachinery - Entry on skips </a></p>

<h2 id=""google-sponsored-computational-resources"">Google Sponsored Computational Resources</h2>

<p>We are very grateful to Google, who have kindly offered to sponsor 100 USD coupons for Google cloud compute resources for participants of this challenge. Teams that have made a valid submission are invited to send an email to <a href=""wsdm-cup-2019@spotify.com"" target=""_blank""> wsdm-cup-2019@spotify.com </a> to request a coupon. This email should have the title ‘Coupon’ and should provide the team name, and should be sent from the email associated with the account which made the valid submission. Every week a team makes an improved submission on the leaderboard, they will be eligible to request a further 100 USD coupon, for as long as coupons remain. Thus, if a team has already received a coupon, but makes an improved submission in the subsequent week starting Monday, they will be eligible for another request.</p>

<h2 id=""contact-us"">Contact Us</h2>

<p>Use one of the public channels:</p>

<ul>
  <li><strong>Gitter Channel</strong> : <a href=""https://gitter.im/crowdAI/spotify-sequential-skip-prediction-challenge"" target=""_blank""> https://gitter.im/crowdAI/spotify-sequential-skip-prediction-challenge </a></li>
  <li><strong>Technical issues</strong> : <a href=""https://github.com/crowdAI/skip-prediction-challenge-starter-kit/issues"" target=""_blank""> https://github.com/crowdAI/skip-prediction-challenge-starter-kit/issues </a></li>
  <li><strong>Discussion Forum</strong> : <a href=""https://www.crowdai.org/challenges/spotify-sequential-skip-prediction-challenge/topics"" target=""_blank""> https://www.crowdai.org/challenges/spotify-sequential-skip-prediction-challenge/topics </a></li>
</ul>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organisers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at:</p>

<ul>
  <li><a href=""mailto:wsdm-cup-2019@spotify.com"" target=""_blank""> wsdm-cup-2019@spotify.com </a></li>
  <li><a href=""mailto: sharada.mohanty@epfl.ch"" target=""_blank"">sharada.mohanty@epfl.ch</a></li>
  <li><a href=""mailto: brianbrost@spotify.com"" target=""_blank"">brianbrost@spotify.com</a></li>
</ul>

<h3 id=""prizes"">Prizes</h3>

<p>The prizes will be administered as part of the 2019 WSDM Cup. The winning team will be awarded 
<strong>AUD2000</strong>, the second placed team will be awarded <strong>AUD750</strong>, and the third placed team will be awarded <strong>AUD250</strong>. All prizes are in Australian Dollars.</p>

<h2 id=""call-for-papers"">Call for Papers</h2>

<p>Submissions must be in English, in PDF format, and should not exceed four pages in the current ACM two-column conference format (including references and figures). Suitable LaTeX and Word templates are available from the ACM Website. The papers can represent reports of original research, preliminary research results, or proposals for new work. The review process is ​​single-blind. ​Please mention the team name in the title or abstract, and provide a link to the repository for the open sourced code in your paper. Papers will be evaluated according to their significance, originality, technical content, style, clarity, and likelihood of generating discussion. The submission deadline is January 11, 2019 (AOE timezone).</p>

<p>Papers should be submitted on <a href=""https://easychair.org/conferences/?conf=wsdmcup2019ssspc"" target=""_blank""> EasyChair </a></p>

<h3 id=""datasets-license"">Datasets License</h3>

"
161,"<hr />
<p><strong>Important note:</strong></p>

<p><em>The <strong>ImageCLEF Tuberculosis - Severity Scoring challenge has officially ended</strong> and we would like to thank everybody for their participation. You can find the official results at <a href=""http://imageclef.org/2018/tuberculosis"">http://imageclef.org/2018/tuberculosis </a>.</em></p>

<p><em>Post-challenge submissions and the leaderboard will remain enabled  for a few weeks so you will still be able to submit result files and have them continuously evaluated during a limited period. 
Please consider that in order to see the version of the leaderboard with the post-challenge submissions integrated, you have to turn on the switch <strong>Show post-challenge submission</strong> right below the leaderboard.</em></p>

<p><em>At the same time we’d like to encourage you to submit a <a href=""http://clef2018.clef-initiative.eu/index.php?page=Pages/InstructionsforCLEF2018WorkingNotes.html"">CLEF Working notes paper</a> until the end of May.</em></p>

<p><em>Please also note that participants registering from now on will not be
automatically registered with CLEF anymore.</em></p>

<hr />

<p><em>Note: ImageCLEF Tuberculosis 2018 is divided into 3 subtasks (challenges). This challenge is about <strong>Severity Scoring</strong>. For information on the <strong>MDR (multi-drug-resistance) Detection</strong> challenge click <a href=""/challenges/imageclef-2018-tuberculosis-mdr-detection"" target=""_blank""> here </a>. For information on the <strong>TBT (tuberculosis type) Classfication</strong> challenge click <a href=""/challenges/imageclef-2018-tuberculosis-tbt-classification"" target=""_blank""> here </a>. All of these challenges share the same dataset, so registering for one of these challenges will automatically give you access to the other ones.</em></p>

<p><em>Note: Do not forget to read the <strong>Rules</strong> section on this page</em></p>

<h3 id=""motivation"">Motivation</h3>

<p>About 130 years after the discovery of Mycobacterium tuberculosis, the disease remains a persistent threat and a leading cause of death worldwide.</p>

<p>The greatest disaster that can happen to a patient with tuberculosis (TB) is that the organisms become resistant to two or more of the standard drugs. In contrast to drug sensitive (DS) tuberculosis, its multi-drug resistant (MDR) form is much more difficult and expensive to recover from. Thus, early detection of the drug resistance (DR) status is of great importance for effective treatment. The most commonly used methods of DR detection are either expensive or take too much time (up to several month). Therefore there is a need for quick and at the same time cheap methods of DR detection. One of the possible approaches for this task is based on Computed Tomography (CT) image analysis. Another challenging task is automatic detection of TB types (TBT) using CT volumes.</p>

<p><em>Differences compared to 2017</em>: Scoring the severity of TB cases based on chest CT images is another task compared to both tuberculosis-related subtasks considered in 2017. There are no direct links between them. Note only that original CT image datasets used in 2017 and in 2018 may slightly overlap.</p>

<h3 id=""challenge-description"">Challenge description</h3>

<p>The goal on this task is to score the TB severity.</p>

<h3 id=""data"">Data</h3>

<p>The dataset for this subtask includes chest CT scans of TB patients along with the corresponding severity score (1 to 5) and the severity level designated as “low” and “high”.</p>

<table>
  <thead>
    <tr>
      <th>Num. Patients</th>
      <th style=""text-align: center"">Train</th>
      <th style=""text-align: center"">Test</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Low severity</td>
      <td style=""text-align: center"">90</td>
      <td style=""text-align: center"">62</td>
    </tr>
    <tr>
      <td>High severity</td>
      <td style=""text-align: center"">80</td>
      <td style=""text-align: center"">47</td>
    </tr>
    <tr>
      <td><strong>Total patients</strong></td>
      <td style=""text-align: center""><strong>170</strong></td>
      <td style=""text-align: center""><strong>109</strong></td>
    </tr>
  </tbody>
</table>

<p>We provide 3D CT images with slice size of 512*512 pixels and number of slices varying from about 50 to 400. All the CT images are stored in NIFTI file format with .nii.gz file extension (g-zipped .nii files). This file format stores raw voxel intensities in Hounsfield units (HU) as well the corresponding image metadata such as image dimensions, voxel size in physical units, slice thickness, etc. A freely-available tool called <a href=""https://www.creatis.insa-lyon.fr/rio/vv"" target=""_blank""> “VV” </a> can be used for viewing image files. Currently, there are various tools available for reading and writing NIFTI files. Among them there are <a href=""https://de.mathworks.com/matlabcentral/fileexchange/8797-tools-for-nifti-and-analyze-image/content/load_nii.m"" target=""_blank""> load_nii </a> and <a href=""https://de.mathworks.com/matlabcentral/fileexchange/8797-tools-for-nifti-and-analyze-image/content/save_nii.m"" target=""_blank""> save_nii </a> functions for Matlab and <a href=""http://niftilib.sourceforge.net/"" target=""_blank""> Niftilib </a> library for C, Java, Matlab and Python.</p>

<p>We also provide automatic extracted masks of the lungs. This material can be downloaded together with the patients CT images. The details of this segmentation can be found <a href=""http://publications.hevs.ch/index.php/publications/show/1871"" target=""_blank""> here </a>.
In case the participants use these masks in their experiments, please refer to the section “Citations” to find the appropriate citation for this lung segmentation technique.</p>

<h3 id=""submission-instructions"">Submission instructions</h3>

<hr />
<p><em>As soon as the submission is open, you will find a “Create Submission” button on this page (just next to the tabs)</em></p>

<hr />

<p>Submit a plain text file named with the prefix <strong>SVR</strong> (e.g. SVRfree-text.txt) with the following format:</p>

<p>&lt;Patient-ID&gt;,&lt;Severity score&gt;,&lt;Probability of “HIGH” severity&gt;</p>

<p>e.g.:</p>

<div class=""highlighter-rouge""><div class=""highlight""><pre class=""highlight""><code>SVR_TST_001,1,0.93
SVR_TST_002,3,0.54
SVR_TST_003,5,0.1
SVR_TST_004,4,0.245
SVR_TST_005,2,0.7
</code></pre></div></div>

<p><strong>Please use an integer value between 1 and 5 to indicate the severity score.</strong></p>

<p><strong>Please use a score between 0 and 1 to indicate the probability of the patient having “HIGH” severity (it corresponds to severity scores 1 to 3).</strong></p>

<p>You need to respect the following constraints:</p>

<ul>
  <li>Patient-IDs must be part of the predefined Patient-IDs</li>
  <li>All patient-IDs must be present in the runfiles</li>
  <li>Only use one integer value from 0 to 5 for the severity score</li>
  <li>Only use numbers between 0 and 1 for the probability. Use the dot (.) as a decimal point (no commas accepted)</li>
</ul>

<h3 id=""citations"">Citations</h3>

<p>Information will be posted after the challenge ends.</p>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>
<p>The results will be evaluated considering this task as a binary classification problem and as a regression problem. The classification problem will be evaluated using ROC-curves produced from the probabilities provided by the participants. For the regression problem, mean square error will be used.</p>

<p>The leaderboard will be visible from the 01.05.2018. However, the submission system will remain open few more days.</p>

<h3 id=""resources"">Resources</h3>

<h3 id=""contact-us"">Contact us</h3>
<ul>
  <li>Technical issues :<a href=""https://gitter.im/crowdAI/imageclef-2018-tuberculosis-severity-scoring"" target=""_blank"">  https://gitter.im/crowdAI/imageclef-2018-tuberculosis-severity-scoring </a></li>
  <li>Discussion Forum :<a href=""https://www.crowdai.org/challenges/imageclef-2018-tuberculosis-severity-scoring/topics"" target=""_blank"">  https://www.crowdai.org/challenges/imageclef-2018-tuberculosis-severity-scoring/topics </a></li>
</ul>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

<ul>
  <li>Sharada Prasanna Mohanty: sharada.mohanty@epfl.ch</li>
  <li>Yashin Dicente Cid: yashin[DOT]dicente[AT]hevs[DOT]ch</li>
  <li>Henning Müller: henning[DOT]mueller[AT]hevs[DOT]ch</li>
  <li>Ivan Eggel: ivan[DOT]eggel[AT]hevs[DOT]ch</li>
</ul>

<h3 id=""more-information"">More information</h3>

<p>You can find additional information on the challenge here:
<a href=""http://imageclef.org/2018/tuberculosis"" target=""_blank""> http://imageclef.org/2018/tuberculosis </a></p>

<h3 id=""prizes"">Prizes</h3>

<p>ImageCLEF 2018 is an evaluation campaign that is being organized as part of the <a href=""http://clef2018.clef-initiative.eu/"" target=""_blank""> CLEF initiative </a> labs. The campaign offers several research tasks that welcome participation from teams around the world. The results of the campaign appear in the working notes proceedings, published by CEUR Workshop Proceedings (CEUR-WS.org). Selected contributions among the participants, will be invited for publication in the following year in the Springer Lecture Notes in Computer Science (LNCS) together with the annual lab overviews.</p>

<h3 id=""datasets-license"">Datasets License</h3>

"
162,"<p><em>Note: Do not forget to read the Rules section on this page.</em></p>

<h3 id=""motivation"">Motivation</h3>

<p>Interpreting and summarizing the insights gained from medical images such as radiology output is a time-consuming task that involves highly trained experts and often represents a bottleneck in clinical diagnosis pipelines.</p>

<p>Consequently, there is a considerable need for automatic methods that can approximate this mapping from visual information to condensed textual descriptions. The more image characteristics are known, the more structured are the radiology scans and hence, the more efficient are the radiologists regarding interpretation. We work on the basis of a large-scale collection of figures from open access biomedical journal articles (PubMed Central). All images in the training data are accompanied by UMLS concepts extracted from the original image caption.</p>

<p>Lessons learned:</p>

<ul>
  <li>
    <p>In the first and second editions of this task, held at ImageCLEF 2017 and ImageCLEF 2018, participants noted a broad variety of content and situation among training images. For this year, the training data is reduced solely to radiology images</p>
  </li>
  <li>
    <p>A large number of concepts was used in the previous years. This year, the captions are first processed before concept extraction, hence leading to a reduced number of concepts</p>
  </li>
  <li>
    <p>As uncertainty regarding additional source was noted, we will clearly separate systems using exclusively the official training data from those that incorporate additional sources of evidence</p>
  </li>
</ul>

<h3 id=""challenge-description"">Challenge description</h3>

<p>The first step to automatic image captioning and scene understanding is identifying the presence and location of relevant concepts in a large corpus of medical images. Based on the visual image content, this subtask provides the building blocks for the scene understanding step by identifying the individual components from which captions are composed. The concepts can be further applied for context-based image and information retrieval purposes.</p>

<p>Evaluation is conducted in terms of set coverage metrics such as precision, recall, and combinations thereof.</p>

<p>This task will be run using a subset of the Radiology Objects in COntext (<a href=""https://www.springerprofessional.de/radiology-objects-in-context-roco-a-multimodal-image-dataset/16204278"" target=""_blank""> ROCO </a>) dataset [1].</p>

<h3 id=""data"">Data</h3>

<p>From the PubMed Open Access subset containing 1,828,575 archives, a total number of 6,031,814
image - caption pairs were extracted. To focus on radiology images and non-compound figures, automatic filtering with deep learning systems as well as manual revisions were applied, reducing the dataset to 72,187 radiology images of several medical imaging modalities.</p>

<p>NOTE: If the usage of an additional source for training is intended, it should not be a subset of PubMed Central Open Access (archiving date: 01.02.2018 - 01.02.2019), to avoid an overlap with the test data.</p>

<p>The data can be downloaded from the “Dataset” tab and will be made available on:</p>

<ul>
  <li><strong>15.01.2019</strong> Training data</li>
  <li><strong>18.03.2019</strong> Test data</li>
</ul>

<h3 id=""submission-instructions"">Submission instructions</h3>

<p>As soon as the submission is open, you will find a “Create Submission” button on this page (just next to the tabs)</p>

<p>For the submission we expect the following format:</p>

<ul>
  <li>Figure-ID TAB Concept-ID-1;Concept-ID-2;Concept-ID-n</li>
</ul>

<p>e.g.:</p>

<ul>
  <li>
    <p>ROCO_41341 C0033785;C0035561</p>
  </li>
  <li>
    <p>ROCO_07563 C0043299;C1306645;C1548003;C1962945</p>
  </li>
</ul>

<p>You need to respect the following constraints:</p>

<ul>
  <li>The separator between the figure ID and the concepts has to be a tabular whitespace</li>
  <li>The separator between the UMLS concepts has to be a semicolon [ ; ]</li>
  <li>Each figure ID of the test set must be included in the submitted file exactly once (even if there 
 are not concepts)</li>
  <li>The same concept cannot be specified more than once for a given figure ID</li>
</ul>

<h3 id=""acknowledgements"">Acknowledgements</h3>

<p><a href=""http://www.ncbi.nlm.nih.gov/pmc/"" target=""_blank""> PubMed Central </a></p>

<h3 id=""references"">References</h3>
<p>[1] O. Pelka, S. Koitka, J. Rückert, F. Nensa und C. M. Friedrich „Radiology Objects in COntext (ROCO): A Multimodal Image Dataset“, Proceedings of the MICCAI Workshop on Large-scale Annotation of Biomedical data and Expert Label Synthesis (MICCAI LABELS 2018), Granada, Spain, September 16, 2018, Lecture Notes in Computer Science (LNCS) Volume 11043, Page 180-189, DOI: 10.1007/978-3-030-01364-6_20, Springer Verlag, 2018.</p>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>
<p>Evaluation is conducted in terms of <strong>F1 scores</strong> between system predicted and ground truth concepts, using the following methodology and parameters:</p>

<ul>
  <li>
    <p>The default implementation of the Python scikit-learn (v0.17.1-2) F1 scoring method is used. It is documented here.</p>
  </li>
  <li>
    <p>A Python (3.x) script loads the candidate run file, as well as the ground truth (GT) file, and processes each candidate-GT concept sets</p>
  </li>
  <li>
    <p>For each candidate-GT concept set, the <strong>y_pred</strong> and <strong>y_true</strong> arrays are generated. They are binary arrays indicating for each concept contained in both candidate and GT set if it is present (1) or not (0).</p>
  </li>
  <li>
    <p>The F1 score is then calculated. The default ‘binary’ averaging method is used.</p>
  </li>
  <li>
    <p>All F1 scores are summed and averaged over the number of elements in the test set (10’000), giving the final score.</p>
  </li>
</ul>

<p>The ground truth for the test set was generated based on the <a href=""https://download.nlm.nih.gov/umls/kss/2018AB/umls-2016AB-full.zip"" target=""_blank""> UMLS Full Release 2018AB </a>.</p>

<p><strong>NOTE</strong> : The source code of the evaluation tool is available here. It <strong>must</strong> be executed using Python <strong>3.x</strong>, on a system where the scikit-learn (<strong>&gt;= v0.17.1-2</strong>) Python library is installed. The script should be run like this:</p>

<p><code class=""highlighter-rouge"">
/path/to/python3 evaluate-f1.py /path/to/candidate/file /path/to/ground-truth/file
</code></p>

<hr />
<p><em>The leaderboard will be visible from 01.05.2019 (official deadline) on. The submission system will remain open a few more days. Results submitted after the deadline will not be part of the official results.</em></p>

<hr />

<h3 id=""resources"">Resources</h3>

<h3 id=""contact-us"">Contact us</h3>

<ul>
  <li>Discussion Forum : <a href=""https://www.crowdai.org/challenges/imageclef-2019-caption-concept-detection/topics"" target=""_blank""> https://www.crowdai.org/challenges/imageclef-2019-caption-concept-detection/topics </a></li>
</ul>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

<ul>
  <li>Obioma Pelka: obioma.pelka@fh-dortmund.de</li>
  <li>Christoph M. Friedrich: christoph.friedrich@fh-dortmund.de</li>
  <li>Alba Garcia Seco de Herrera: alba.garcia@essex.ac.uk</li>
  <li>Henning Müller: henning.mueller@hevs.ch</li>
</ul>

<h3 id=""more-information"">More information</h3>

<p>You can find additional information on the challenge here:
<a href=""https://www.imageclef.org/2019/medical/caption"" target=""_blank""> https://www.imageclef.org/2019/medical/caption </a></p>

<h3 id=""prizes"">Prizes</h3>

<p>ImageCLEF 2019 is an evaluation campaign that is being organized as part of the <a href=""http://clef2019.clef-initiative.eu/"" target=""_blank""> CLEF initiative </a> labs. The campaign offers several research tasks that welcome participation from teams around the world. The results of the campaign appear in the working notes proceedings, published by CEUR Workshop Proceedings (CEUR-WS.org). Selected contributions among the participants, will be invited for publication in the following year in the Springer Lecture Notes in Computer Science (LNCS) together with the annual lab overviews.</p>

<h3 id=""datasets-license"">Datasets License</h3>

"
205,"<h3 id=""introduction"">Introduction</h3>

<p>Identifying suspicious clients undertaking abnormal behaviors, fraudulent activities or terrorist financing is a major challenge in Compliance at Credit Suisse. Nowadays, rule based scenarios are not relevant anymore and data science is seen as a game changer for the bank. In a world where there are more and more regulations and where the reputational risk is more and more at stake for Credit Suisse, we need you to find out the methods for the future and help us identify our suspicious clients using state of the art machine learning techniques and your smart analysis.</p>

<p>You will be given a set of clients with a list of features regarding their profiles and activity with the bank such as the total transaction amount during the year, or the age of the client. You will then have to derive a list of suspicious clients to investigate for the financial crime compliance team - taking also into account not only the features but also regulators rules.</p>

<p>The submission will be evaluated both on the algorithmic approach, the scoring results and the novelty in visualizing the data and the model results.</p>

<h3 id=""the-challenge"">The Challenge</h3>

<p>LZHCK Bank is required to monitor some suspicious client activity by the regulators. Unfortunately, the scenarios are getting deprecated and returns more and more false positives - LZHCK  is getting some pressure from the Regulators to have better results and the Investigation Team is asking for help from the data science team.<br />
You are in charge of the data science team and you are required to provide a  list of clients which are worth being investigated. To find this list of suspicious clients, you are given client data from the previous year :</p>

<p>Year 2016:</p>

<table>
  <thead>
    <tr>
      <th>Client ID</th>
      <th>SUSPICIOUS?</th>
      <th>Turnover</th>
      <th>Age</th>
      <th>…</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>C00100</td>
      <td>Yes</td>
      <td>$1600</td>
      <td>32</td>
      <td>…</td>
    </tr>
    <tr>
      <td>C00200</td>
      <td>No</td>
      <td>$12</td>
      <td>55</td>
      <td>…</td>
    </tr>
    <tr>
      <td>C00300</td>
      <td>No</td>
      <td>$1</td>
      <td>null</td>
      <td>…</td>
    </tr>
  </tbody>
</table>

<p>You then have to guess the suspicious clients for the next year 2017.</p>

<p>Column names:<br />
customer: customer number<br />
category: 0-Individual, 1-Business 2-Organization<br />
turnover: amount_going_in-amount_going_out<br />
transaction_count: number of transactions the client made<br />
io_ratio: number_transaction_in/(number_transaction_in+number_transaction_out)<br />
is_pep: is the client political<br />
inactive_days_average : average number of consecutive days without transaction<br />
inactive_days_max: maximum number of consecutive days without transactions<br />
n_of_accounts: number of accounts own by the customer<br />
distinct_counterparties: number of distinct counterparties (people you send moeny to)<br />
channel_risk: global risk score based on the different channels (online banking, mobile, trading platform etc…)<br />
atm_withdrawal: amounts withdrawn from atm<br />
atm_deposit: amount deposited with cash on the accounts</p>

<h3 id=""your-mission"">Your Mission</h3>

<ul>
  <li>
    <p>Along the 3 rounds, the investigation team requires you not only to provide your results, but also to help them understand your decision by building a tool or a visualisation interface or whatever you can imagine to help them make a decision on who has a fraudulent behaviour vs who doesn’t.</p>
  </li>
  <li>
    <p>Round 1: Get the first batch - 2 hours   <br />
The bank committed to the regulators that a new program of transaction monitoring will be put in place in the horizon end-of-the-year. However, deadlines are tight and regulators expect to have some customers to investigate already.<br />
Therefore, you have to provide a list of 200 customers that are worth being investigated. The Data Sourcing team and Transformation Team already provided you with some features, now it’s up to you to decide!</p>
  </li>
  <li>
    <p>Round 2: More Data, More Batch - 4 hours   <br />
The first batch ran successfully. Now you are asked to provide a set of 1000 customers.<br />
It’s now your turn to decide which customers should the investigation team have a careful look at!</p>
  </li>
  <li>
    <p>Round 3: Business As Usual - Rest of the challenge <br />
The regulators are very happy with the performance of the bank. You successfully managed to build a model to identify risky clients - so much so that the project receives more fund to maintain it for the next year.
You are now given a new dataset and you have to provide a full list of customers that needs to be investigated. You can choose the size of the list but you have to take the following into consideration:<br />
1 The cost of an investigation is inflexible and is I.<br />
2 The code of a Fine by the regulators is F. Each undiscovered fraudulent behavior has a probability P of being discovered during the year by the regulators.<br />
Therefore, you have to provide a full list of N suspicious clients with M suspicious customers that you missed, your final score will be:<br />
S = -(I * N + F * P * M)<br />
This is the money you lose. Since it’s negative (because we are losing money), you need to maximize it (get it close to zero).</p>
  </li>
</ul>

<p>I = 1’000 chf</p>

<p>F = 6’000’000 chf</p>

<p>P = 1%</p>

<p>Good Luck!</p>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>
<p>The evaluation will be based on multiple aspects of your work:</p>

<ul>
  <li>The scores from the 3 rounds of the hackathon</li>
  <li>The quality and originality of your innovative data visualization tool to help the investigation team to understand your results and have a deeper understanding of why you raised some specific alerts.</li>
  <li>The presentation you will make to the jury.</li>
</ul>

<p>Submissions should be csv files with a header with the following format:</p>

<p>customer<br />
900000001<br />
900000017
…</p>

<h3 id=""resources"">Resources</h3>

<p>You will find a training dataset of 1 million rows located here:</p>

<p><a href=""https://www.dropbox.com/s/ukurfe8021n5nvp/train.csv.zip?dl=0"" target=""_blank""> train.csv.zip  </a></p>

<p>You will have to evaluate the following datasets:</p>

<p><a href=""https://www.dropbox.com/s/izlt3krojguyt1i/test.csv.zip?dl=0"" target=""_blank""> test.csv.zip  </a></p>

<p>Example submission for round 1:</p>

<p><a href=""https://www.dropbox.com/s/7j9jglriu0817fr/prediction_sample.csv?dl=0"" target=""_blank""> prediction_sample.csv </a></p>

<h3 id=""prizes"">Prizes</h3>

<p>At the end of the Hackathon.</p>

<h3 id=""datasets-license"">Datasets License</h3>

"
206,"<p>Snakebite is the most deadly neglected tropical disease (NTD), being responsible for <a href=""http://www.who.int/snakebites/en/"" target=""_blank""> a dramatic humanitarian crisis in global health </a></p>

<p>Snakebite causes over 100,000 human deaths and 400,000 victims of disability and disfigurement globally every year. It affects poor and rural communities in developing countries, which host the highest venomous snake diversity and the highest burden of snakebite due to limited medical expertise and access to antivenoms <a href=""https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(18)31224-8/fulltext"" target=""_blank""> due to limited medical expertise and access to antivenoms </a></p>

<p>Antivenoms can be life‐saving when correctly administered but this depends first on the correct taxonomic identification (i.e. family, genus, species) of the biting snake. Snake identification is challenging due to</p>

<ul>
  <li>
    <p>their high diversity</p>
  </li>
  <li>
    <p>the incomplete or misleading information provided by snakebite victims</p>
  </li>
  <li>
    <p>the lack of knowledge or resources in herpetology that healthcare professionals have</p>
  </li>
</ul>

<p>In this challenge we want to explore how Machine Learning can help with snake identification, in order to potentially reduce erroneous and delayed healthcare actions.</p>

<h2 id=""task"">Task</h2>

<p>In this challenge you will be provided with a dataset of RGB images of snakes, and their corresponding species (class). The goal is to train a classification model.</p>

<p>The difficulty of the challenge relies on the dataset characteristics, as there might be a high intraclass variance for certain classes and a low interclass variance among others, as shown in the examples from the Datasets section. Also, the distribution of images between class is not equal for all classes: the class with the most images has 11,794, while the class with the fewest images has 508.</p>

<p>For now, we would like to make the barrier to entry much lower and demonstrate that an approach works well on 76 species and 123,665 images. The idea would be then to renew the challenge every 4 months in order to get closer to our final goal, which is to build an algorithm which best predicts which antivenin should be given (if any) when given a specific image.</p>

<h2 id=""datasets"">Datasets</h2>

<p>Snakes are extremely diverse, and snake biologists continue to document &amp; describe snake diversity, with an average of 30 new species described per year since the year 2000. Although most people probably think of snakes as a single “kind” of animal, humans are as evolutionarily close to whales as pythons are to rattlesnakes, so snakes in fact are very diverse! Taxonomically speaking, 
 <a href=""http://reptile-database.reptarium.cz/advanced_search?taxon=serpentes&amp;submit=Search"" target=""_blank"">snakes are classified into 24 families, containing 528 genera and 3,709 species.   </a></p>

<p><img src=""https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/836c44312cef907354e41d7e5c84bd3c_image1_challenge_dataset.png"" alt=""image1_challenge_dataset.png"" /></p>

<p>You can download the datasets in the Datasets Section. You are provided with a Train.tar.gz, file composed of 123,665 RGB images of varying size, split into 76 species. This dataset has an independent unit column (‘ind_unit’), which should be used to split the data into validation and training sets. Indeed, two images having the same independent unit might (but not always) be highly correlated, so if you don’t want to be unpleasantly surprised during the testing phase, you should be splitting according to the independent unit.</p>

<p>Several aspects of snake morphology make this challenge more challenging:</p>

<ul>
  <li>
    <p>Some species have patterns that vary depending on their age</p>
  </li>
  <li>
    <p>Some species have patterns that vary depending on their location</p>
  </li>
  <li>
    <p>Two species might look very similar, with one being venomous and the other not</p>
  </li>
</ul>

<p>The first iteration of the data set contains few such species, but we will add in more later.</p>

<p><img src=""https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/f4e927cb3680ceb410ff825a8c0a53c4_picture2_challenge_datasets.png"" alt=""picture2_challenge_datasets.png"" /></p>

<p><img src=""https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/94e519cc1e7fabdd92f535897bbd7356_picture3_challenge_datasets.png"" alt=""picture3_challenge_datasets.png"" /></p>

<h2 id=""timeline"">Timeline</h2>
<p>This is the very first <strong>benchmarking</strong> challenge, meaning that it has no end date, but it will be updated every 3 months. Here are the first deadlines:</p>

<ul>
  <li>
    <p>November 21th, 2018: Competition Open</p>
  </li>
  <li>
    <p>February 21th, 2018: Qualifying Round 1 submission deadline</p>
  </li>
  <li>
    <p>Mai 21th, 2018: Qualifying Round 2 submission deadline</p>
  </li>
</ul>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>
<p>The Precision and Recall score will be used to evaluate the performance of your model. It is first computed separately for all classes by using the following formula,</p>

<ul>
  <li>
    <p><strong>precision = TP/(TP+FP)</strong></p>
  </li>
  <li>
    <p><strong>recall = TP/(TP + FN)</strong></p>
  </li>
</ul>

<p>where TP refers to the number of True Positives, FP refers to the number of False Positives and FN refers to the number of False Negatives.</p>

<p>Then the mean of the Precision and the mean of the Recall across all the classes is used to come up with the final scores. All submissions will be evaluated on a testing dataset composed of 39’975 images.</p>

<h3 id=""resources"">Resources</h3>

<p>We have put together a Starter Kit that contains the necessary documentation in, hopefully, easily understandable form providing some utility scripts that help you get startet. Please head over to the Starter Kit, where the README should guide you through the content available there.</p>

<p>In case of questions about the material, please do not hesitate to contact us.</p>

<h2 id=""contact-us"">Contact us</h2>

<p><strong>For technical questions:</strong></p>

<ul>
  <li>
    <p>Gitter Channel :</p>
  </li>
  <li>
    <p>Technical Issues :</p>
  </li>
  <li>
    <p>Discussion Forum :</p>
  </li>
</ul>

<p><strong>For snake biology, taxonomy or anatomy questions:</strong></p>

<p><a href=""https://www.crowdai.org/participants/amdurso"">@amdurso</a> in <a href=""link_url"" target=""https://www.crowdai.org/topics/questions-about-snake-classification-snake-biology/discussion"">snake classification &amp; snake biology forum thread</a></p>

<h3 id=""prizes"">Prizes</h3>

<p>Top-1 participant: Invitation to co-author a paper describing their solution</p>

<h3 id=""datasets-license"">Datasets License</h3>

"
207,"<p>Submission Instructions : <a href=""https://gitlab.crowdai.org/Microsoft-TextWorld/microsoft-textworld-2018-starter-kit"">https://gitlab.crowdai.org/Microsoft-TextWorld/microsoft-textworld-2018-starter-kit</a></p>

<p>TODO: Add Overview, etc</p>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>

<h3 id=""resources"">Resources</h3>

<h3 id=""prizes"">Prizes</h3>

<h3 id=""datasets-license"">Datasets License</h3>

"
208,"<p><strong>Version française disponible <a href=""https://theory.epfl.ch/osven/SVOR2019/VERSIONf.pdf"" target=""_blank""> ici</a>.</strong></p>

<p><strong>Deutsche Version <a href=""https://theory.epfl.ch/osven/SVOR2019/VERSIONd.pdf"" target=""_blank"">hier</a> erhältlich.</strong></p>

<p><span style=""color:red""> <b> Red alert! </b></span> Last night, burglars stole cheese stocks throughout the canton of Fribourg! Even immediate police intervention could not prevent this theft. Nevertheless, thanks to the close collaboration between the special units in Europe, you, a collaborator of the PMS (Possible Mission Service) have been able to locate the places where the thieves hide the cheese.</p>

<p>It turns out that they have stored it at different locations across Europe. These places are all protected by an alarm system that no human being can disable. But as a manager of new technologies within the PMS, you have recently developed a special drone that could disable this type of alarm.</p>

<p>For each location, a due time has been fixed by which we would like to recover the cheese stored in that location. Your mission is to program the drone to visit all storage locations exactly once and turn off the alarm such that the total delay is minimized.  This will allow the special anti-thief-cheese units to recover the cheese as quickly as possible.
The delay of a storage location is computed as follows: If the drone can deactivate the alarm before the due time of that location, then the delay is equal to 0; otherwise, the delay equals the difference between the time of arrival of the drone and the due time of the location. The total delay is then the sum of the delays of the different locations. Finally, the time it takes to arrive at a location is the total distance travelled from the point of departure to that location.</p>

<p>To understand the challenge you are facing consider the following small example:
<img src=""https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/df62eb40eba2c95e47ef9cd9006947aa_Four_examplecenter.jpg"" alt=""Four_examplecenter.jpg"" /></p>

<p>In this example there are exactly four different storage locations. The position of each location  is given by its x-coordinate and its y-coordinate. For example, the position of Location 4 is (3,4), i.e., its x-coordinate is 3 and its y-coordinate is 4. The distance between two locations is given by the Euclidean distance <strong>rounded to its closest integer</strong>. For example,</p>

<ul>
  <li>The distance between Location 1 and Location 4 is the square-root of (3-0)*(3-0) + (4-0) * (4-0) rounded to the closest integer, which equals 5.</li>
  <li>The distance between Location 2 and Location 3 is the square-root of (-2-0)<em>(-2-0) + (-2-7)</em>(-2-7) rounded to the closest integer, which equals 9.</li>
</ul>

<p>Suppose now that the locations have the following due times. Location 1: 2; Location 2: 3; Location 3: 8 and Location 4: 13.  If, in the above example, you decide to program the drone to visit the locations in the order 1-2-3-4, then we have the following arrival times:</p>

<ul>
  <li>Arrival time for Location 1:  0 = 0</li>
  <li>Arrival time for Location 2: 0 + 3  = 3</li>
  <li>Arrival time for Location 3: 0 + 3 + 9 = 12</li>
  <li>Arrival time for Location 4: 0 + 3 + 9 + 4 = 16</li>
</ul>

<p>This would thus result in the following delays:</p>

<ul>
  <li>Delay for Location 1: 0</li>
  <li>Delay for Location 2: 0</li>
  <li>Delay for Location 3: 12-8 =4</li>
  <li>Delay for Location 4: 16-13 = 3</li>
</ul>

<p>So, the total delay is 7 if the locations are visited in the order 1-2-3-4.</p>

<p>Your goal is to find the order that minimizes the delay for two larger instances.</p>

<h2 id=""format-of-datasets"">Format of datasets</h2>

<p>There are two datasets <a href=""https://crowdai-prd.s3.eu-central-1.amazonaws.com/dataset_files/challenge_69/43c9d1d7-77cd-485c-9272-dfa22bd61ec0_Switzerland30.txt?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAILFF3ZEGG7Y4HXEQ%2F20190418%2Feu-central-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20190418T142304Z&amp;X-Amz-Expires=604800&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Signature=6ab9ce6af34beadb9bcafce8d2ec4f6c751177543622eeae23b8242c2a2add21"" target=""_blank""> “Swiss.txt” </a> and <a href=""https://crowdai-prd.s3.eu-central-1.amazonaws.com/dataset_files/challenge_69/ee10b726-9fa9-4bc7-a773-ddfc135b69b3_Large.txt?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAILFF3ZEGG7Y4HXEQ%2F20190418%2Feu-central-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20190418T142454Z&amp;X-Amz-Expires=604800&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Signature=f0ffcafe08389ab969ddbfc00be0233c459f7326a76f0d295d060759f85042ba"" target=""_blank""> “Large.txt” </a>. They have the following format.  One row per location consisting of four integers:</p>

<ul>
  <li>ID: the identification number of the location (1 is the starting location)</li>
  <li>X-COORDINATE: the X-coordinate of the location</li>
  <li>Y-COORDINATE: the Y-coordinate of the location</li>
  <li>DUE-TIME: the due time of the location</li>
</ul>

<p>So <a href=""https://crowdai-prd.s3.eu-central-1.amazonaws.com/dataset_files/challenge_69/43c9d1d7-77cd-485c-9272-dfa22bd61ec0_Switzerland30.txt?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAILFF3ZEGG7Y4HXEQ%2F20190418%2Feu-central-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20190418T142304Z&amp;X-Amz-Expires=604800&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Signature=6ab9ce6af34beadb9bcafce8d2ec4f6c751177543622eeae23b8242c2a2add21"" target=""_blank""> “Swiss.txt” </a> consists of 30 rows and <a href=""https://crowdai-prd.s3.eu-central-1.amazonaws.com/dataset_files/challenge_69/ee10b726-9fa9-4bc7-a773-ddfc135b69b3_Large.txt?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAILFF3ZEGG7Y4HXEQ%2F20190418%2Feu-central-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20190418T142454Z&amp;X-Amz-Expires=604800&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Signature=f0ffcafe08389ab969ddbfc00be0233c459f7326a76f0d295d060759f85042ba"" target=""_blank""> “Large.txt” </a> consists of 1379 rows.</p>

<p>In addition we have prepared an <a href=""https://crowdai-prd.s3.eu-central-1.amazonaws.com/dataset_files/challenge_69/0145ae47-725a-4cd9-aa6f-bb4c6c0759a5_SwissInstance.xlsx?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAILFF3ZEGG7Y4HXEQ%2F20190418%2Feu-central-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20190418T142013Z&amp;X-Amz-Expires=604800&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Signature=5471799a0cb5ec4c18e25f3401a680611cb6dce248ed3e326446566915e28793"" target=""_blank""> Excel document </a> where the cities of <a href=""https://crowdai-prd.s3.eu-central-1.amazonaws.com/dataset_files/challenge_69/43c9d1d7-77cd-485c-9272-dfa22bd61ec0_Switzerland30.txt?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAILFF3ZEGG7Y4HXEQ%2F20190418%2Feu-central-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20190418T142751Z&amp;X-Amz-Expires=604800&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Signature=9ef303ce0257eb496522f98e375639ab7cc632691bec35805f4016e7f812633f"" target=""_blank""> “Swiss.txt” </a> are illustrated and the objective function is evaluated automatically for inserted solutions.</p>

<p>All the indicated documents are available under the item <a href=""https://www.crowdai.org/challenges/cheese-hunting-for-swiss-highschool-students/dataset_files"" target=""_blank""> “Dataset” </a>.</p>

<h2 id=""submission-format"">Submission format</h2>

<p>The submission should be a text file consisting of one or two rows.</p>

<ul>
  <li>The first row contains the solution to the Swiss instance: a permutation of the Location IDs 1, 2, …, 30 written with spaces between the IDs.</li>
  <li>The second row (if it exists) contains the solution of the Large instance: a permutation of the Location IDs from 1 to 1379 written with spaces between the IDs.</li>
</ul>

<p>If no second row is given then a delay of 9999999999 is given to the Large instance.</p>

<p>An example of a submission can be found <a href=""https://crowdai-prd.s3.eu-central-1.amazonaws.com/dataset_files/challenge_69/4e821409-d5ce-4e30-84e3-032dd094d37d_sample_submission.txt?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAILFF3ZEGG7Y4HXEQ%2F20190418%2Feu-central-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20190418T141807Z&amp;X-Amz-Expires=604800&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Signature=371114d2d5a71206c85d4da8826c1a48d72be2a652a9122ece3600345e1300d8"" target=""_blank""> here. </a></p>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>
<p>Submissions are ranked by their delay (a smaller delay is better) of the submitted solutions to the Swiss challenge consisting of the 30 most populous cities of Switzerland.</p>

<p>Two submissions with the same delay on the Swiss challenge are then ranked according to</p>

<ul>
  <li>the delay of their submissions to the large instance of 1379 locations.</li>
  <li>If the delay of the solutions to the large instance is also the same then the submissions are ranked according to the date of submission (the earlier the better).</li>
</ul>

<h3 id=""resources"">Resources</h3>

<p>The competition is organized by the <a href=""http://svor.ch"" target=""_blank"">Swiss Operations Research Society </a></p>

<h3 id=""prizes"">Prizes</h3>

<ul>
  <li>CHF 500.- for first prize</li>
  <li>CHF 300.- for second prize</li>
  <li>CHF 200.- for third prize</li>
</ul>

<h3 id=""datasets-license"">Datasets License</h3>

"
163,"<p><em>Note: ImageCLEF Tuberculosis 2019 is divided into 2 subtasks (challenges). This challenge is about <strong>Severity Scoring</strong>. For information on the <strong>CT report</strong> challenge click <a href=""/challenges/imageclef-2019-tuberculosis-ct-report"" target=""_blank""> here </a>. Both challenges share the same dataset, so registering for one of these challenges will automatically give you access to the other one.</em></p>

<p><em>Note: Do not forget to read the Rules section on this page.</em></p>

<h3 id=""motivation"">Motivation</h3>

<p>Tuberculosis (TB) is a bacterial infection caused by a germ called Mycobacterium tuberculosis. About 130 years after its discovery, the disease remains a persistent threat and a leading cause of death worldwide according to WHO. This bacteria usually attacks the lungs, but it can also damage other parts of the body. Generally, TB can be cured with antibiotics. However, the different types of TB require different treatments, and therefore the detection of the TB type and the evaluation of the severity stage are two important tasks.</p>

<h3 id=""challenge-description"">Challenge description</h3>

<p>This subtask is aimed at assessing TB severity score. The Severity score is a cumulative score of severity of TB case assigned by a medical doctor. Originally, the score varied from 1 (“critical/very bad”) to 5 (“very good”). In the process of scoring, the medical doctors considered many factors like pattern of lesions, results of microbiological tests, duration of treatment, patient’s age and some other. The goal of this subtask is to assess the severity based on the CT image and some additional meta-data, including disability, relapse, comorbidity, bacillary and smoking among others.</p>

<h3 id=""data"">Data</h3>

<p>In this edition, both subtasks (SVR and CTR) use the same dataset containing 335 chest CT scans of TB patients along with a set of clinically relevant metadata. 218 patients are used for training and 117 for test. The selected metadata includes the following binary measures: disability, relapse, symptoms of TB, comorbidity, bacillary, drug resistance, higher education, ex-prisoner, alcoholic, smoking.</p>

<p>For all patients we provide 3D CT images with slice size of 512*512 pixels and number of slices varying from about 50 to 400. All the CT images are stored in NIFTI file format with .nii.gz file extension (g-zipped .nii files). This file format stores raw voxel intensities in Hounsfield units (HU) as well the corresponding image metadata such as image dimensions, voxel size in physical units, slice thickness, etc. A freely-available tool called <a href=""https://www.creatis.insa-lyon.fr/rio/vv"" target=""_blank""> “VV” </a> can be used for viewing image files. Currently, there are various tools available for reading and writing NIFTI files. Among them there are <a href=""https://de.mathworks.com/matlabcentral/fileexchange/8797-tools-for-nifti-and-analyze-image/content/load_nii.m"" target=""_blank""> load_nii </a> and <a href=""https://de.mathworks.com/matlabcentral/fileexchange/8797-tools-for-nifti-and-analyze-image/content/save_nii.m"" target=""_blank""> save_nii </a> functions for Matlab and <a href=""http://niftilib.sourceforge.net/"" target=""_blank""> Niftilib </a> library for C, Java, Matlab and Python.</p>

<p>We also provide automatic extracted masks of the lungs. This material can be downloaded together with the patients CT images. The details of this segmentation can be found <a href=""http://publications.hevs.ch/index.php/publications/show/1871"" target=""_blank""> here </a>.
In case the participants use these masks in their experiments, please refer to the section “Citations” in the <a href=""https://www.imageclef.org/2019/medical/tuberculosis"" target=""_blank""> ImageCLEF TB 2019 </a>
website to find the appropriate citation for this lung segmentation technique.</p>

<p><strong>Remarks on the automatic lung segmentation:</strong></p>

<p>The segmentations were manually analysed based on statistics on number of lungs found and size ratio between right-left lung. Only those segmentations with anomalies on these statistics were visualized. The code used to segment the patients was adapted for the cases with unsatisfactory segmentation. After this proceeding, all patients with anomalies presented a satisfactory mask.</p>

<h3 id=""submission-instructions"">Submission instructions</h3>

<hr />
<p><em>As soon as the submission is open, you will find a “Create Submission” button on this page (just next to the tabs)</em></p>

<hr />

<p>Submit a plain text file named with the prefix <strong>SVR</strong> (e.g. SVRfree-text.txt) with the following format:</p>

<p>&lt;Patient-ID&gt;,&lt;Probability of “HIGH” severity&gt;</p>

<p>e.g.:</p>

<div class=""highlighter-rouge""><div class=""highlight""><pre class=""highlight""><code>CTR_TST_001,0.93
CTR_TST_002,0.54
CTR_TST_003,0.1
CTR_TST_004,0.245
CTR_TST_005,0.7
</code></pre></div></div>

<p><strong>Please use a score between 0 and 1 to indicate the probability of the patient having “HIGH” severity (it corresponds to severity scores 1 to 3).</strong></p>

<p>You need to respect the following constraints:</p>

<ul>
  <li>Patient-IDs must be part of the predefined Patient-IDs</li>
  <li>All patient-IDs must be present in the runfiles</li>
  <li>Only use numbers between 0 and 1 for the probability. Use the dot (.) as a decimal point (no commas accepted)</li>
</ul>

<h3 id=""citations"">Citations</h3>

<p>Information will be posted after the challenge ends.</p>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>
<p>This task will be evaluated as binary classification problem, including measures such as Area Under the ROC Curve (AUC) and accuracy.
The ranking of the techniques will be first based on the <strong>AUC</strong> and then by the <strong>accuracy</strong>.</p>

<h3 id=""resources"">Resources</h3>

<h3 id=""contact-us"">Contact us</h3>

<ul>
  <li>Discussion Forum : <a href=""https://www.crowdai.org/challenges/imageclef-2019-tuberculosis-severity-scoring/topics"" target=""_blank""> https://www.crowdai.org/challenges/imageclef-2019-tuberculosis-severity-scoring/topics </a></li>
</ul>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

<p>Yashin Dicente Cid &lt;yashin.dicente(at)hevs.ch&gt;, University of Applied Sciences Western Switzerland, Sierre, Switzerland
Vitali Liauchuk &lt;vitali.liauchuk(at)gmail.com&gt;, Institute for Informatics, Minsk, Belarus
Vassili Kovalev &lt;vassili.kovalev(at)gmail.com&gt;, Institute for Informatics, Minsk, Belarus
Henning Müller &lt;henning.mueller(at)hevs.ch&gt;, University of Applied Sciences Western Switzerland, Sierre, Switzerland</p>

<h3 id=""more-information"">More information</h3>

<p>You can find additional information on the challenge here:
<a href=""https://www.imageclef.org/2019/medical/tuberculosis"" target=""_blank""> https://www.imageclef.org/2019/medical/tuberculosis</a></p>

<h3 id=""prizes"">Prizes</h3>

<p>ImageCLEF 2019 is an evaluation campaign that is being organized as part of the <a href=""http://clef2019.clef-initiative.eu/"" target=""_blank""> CLEF initiative </a> labs. The campaign offers several research tasks that welcome participation from teams around the world. The results of the campaign appear in the working notes proceedings, published by CEUR Workshop Proceedings (CEUR-WS.org). Selected contributions among the participants, will be invited for publication in the following year in the Springer Lecture Notes in Computer Science (LNCS) together with the annual lab overviews.</p>

<h3 id=""datasets-license"">Datasets License</h3>

"
165,"<p><em>Note: ImageCLEF Coral 2019 is divided into 2 subtasks (challenges). This challenge is about <strong>Annotation and Localisation</strong>. For information on the <strong>Pixel-wise Parsing</strong> challenge click <a href=""/challenges/imageclef-2019-coral-pixel-wise-parsing"" target=""_blank""> here </a>. Both challenges share the same dataset, so registering for one of these challenges will automatically give you access to the other one.</em></p>

<p><em>Note: Do not forget to read the Rules section on this page.</em></p>

<h3 id=""motivation"">Motivation</h3>

<p>The increasing use of structure-from-motion photogrammetry for modelling large-scale environments from action cameras attached to drones has driven the next-generation of visualisation techniques that can be used in augmented and virtual reality headsets. It has also created a need to have such models labelled, with objects such as people, buildings, vehicles, terrain, etc. all essential for machine learning techniques to automatically identify as areas of interest and to label them appropriately. However, the complexity of the images makes impossible for human annotators to assess the contents of images on a large scale.
Advances in automatically annotating images for complexity and benthic composition have been promising, and we are interested in automatically identify areas of interest and to label them appropriately for monitoring coral reefs. .Coral reefs are in danger of being lost within the next 30 years, and with them the ecosystems they support. This catastrophe will not only see the extinction of many marine species, but also create a humanitarian crisis on a global scale for the billions of humans who rely on reef services. By monitoring the changes and composition of coral reefs we can help prioritise conservation efforts.</p>

<h3 id=""challenge-description"">Challenge description</h3>

<p>This task requires the participants to segment and parse each coral reef image into different image regions associated with benthic substrate types. For each image, segmentation algorithms will produce a semantic segmentation mask, predicting the semantic category for each pixel in the image.</p>

<h3 id=""data"">Data</h3>

<p>The data for this task originates from a growing, large-scale collection of images taken from coral reefs around the world as part of a coral reef monitoring project with the Marine Technology Research Unit at the University of Essex. 
Substrates of the same type can have very different morphologies, color variation and patterns. Some of the images contain a white line (scientific measurement tape) that may occlude part of the entity. The quality of the images is variable, some are blurry, and some have poor color balance. This is representative of the Marine Technology Research Unit dataset and all images are useful for data analysis. The images contain annotations of the following 13 types of substrates: Hard Coral – Branching, Hard Coral – Submassive, Hard Coral – Boulder, Hard Coral – Encrusting, Hard Coral – Table, Hard Coral – Foliose, Hard Coral – Mushroom, Soft Coral, Soft Coral – Gorgonian, Sponge, Sponge – Barrel, Fire Coral – Millepora and Algae - Macro or Leaves.</p>

<p>The training set contains contains 240 images with 6670 substrates annotated. The test set contains 200 images.</p>

<p>The data can be downloaded from the “Dataset” tab and will be made available on:</p>

<ul>
  <li>
    <p>04.02.2019 Training data</p>
  </li>
  <li>
    <p>18.03.2019 Test data</p>
  </li>
</ul>

<h3 id=""submission-instructions"">Submission instructions</h3>

<hr />
<p><em>As soon as the submission is open, you will find a “Create Submission” button on this page (just next to the tabs)</em></p>

<hr />
<p>The submissions will be received through the crowdai system.</p>

<p>Participants will be permitted to submit up to 10 runs. External training data is allowed and encouraged.</p>

<p>Each system run will consist of a single ASCII plain text file. The results of each test set should be given in separate lines in the text file. The format of the text file is as follows:</p>

<p>[image_ID/document_ID] [results]</p>

<p>The results of each test set image should be given in separate lines, each line providing only up to 500 localised substrates, with up to 500 coordinate localisations of the same substrate expected. The format has characters to separate the elements, semicolon ‘;’ for the substrates, colon ‘:’ for the confidence, comma ‘,’ to separate multiple bounding polygons, and ‘x’ and ‘+’ for the size-offset bounding polygon format, i.e.:</p>

<p>[image_ID];[substrate1] [[confidence1,1]:][x1,1]+[y1,1]+[x2,1]+[y2,1]+….+[xn,1]+[yn,1],[[confidence1,2][x1,2]+[y1,2]+[x2,2]+[y2,2]+….+[xn,2]+[yn,2];[substrate2] …</p>

<p>[confidence] are floating point values 0-1 for which a higher value means a higher score and the [xi,yi] represents consecutive points.</p>

<p>For example, in the development set format (notice that there are 2 polygons for substrate c_soft_coral):</p>

<ul>
  <li>2018_0714_112604_057 0 c_hard_coral_branching 1 1757 833 1645 705 1559 598 1442 540 1249 593 1121 679 1020 705 998 844 891 967 966 1122 1137 1143 1324 1122 1468 1074 1655 978</li>
  <li>2018_0714_112604_057 3 c_soft_coral 1 2804 1368 2745 1368 2724 1427 2729 1507 2809 1507 2825 1453</li>
  <li>2018_0714_112604_057 4 c_soft_coral 1 2697 1576 2638 1592 2638 1608 2622 1667 2654 1694 2713 1731 2777 1731 2777 1635</li>
</ul>

<p>In the submission format, it would be a line as:</p>

<p>*2018_0714_112604_057;c_hard_coral_branching 0.6:1757+833+1645+705+1559+598+1442+540+1249+593+1121+679+1020+705+998+844+891+967+966+1122+1137+1143+1324+1122+1468+1074+1655+978;c_soft_coral 0.7:2804+1368+2745+1368+2724+1427+2729+1507+2809+1507+2825+1453,0.3:2697+1576+2638+1592+2638+1608+2622+1667+2654+1694+2713+1731+2777+1731+2777+1635</p>

<h3 id=""citations"">Citations</h3>

<p>Information will be posted after the challenge ends.</p>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>
<p>More on the evaluation criteria will be published soon.</p>

<h3 id=""resources"">Resources</h3>

<h3 id=""contact-us"">Contact us</h3>

<ul>
  <li>
    <p>Discussion Forum : <a href=""https://www.crowdai.org/challenges/imageclef-2019-coral-pixel-wise-parsing/topics"" target=""_blank""> https://www.crowdai.org/challenges/imageclef-2019-coral-pixel-wise-parsing/topics </a></p>
  </li>
  <li>
    <p>Join our mailing list: <a href=""https://groups.google.com/d/forum/imageclefcoral"" target=""_blank"">https://groups.google.com/d/forum/imageclefcoral</a></p>
  </li>
</ul>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

<ul>
  <li>
    <p>Jon Chamberlain &lt;jchamp(at)essex.ac.uk&gt;,University of Essex, UK</p>
  </li>
  <li>
    <p>Adrian Clark &lt;alien(at)essex.ac.uk&gt;,University of Essex, UK</p>
  </li>
  <li>
    <p>Antonio Campello &lt;antonio.campello(at)filament.ai&gt;,Filament, UK</p>
  </li>
  <li>
    <p>Alba García Seco de Herrera &lt;alba.garcia(at)essex.ac.uk&gt;,University of Essex, UK</p>
  </li>
</ul>

<h3 id=""more-information"">More information</h3>

<p>You can find additional information on the challenge here:
<a href=""https://www.imageclef.org/2019/coral"" target=""_blank""> https://www.imageclef.org/2019/coral </a></p>

<h3 id=""prizes"">Prizes</h3>

<p>ImageCLEF 2019 is an evaluation campaign that is being organized as part of the <a href=""http://clef2019.clef-initiative.eu/"" target=""_blank""> CLEF initiative </a> labs. The campaign offers several research tasks that welcome participation from teams around the world. The results of the campaign appear in the working notes proceedings, published by CEUR Workshop Proceedings (CEUR-WS.org). Selected contributions among the participants, will be invited for publication in the following year in the Springer Lecture Notes in Computer Science (LNCS) together with the annual lab overviews.</p>

<h3 id=""datasets-license"">Datasets License</h3>

"
166,"<p><em>Note: ImageCLEF Coral 2019 is divided into 2 subtasks (challenges). This challenge is about <strong>Annotation and Localisation</strong>. For information on the <strong>Pixel-wise Parsing</strong> challenge click <a href=""/challenges/imageclef-2019-coral-pixel-wise-parsing"" target=""_blank""> here </a>. Both challenges share the same dataset, so registering for one of these challenges will automatically give you access to the other one.</em></p>

<p><em>Note: Do not forget to read the Rules section on this page.</em></p>

<h3 id=""motivation"">Motivation</h3>

<p>The increasing use of structure-from-motion photogrammetry for modelling large-scale environments from action cameras attached to drones has driven the next-generation of visualisation techniques that can be used in augmented and virtual reality headsets. It has also created a need to have such models labelled, with objects such as people, buildings, vehicles, terrain, etc. all essential for machine learning techniques to automatically identify as areas of interest and to label them appropriately. However, the complexity of the images makes impossible for human annotators to assess the contents of images on a large scale.
Advances in automatically annotating images for complexity and benthic composition have been promising, and we are interested in automatically identify areas of interest and to label them appropriately for monitoring coral reefs. .Coral reefs are in danger of being lost within the next 30 years, and with them the ecosystems they support. This catastrophe will not only see the extinction of many marine species, but also create a humanitarian crisis on a global scale for the billions of humans who rely on reef services. By monitoring the changes and composition of coral reefs we can help prioritise conservation efforts.</p>

<h3 id=""challenge-description"">Challenge description</h3>

<p>This task requires the participants to label the images with types of benthic substrate together with their bounding box in the image. Each image is provided with possible class types. For each image, participants will produce a set of bounding boxes, predicting the benthic substrate for each bounding box in the image.</p>

<h3 id=""data"">Data</h3>

<p>The data for this task originates from a growing, large-scale collection of images taken from coral reefs around the world as part of a coral reef monitoring project with the Marine Technology Research Unit at the University of Essex. 
Substrates of the same type can have very different morphologies, color variation and patterns. Some of the images contain a white line (scientific measurement tape) that may occlude part of the entity. The quality of the images is variable, some are blurry, and some have poor color balance. This is representative of the Marine Technology Research Unit dataset and all images are useful for data analysis. The images contain annotations of the following 13 types of substrates: Hard Coral – Branching, Hard Coral – Submassive, Hard Coral – Boulder, Hard Coral – Encrusting, Hard Coral – Table, Hard Coral – Foliose, Hard Coral – Mushroom, Soft Coral, Soft Coral – Gorgonian, Sponge, Sponge – Barrel, Fire Coral – Millepora and Algae - Macro or Leaves.</p>

<p>The training set contains contains 240 images with 6670 substrates annotated.
Two files are provided with ground truth annotations: one based on bounding boxes “imageCLEFcoral2019_annotations_training_task_1”  and a more detailed annotation  based on bounding polygon “imageCLEFcoral2019_annotations_training_task_2”. The test set contains 200 images.</p>

<p>The data can be downloaded from the “Dataset” tab and will be made available on:</p>

<ul>
  <li>
    <p>04.02.2019 Training data</p>
  </li>
  <li>
    <p>18.03.2019 Test data</p>
  </li>
</ul>

<h3 id=""submission-instructions"">Submission instructions</h3>

<hr />
<p><em>As soon as the submission is open, you will find a “Create Submission” button on this page (just next to the tabs)</em></p>

<hr />
<p>The submissions will be received through the crowdai system.</p>

<p>Participants will be permitted to submit up to 10 runs. External training data is allowed and encouraged.</p>

<p>Each system run will consist of a single ASCII plain text file. The results of each test set should be given in separate lines in the text file. The format of the text file is as follows:</p>

<p>[image_ID/document_ID] [results]</p>

<p>The results of each test set image should be given in separate lines, each line providing only up to 500 localised substrates. The format has characters to separate the elements, semicolon ‘;’ for the substrates, colon ‘:’ for the confidence, comma ‘,’ to separate multiple bounding boxes, and ‘x’ and ‘+’ for the size-offset bounding box format, i.e.:</p>

<p>[image_ID];[substrate1] [[confidence1,1]:][width1,1]x[height1,1]+[xmin1,1]+[ymin1,1],[[confidence1,2]:][width1,2]x[height1,2]+[xmin1,2]+[ymin1,2],…;[substrate2] ..</p>

<p>[confidence] are floating point values 0-1 for which a higher value means a higher score.</p>

<p>For example, in the development set format (notice that there are 2 bounding boxes for substrate c_soft_coral):</p>

<ul>
  <li>2018_0714_112604_057 0 c_hard_coral_branching 1 891 540 1757 1143</li>
  <li>2018_0714_112604_057 3 c_soft_coral 1 2724 1368 2825 1507</li>
  <li>2018_0714_112604_057 4 c_soft_coral 1 2622 1576 2777 1731</li>
</ul>

<p>In the submission format, it would be a line as:</p>

<ul>
  <li>2018_0714_112604_057;c_hard_coral_branching 0.6:867x 604+891+540;c_soft_coral 0.7:102x140+2724+2825,0.3:156x156+2622+1576</li>
</ul>

<h3 id=""citations"">Citations</h3>

<p>Information will be posted after the challenge ends.</p>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>
<p>More on the evaluation criteria will be published soon.</p>

<h3 id=""resources"">Resources</h3>

<h3 id=""contact-us"">Contact us</h3>

<ul>
  <li>
    <p>Discussion Forum : <a href=""https://www.crowdai.org/challenges/imageclef-2019-coral-annotation-and-localisation/topics"" target=""_blank""> https://www.crowdai.org/challenges/imageclef-2019-coral-annotation-and-localisation/topics </a></p>
  </li>
  <li>
    <p>Join our mailing list: <a href=""https://groups.google.com/d/forum/imageclefcoral"" target=""_blank"">https://groups.google.com/d/forum/imageclefcoral</a></p>
  </li>
</ul>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

<ul>
  <li>
    <p>Jon Chamberlain &lt;jchamp(at)essex.ac.uk&gt;,University of Essex, UK</p>
  </li>
  <li>
    <p>Adrian Clark &lt;alien(at)essex.ac.uk&gt;,University of Essex, UK</p>
  </li>
  <li>
    <p>Antonio Campello &lt;antonio.campello(at)filament.ai&gt;,Filament, UK</p>
  </li>
  <li>
    <p>Alba García Seco de Herrera &lt;alba.garcia(at)essex.ac.uk&gt;,University of Essex, UK</p>
  </li>
</ul>

<h3 id=""more-information"">More information</h3>

<p>You can find additional information on the challenge here:
<a href=""https://www.imageclef.org/2019/coral"" target=""_blank""> https://www.imageclef.org/2019/coral </a></p>

<h3 id=""prizes"">Prizes</h3>

<p>ImageCLEF 2019 is an evaluation campaign that is being organized as part of the <a href=""http://clef2019.clef-initiative.eu/"" target=""_blank""> CLEF initiative </a> labs. The campaign offers several research tasks that welcome participation from teams around the world. The results of the campaign appear in the working notes proceedings, published by CEUR Workshop Proceedings (CEUR-WS.org). Selected contributions among the participants, will be invited for publication in the following year in the Springer Lecture Notes in Computer Science (LNCS) together with the annual lab overviews.</p>

<h3 id=""datasets-license"">Datasets License</h3>

"
145,"<h2 id=""agent"">Agent</h2>

<p>Flatland is a discrete time simulation, that means that all actions performed happen with a constant time step. At each step, the agents can choose an action. The term agent is defined as an entity that can move within the grid and must solve tasks - these agents are, who would have thought, trains. A train does basically two things: wait or go into a particular direction. Depending on the train type (e.g. freight train or passenger train), they have different speeds. An agent can move in any arbitrary direction (if the environment permits it) and transition from one cell to the next. If the agent chooses a valid action, the corresponding transition will be executed and the agent’s position and orientation is updated. Each agent has its individual start and target.</p>

<p>Agent at start:</p>

<p><img src=""https://i.imgur.com/mXW7O3L.png"" alt=""starting_agent"" /></p>

<p>Target Destination:</p>

<p><img src=""https://i.imgur.com/NiSEryT.png"" alt=""destination"" /></p>

<p>The cell where the agent is located at must have enough capacity to hold the agent on (thus a “blank” or already reserved cell is impossible). Every agent reserves exact one capacity or resource and since the capacity of a cell is maximal one, it can never hold more than one agent. The different cell-types are introduced in the next section.</p>

<h2 id=""grid-world"">Grid World</h2>

<p>As you know by now, the Flatland environment consists of cells that are arranged in a simulation grid. These cells have a tile type - for a railway specific problem, 8 basic tile types can be defined. These tile types determine where the agent can be located and how the agent can move through the cell. Here is a quick overview of the tile types:</p>

<p><img src=""https://i.imgur.com/geH2KOV.png"" alt=""Basic Tiles"" /></p>

<p><img src=""http://kidzinski.com/out.gif"" alt=""2 steps"" /></p>

<h2 id=""partners"">Partners</h2>

<p><img src=""https://s3.amazonaws.com/salathegroup-static/nips/logos/Stanford.png"" alt=""stanford"" class=""img-logo"" />
<img src=""https://s3.amazonaws.com/salathegroup-static/nips/logos/epfl.png"" alt=""epfl"" class=""img-logo"" />
<img src=""https://s3.amazonaws.com/salathegroup-static/nips/logos/Berkeley.png"" alt=""berkley"" class=""img-logo"" />
<a href=""http://mobilize.stanford.edu/""><img src=""https://s3.amazonaws.com/salathegroup-static/nips/logos/mobilize.png"" alt=""stanford mobilize"" class=""img-logo"" /></a></p>

<h2 id=""sponsors"">Sponsors</h2>

<p><a href=""https://aws.amazon.com/""><img src=""https://upload.wikimedia.org/wikipedia/commons/thumb/1/1d/AmazonWebservices_Logo.svg/2000px-AmazonWebservices_Logo.svg.png"" alt=""Amazon AWS"" class=""img-logo"" /></a>
<a href=""https://nvidia.com/""><img src=""https://vignette1.wikia.nocookie.net/logopedia/images/3/38/Nvidia_logo.png/revision/latest?cb=20120829072950"" alt=""NVIDIA"" class=""img-logo"" /></a>
<a href=""http://www.tri.global/""><img src=""https://s3-eu-west-1.amazonaws.com/kidzinski/nips-challenge/tri1.png"" alt=""TRI"" class=""img-logo"" /></a></p>

<h2 id=""media"">Media</h2>

<p><a href=""https://techcrunch.com/2017/08/07/dueling-ais-compete-in-learning-to-walk-secretly-manipulating-images-and-more-at-nips/""><img src=""https://seeklogo.com/images/T/techcrunch-logo-B444826970-seeklogo.com.png"" alt=""TechCrunch"" class=""img-logo"" /></a>
<a href=""http://news.stanford.edu/2017/08/07/virtual-competitors-vie-different-kind-athletic-title/""><img src=""https://cehg.stanford.edu/sites/default/files/styles/large-scaled/public/c876e3f31ce0c5ba771fbdccdcb3c1dc.png?itok=-83R2NJW"" alt=""Stanford News"" class=""img-logo"" /></a>
<a href=""http://insights.globalspec.com/article/6167/watch-computer-generated-skeletons-run-for-cerebral-palsy""><img src=""https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/50fa0a860a431c503132b1fa0cac8377_logo%20%283%29.png"" alt=""IEEE"" class=""img-logo"" /></a></p>

<p><img src=""https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/50fa0a860a431c503132b1fa0cac8377_logo%20%283%29.png"" alt=""logo (3).png"" /></p>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>

<h3 id=""resources"">Resources</h3>

<h3 id=""prizes"">Prizes</h3>

<h3 id=""datasets-license"">Datasets License</h3>

"
167,"<p><em>Note: ImageCLEF Lifelog 2019 is divided into 2 subtasks (challenges). This challenge is about the <strong>Puzzle</strong> task. For information on the Lifelog moment retrieval (<strong>LMRT</strong>) challenge click <a href=""/challenges/imageclef-2019-lifelog-lmrt"" target=""_blank""> here </a>. Both challenges share the same dataset, so registering for one of these challenges will automatically give you access to the other one.</em></p>

<p><em>Note: Do not forget to read the Rules section on this page.</em></p>

<h3 id=""motivation"">Motivation</h3>

<p>An increasingly wide range of personal devices, such as smartphones, video cameras as well as wearable devices that allow capturing pictures, videos, and audio clips for every moment of our lives are becoming available. Considering the huge volume of data created, there is a need for systems that can automatically analyse the data in order to categorize, summarize and also query to retrieve the information the user may need.</p>
<p>Despite the increasing number of successful related workshops and panels (<a href=""http://www.jcdl.org/archived-conf-sites/jcdl2015/www.jcdl2015.org/panels.html"" target=""_new""> JCDL 2015 </a>, <a href=""http://irlld2016.computing.dcu.ie/index.html"" target=""_new""> iConf 2016 </a>, <a href=""http://lta2016.computing.dcu.ie/styled/index.html"" target=""_new""> ACM MM 2016 </a>, <a href=""http://lta2017.computing.dcu.ie"" target=""_new""> ACM MM 2017 </a>, <a href=""http://lsc.dcu.ie"" target=""_new""> ICMR 2018 </a>) lifelogging has seldom been the subject of a rigorous comparative benchmarking exercise as, for example, the new lifelog evaluation task at <a href=""http://ntcir-lifelog.computing.dcu.ie"" target=""_new""> NTCIR-13 </a> or the last editions of the <a href=""http://www.imageclef.org/2018/lifelog"" target=""_new""> ImageCLEFlifelog </a> task. In this edition of this task we aim to bring the attention of lifelogging to an as wide as possible audience and to promote research into some of the key challenges of the coming years.
</p>

<h3 id=""challenge-description"">Challenge description</h3>

<p>Solve my life puzzle. Given a set of lifelog images with associated metadata (e.g., biometrics, location, etc.), but no timestamps, the participants need to analyse these images and rearrange them in chronological order and predict the correct day (Monday or Sunday) and part of the day (morning, afternoon, or evening). The dataset will be arranged in 75% training and 25% test data.</p>

<h3 id=""schedule"">Schedule</h3>

<ul>
<li><b>05.11.2018</b>: Registration opens</li><!-- (<a href=""http://imageclef.org/2017#registration"">Register here</a>).</li>-->
<li><b>26.11.2018</b>: Development data release</li>
<li><b>18.03.2019</b>: Test data release</li>
<li><b>26.04.2019</b>: Registration closes</li>
<!--<li><b>10.05.2019 (firm)</b> <strike>01.05.2019</strike>: Deadline for submission of runs by the participants 11:59:59 PM GMT.</li>-->
<li><del>01.05.2019</del><b>15.05.2019</b>: Deadline for submission of runs by the participants 11:59:59 PM GMT.</li>
<!--<li><b>20.05.2019</b> <strike>17.05.2019</strike>: Release of processed results by the task organizers.</li>-->
<li><b>17.05.2019</b>: Release of processed results by the task organizers.</li>
<li><b>24.05.2019</b>: Deadline for submission of working notes papers by the participants</li>
<li><b>14.06.2019</b>: Notification of acceptance of the working notes papers.</li>
<li><b>28.06.2019</b>: Camera ready working notes papers.</li>
<li><b>09.-12.09.2019</b>: <a href=""http://clef2019.clef-initiative.eu"">CLEF 2019</a>, Lugano, Switzerland </li>
<li><b>07.02.2019</b>: Development queries released and task descriptions revised!</li>
<li><b>21.03.2019</b>: Test data is delayed to be released by 31.03.2019</li>
<li><b>29.04.2019</b>: Deadline of CrowAI submission has been postponed to May 15th at 12:00 UTC</li>
</ul>

<h3 id=""data"">Data</h3>

<p>The task will be split into two related subtasks using a completely new rich multimodal dataset which consists of 29 days of data from one lifelogger, namely: images (1,500-2,500 per day from wearable cameras), visual concepts (automatically extracted visual concepts with varying rates of accuracy), semantic content (semantic locations, semantic activities) based on sensor readings (via the Moves App) on mobile devices, biometrics information (heart rate, galvanic skin response, calorie burn, steps, continual blood glucose, etc.), music listening history, computer usage (frequency of typed words via the keyboard and information consumed on the computer via ASR of on-screen activity on a per-minute basis). Detailed description about the meta data is <a href=""https://www.imageclef.org/system/files/Imageclef%202019-meta-data-description-2.pdf"">here</a>!</p>

<p>The development queries can be downloaded <a href=""https://drive.google.com/open?id=1LSnZBwpUZCA4HdHHHTXIwCY95SfxeH8e"">here</a>. The ground truth of this development queries can be downloaded <a href=""https://drive.google.com/open?id=1ZReyylhunjDDtjgDV60OQHdBuf1YTUCi"">here</a>.</p>

<h3 id=""submission-instructions"">Submission instructions</h3>

<hr />
<p><em>As soon as the submission is open, you will find a “Create Submission” button on this page (just next to the tabs)</em></p>

<hr />
<p>Participants will be permitted to submit up to 10 runs.</p>

<p>Each system run will consist of a single ASCII plain text file.</p>

<p>The results of each run should be given in separate lines in the text file.</p>
<p>A submitted run for the Puzzle task must be in the form of a text file in the following format:</p>
<p align=""center"">[query_id, image id, order, part of the day]</p>
<p>Where: </p>
<ul>
        <li>query id: The id of the query that participant want to answer.</li>
	<li>image id: Each image id is mapped to a specific order and should be predicted which part of the day that the image belongs to.</li>
	<li>order: The answer of the task after the moments are arranged. The indices are from 1 to N (N is the number of images in the query). For each query, there should not be any image id with the same index.</li>
	<li>part of the day: it should be morning, afternoon, evening, or night. The answer must be the index of the part of the day as follows: morning - 1 (4h00 AM to 11h59 AM), afternoon - 2 (12h00 PM to 4h59 PM) , evening - 3 (5h00 PM to 10h59 PM), night - 4 (11h00 PM to 3h59 AM). </li>
</ul>

<h3 id=""note-please-sort-the-results-by-the-query-id-then-the-imageid"">Note: Please sort the results by the query id, then the image_id.</h3>

<p>Sample: </p>
<p><code>
001, 001.JPG, 5, 2<br />
001, 002.JPG, 1, 1<br />
 …<br />
010, 025.JPG, 23, 3<br />
</code></p>

<h3 id=""submission-files"">Submission files</h3>
<p>The file name must be followed the rule &lt;task abbreviation&gt;_&lt;team name without spaces&gt;_&lt;run name without spaces&gt;.csv</p>
<p>Examples: </p>
<p>- PUZZLE_DCU_run1.csv</p>

<h3 id=""citations"">Citations</h3>

<p>Information will be posted after the challenge ends.</p>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>
<p>We use Kendall rank correlation coefficient to evaluate the similarity between the participant's arrangement and the ground truth. For more information, please refer to this <a href=""https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient"">link</a>.</p>
<p> The final score of this task is calculated by the mean of the accuracy of the prediction of which part of the day the image belongs to and the Kendall's Tau coefficient.</p>

<h3 id=""resources"">Resources</h3>

<h3 id=""contact-us"">Contact us</h3>

<ul>
  <li>Discussion Forum : <a href=""https://www.crowdai.org/challenges/imageclef-2019-lifelog-puzzle/topics"" target=""_blank""> https://www.crowdai.org/challenges/imageclef-2019-lifelog-puzzle/topics </a></li>
</ul>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

<ul>
<li><a href=""https://www.uib.no/en/persons/Duc.Tien.Dang.Nguyen"" target=""_new"">Duc-Tien Dang-Nguyen</a> <b>&lt;ductien.dangnguyen(at)uib.no&gt;</b>, University of Bergen, Norway</li>
<li><a href=""https://pralab.diee.unica.it/en/LucaPiras"" target=""_new"">Luca Piras</a> <b>&lt;luca.piras(at)diee.unica.it&gt;</b>, University of Cagliari, Cagliari, Italy </li>
<li> <a href=""https://www.simula.no/people/michael"" target=""_new"">Michael Riegler</a> <b>&lt;michael(at)simula.no&gt;</b>, University of Oslo, Norway</li>
<li><a href=""http://www.fit.hcmus.edu.vn/~tmtriet/"" target=""_new"">Minh-Triet Tran</a> <b>&lt;tmtriet(at)hcmus.edu.vn&gt;</b>, University of Science, Ho Chi Minh City, Vietnam </li>
<li><a href=""http://www.itec.uni-klu.ac.at/~mlux/"" target=""_new"">Mathias Lux</a> <b>&lt;mlux(at)itec.aau.at&gt;</b>, Klagenfurt University, Austria</li>
<li> <a href=""http://www.computing.dcu.ie/~cgurrin/"" target=""_new"">Cathal Gurrin</a> <b>&lt;cgurrin(at)computing.dcu.ie&gt;</b>, Dublin City University, Ireland </li>
</ul>

<h3 id=""more-information"">More information</h3>

<p>You can find additional information on the challenge here:
<a href=""https://www.imageclef.org/2019/lifelog"" target=""_blank""> https://www.imageclef.org/2019/lifelog </a></p>

<h3 id=""prizes"">Prizes</h3>

<p>ImageCLEF 2019 is an evaluation campaign that is being organized as part of the <a href=""http://clef2019.clef-initiative.eu/"" target=""_blank""> CLEF initiative </a> labs. The campaign offers several research tasks that welcome participation from teams around the world. The results of the campaign appear in the working notes proceedings, published by CEUR Workshop Proceedings (CEUR-WS.org). Selected contributions among the participants, will be invited for publication in the following year in the Springer Lecture Notes in Computer Science (LNCS) together with the annual lab overviews.</p>

<h3 id=""datasets-license"">Datasets License</h3>

"
146,"<p>Ant societies have long fascinated mankind by their resemblance to human societies and elaborate collective behaviour. Ants cultivate and farm fungus in their nests, herd aphids as cattle and wage war with each other.</p>

<p>In contrast to human societies, ants present an ideal study system to study how collective patterns emerge through self-organization and how societies respond to perturbations in their environment. We can easily manipulate crowding, resource availability and parasite pressure, thereby allowing us to study questions that would be impossible to answer in humans. One of the most interesting collective dynamics in a colony  is the trade-off between fast transmission of food and how ants avoid the outbreak of diseases.</p>

<p>To better understand transmission dynamics and the underlying socio-ecological drivers, we have to analyze ant behaviour across time and space. The goal of this challenge is to generate algorithms that can:</p>

<ul>
  <li>identify and track individual ants over time</li>
  <li>recognize when ants engage in food transfer</li>
</ul>

<p>In the first part of this challenge, we will focus on the task of <strong>identification and tracking of individual ants over time</strong>. The training data provides the coordinates of all the ants for a subset of the time frames, and the goal of the challenge is to <strong>predict the coordinates of all the ants for the rest of the time frames</strong>.</p>

<p><img src=""https://i.imgur.com/KAR8d4v.jpg"" alt=""image_1"" />
(Image 1)</p>

<p>To identify individuals in the colony, we have attached a unique barcode (as described in http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0136487 <em>Crall JD, Gravish N, Mountcastle AM, Combes SA (2015) BEEtag: A Low-Cost, Image-Based Tracking System for the Study of Animal Behavior and Locomotion. PLoS ONE 10(9): e0136487. doi:10.1371/journal.pone.0136487</em> ) to the <strong>gaster</strong> of each worker (see image 1).</p>

<p>The only individual that was not marked was the queen, since her distinct appearance (larger body size) makes her very easy to identify, and moreover, we did not want to stress or injure her. Students have manually tracked several hours of ant movement inside a colony allowing us to determine the location of each worker for each second of video footage.</p>

<p>To ensure that the student’s tracking data is as exact and repeatable as possible, we asked the students to track the neck area of each ant (see image 2), because it is easily recognisable and relatively small leading to less variation among the students’ tracking data. In particular, each student tracked a focal ant by clicking on its neck area in each screenshot. To speed up this process, we used a customised Python code to record the x- and y-coordinates that were generated by the students’ clicking automatically. Thus we were able to obtain x and y coordinates (unit is pixels) for the location of all individuals at each second of the observation period. In instances when the ant neck was not visible, e.g., when an ant was underneath another ant, we estimated the neck position based on the last known location and the other visible body parts of this ant. If an ant left the nest, its coordinates were set to x = 0, y = 0.</p>

<p><img src=""https://i.imgur.com/XRJtWOC.jpg"" alt=""image_2"" />
(Image 2)</p>

<p>Challenge Logo Source : https://www.flickr.com/photos/98180998@N04/9187248340/</p>

<p>The training data provides the coordinates of all the ants for a subset of the time frames, and the goal of the challenge is to predict the coordinates of all the ants for the rest of the time frames.</p>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>

<p>The overall error (or Loss) functions are defined as : 
<img src=""https://i.imgur.com/XZLaN1i.png"" alt="""" /></p>

<ul>
  <li>$i$ refers to the ant-id</li>
  <li>$t$ refers to the frame-id</li>
  <li>$I$ refers to the highest ant-id we have in the test set</li>
  <li>$\tau$ refers to the lowest time-frame-id we have in the test set</li>
  <li>$T$ refers to the highest time-frame-id we have in the test set</li>
  <li>$X_{it}$ refers to the actual x-coordinate of the neck of the i-th ant at time-frame t</li>
  <li>$Y_{it}$ refers to the actual y-coordinate of the nect of the j-th ant at the timeframe t</li>
  <li>$x_{it}$ refers to the predicted x-coordinate of the neck of the i-th ant at time-frame t</li>
  <li>$y_{it}$ refers to the predicted y-coordinate of the nect of the j-th ant at the timeframe t</li>
  <li>$ɣ$ refers to the mean-squared error threshold required for valid predictions, its value is 100</li>
  <li>$\beta$ refers to the boolean value representing if the ant with ant-id $i$ is inside the frame with frame-id $t$ which uses the integral stand-in of 0 or 1 depending on when the variable is False or True.</li>
</ul>

<p>The goal of the challenge is to minimise this error function/loss $L_1$ (which approximates the overall accuracy), and if multiple participants have the same $L_1$ score, then they will be evaluated and ranked based on $L_2$ (which is the mean squared error).</p>

<p>All submissions will be evaluated on the test dataset in the docker containers referenced in the Resources section. The code archive will be uncompressed into the <code class=""highlighter-rouge"">/ants</code> path, and every code archive is expected to contain a <code class=""highlighter-rouge"">main.sh</code> script which takes path to a folder containing frame images that will be tested, as its first parameter. So to test your code submission, we will finally execute :</p>

<p><code class=""highlighter-rouge"">
/ants/main.sh pathToFolderContainingTestFrameImages
</code></p>

<p>This is expected to output a CSV file containing the name of the file, and the associated probabilities for all the classes at the location :</p>

<p><code class=""highlighter-rouge"">
/ants/predictions.csv
</code></p>

<h3 id=""resources"">Resources</h3>

<p>The <a href=""https://www.crowdai.org/challenges/ants-challenge-part-1/dataset_files"">challenge dataset</a> contains 6 files, including:</p>

<ul>
  <li><strong>frames.tar.gz.part-1</strong></li>
  <li><strong>frames.tar.gz.part-2</strong></li>
  <li><strong>frames.tar.gz.part-3</strong></li>
</ul>

<p>Which can be concatenated into a single file using :  <br />
<code class=""highlighter-rouge"">cat frames.tar.gz.part-1 frames.tar.gz.part-2 frames.tar.gz.part-3 &gt; frames.tar.gz</code></p>

<p><strong>frames.tar.gz</strong> : a tarball of a folder which has separate images for all frames, with the name convention “frame-id”.jpeg (Note: the frame-id is an integer)</p>

<ul>
  <li><strong>train.csv</strong>: A CSV file holding the training dataset with the following columns :
    <ul>
      <li><strong>ant_id</strong> : Holds the ant-id (as can be read from the barcode on the ant)</li>
      <li><strong>frame_id</strong> : The frame_id associated with the particular data point</li>
      <li><strong>x-coord</strong> : the X-Coordinate of the Neck of the Ant (or 0, if the Ant is not in the frame)</li>
      <li><strong>y-coord</strong>: the Y-Coordinate of the Neck of the Ant (or 0, if the Ant is not in the frame)</li>
      <li><strong>is-in-frame?</strong> : holds 1 or 0 depending on if the Ant if present in the frame or not.</li>
    </ul>
  </li>
  <li><strong>test.csv</strong>: A CSV file holding the testing dataset with the following columns:
    <ul>
      <li><strong>ant_id</strong>: Holds the ant-id (as can be read from the barcode on the ant)</li>
      <li><strong>frame_id</strong>: The frame_id associated with the particular data point</li>
    </ul>
  </li>
  <li><strong>sample_prediction.csv</strong>: A sample prediction file in the format that we expect the participants to submit their results. The columns are the same as the train.csv file.</li>
</ul>

<h2 id=""coding-environments"">Coding Environments</h2>

<p>The code must be runnable in one of these Docker containers:</p>

<p><strong>Caffe</strong>  : https://hub.docker.com/r/tleyden5iwx/caffe-gpu-master/</p>

<p><strong>Tensorflow</strong> : https://hub.docker.com/r/tensorflow/tensorflow/</p>

<p><strong>Torch7</strong> : https://hub.docker.com/r/kaixhin/cuda-torch/</p>

<p><strong>Scikit-Learn</strong> :(Python-2): https://github.com/dataquestio/ds-containers/tree/master/python2</p>

<p><strong>Scikit-Learn</strong> : (Python-3): https://github.com/dataquestio/ds-containers/tree/master/python3</p>

<p><strong>Octave</strong> : https://hub.docker.com/r/schickling/octave/</p>

<p><strong>Keras</strong> :  https://hub.docker.com/r/patdiscvrd/keras/~/dockerfile/</p>

<p>If you wish to use another coding environment please <a href=""https://www.crowdai.org/pages/contact"">contact us</a>.</p>

<h3 id=""prizes"">Prizes</h3>

<h4 id=""applied-machine-learning-days-at-epfl"">Applied Machine Learning Days at EPFL</h4>

<p>The author of the most highly ranked submission will be invited to the <a href=""https://www.appliedmldays.org/"">Applied Machine Learning Days</a> symposium at <a href=""https://www.epfl.ch/"">EPFL</a> in Switzerland on January 30/31, 2017. This educational award is granted to the participant with the either the most insightful submission posts, or the best tutorial. Expenses for travel and accommodation are covered by crowdAI.</p>

<p>The submission needs to have at least an L_1 score of 80% to be eligible for this prize.</p>

<h4 id=""monetary-prize"">Monetary Prize</h4>

<p>There is also a monetary prize for the most highly ranked submission, which is determined as follows :</p>

<p><strong>$500</strong> if the L_1 score of the best submission reaches &gt;= 80%, but less than 85%
<strong>$1000</strong> if the L_1 score of the best submission reaches &gt;= 85%, but less than 90%
<strong>$2000</strong> if the L_1 score of the best submission reaches &gt;= 90%, but less than 95%
<strong>$5000</strong> if the L_1 score of the best submission reaches &gt;= 95%</p>

<h3 id=""datasets-license"">Datasets License</h3>

<p>All data is released under the Creative Commons Attribution-ShareAlike 3.0 Unported (CC BY-SA 3.0), with the clarification that algorithms trained on the data fall under the same license. All the observation frames can be downloaded at : <a href=""https://s3.amazonaws.com/salathegroup-static/antchallenge/frames.tar.gz"">https://s3.amazonaws.com/salathegroup-static/antchallenge/frames.tar.gz</a></p>
"
168,"<p><em>Note: ImageCLEF Lifelog 2019 is divided into 2 subtasks (challenges). This challenge is about Lifelog moment retrieval (<strong>LMRT</strong>). For information on the  <strong>Puzzle</strong> challenge click <a href=""/challenges/imageclef-2019-lifelog-puzzle"" target=""_blank""> here </a>. Both challenges share the same dataset, so registering for one of these challenges will automatically give you access to the other one.</em></p>

<p><em>Note: Do not forget to read the Rules section on this page.</em></p>

<h3 id=""motivation"">Motivation</h3>

<p>An increasingly wide range of personal devices, such as smartphones, video cameras as well as wearable devices that allow capturing pictures, videos, and audio clips for every moment of our lives are becoming available. Considering the huge volume of data created, there is a need for systems that can automatically analyse the data in order to categorize, summarize and also query to retrieve the information the user may need.</p>
<p>Despite the increasing number of successful related workshops and panels (<a href=""http://www.jcdl.org/archived-conf-sites/jcdl2015/www.jcdl2015.org/panels.html"" target=""_new""> JCDL 2015 </a>, <a href=""http://irlld2016.computing.dcu.ie/index.html"" target=""_new""> iConf 2016 </a>, <a href=""http://lta2016.computing.dcu.ie/styled/index.html"" target=""_new""> ACM MM 2016 </a>, <a href=""http://lta2017.computing.dcu.ie"" target=""_new""> ACM MM 2017 </a>, <a href=""http://lsc.dcu.ie"" target=""_new""> ICMR 2018 </a>) lifelogging has seldom been the subject of a rigorous comparative benchmarking exercise as, for example, the new lifelog evaluation task at <a href=""http://ntcir-lifelog.computing.dcu.ie"" target=""_new""> NTCIR-13 </a> or the last editions of the <a href=""http://www.imageclef.org/2018/lifelog"" target=""_new""> ImageCLEFlifelog </a> task. In this edition of this task we aim to bring the attention of lifelogging to an as wide as possible audience and to promote research into some of the key challenges of the coming years.
</p>

<h3 id=""challenge-description"">Challenge description</h3>

<p>
This sub-task follows the success of the LMRT sub-task in ImageCLEFlifelog2018 with some minor adjustments. The participants have to retrieve a number of specific predefined activities in a lifelogger's life. For example, they should return the relevant moments for the query “Find the moment(s) when I was shopping”. Particular attention should be paid to the diversification of the selected moments with respect to the target scenario. The ground truth for this subtask was created using manual annotation. </p>

<h3 id=""schedule"">Schedule</h3>

<ul>
<li><b>05.11.2018</b>: Registration opens</li><!-- (<a href=""http://imageclef.org/2017#registration"">Register here</a>).</li>-->
<li><b>26.11.2018</b>: Development data release</li>
<li><b>18.03.2019</b>: Test data release</li>
<li><b>26.04.2019</b>: Registration closes</li>
<!--<li><b>10.05.2019 (firm)</b> <strike>01.05.2019</strike>: Deadline for submission of runs by the participants 11:59:59 PM GMT.</li>-->
<li><b>01.05.2019 </b>: Deadline for submission of runs by the participants 11:59:59 PM GMT.</li>
<!--<li><b>20.05.2019</b> <strike>17.05.2019</strike>: Release of processed results by the task organizers.</li>-->
<li><b>17.05.2019</b>: Release of processed results by the task organizers.</li>
<li><b>24.05.2019</b>: Deadline for submission of working notes papers by the participants</li>
<li><b>14.06.2019</b>: Notification of acceptance of the working notes papers.</li>
<li><b>28.06.2019</b>: Camera ready working notes papers.</li>
<li><b>09.-12.09.2019</b>: <a href=""http://clef2019.clef-initiative.eu"">CLEF 2019</a>, Lugano, Switzerland </li>
<li><b>07.02.2019</b>: Development queries released and task descriptions revised!</li>
<li><b>08.02.2019</b>: Development PUZZLE dataset and LMRT ground truth are released!</li>
<li><b>21.03.2019</b>: Test data is delayed to be released by 31.03.2019</li>
<li><b>04.04.2019</b>: Development LMRT groundtruth and clusters have been revised and updated!!! Please check the updated one!!!</li>
<li><b>05.04.2019</b>: Ten new test topics have also been revised and updated! </li>
<li><b>18.04.2019</b>: The evaluation system is ready to use in CrowAI! </li>
<li><b>25.04.2019</b>: Notification of the meaning of released LMRT ground truth format </li>
<li><b>29.04.2019</b>: Deadline of CrowAI submission has been postponed to May 15th at 12:00 UTC </li>
</ul>

<h3 id=""data"">Data</h3>

<p>
The task will be split into two related subtasks using a completely new rich multimodal dataset which consists of 29 days of data from one lifelogger, namely: images (1,500-2,500 per day from wearable cameras), visual concepts (automatically extracted visual concepts with varying rates of accuracy), semantic content (semantic locations, semantic activities) based on sensor readings (via the Moves App) on mobile devices, biometrics information (heart rate, galvanic skin response, calorie burn, steps, continual blood glucose, etc.), music listening history, computer usage (frequency of typed words via the keyboard and information consumed on the computer via ASR of on-screen activity on a per-minute basis). Detailed description about the meta data is <a href=""https://www.imageclef.org/system/files/Imageclef%202019-meta-data-description-2.pdf"">here</a>!
</p>

<h3 id=""topics-and-ground-truth-release"">Topics and Ground Truth Release</h3>

<p>There are <a href=""https://www.imageclef.org/system/files/ImageClef2019_LMRT%20topics.pdf"">10 dev topics</a> for LMRT Tasks, like linked! The <a href=""https://www.imageclef.org/system/files/clusters_0.txt"">clusters</a> and <a href=""https://www.imageclef.org/system/files/LMRT_gt_0.txt"">ground truth</a> for these 10 topics are here! </p>

<p>
<b>Notice</b>: the third column of ground truth is [topic id, image id, cluster id], which is different from the one of submission instruction [topic id, image id, confidence score]. The meaning of cluster id is to measure the diversity of the retrieved results for each topic. Participants should follow the submission instruction to generate the correct format of submission file.

<a href=""https://www.imageclef.org/system/files/ImageClef2019%20test_topics%20%281%29.pdf"">New 10 test topics</a> for LMRT are revised and upgraded! 
 </p>

<h3 id=""submission-instructions"">Submission instructions</h3>

<p>The submissions will be received through the ImageCLEF 2019 system. Go to ""Runs"", then ""Submit run"", and then select the track.</p>

<p>Participants will be permitted to submit up to 10 runs. </p>
<p>Each system run will consist of a single ASCII plain text file. <!--Before submitting the run files, you are expected to pre-validate their format using the automated format check tool provided by the organizers. You can access it <a href=""#Verification"">here</a>. Upon the passing of the checks, please add to your run file name the tag ""OK"". This will allow us to know that everything is OK with the format. -->The results of each run should be given in separate lines in the text file. The format of the text file is as follows:</p>

<p>A submitted run for the LMRT sub-task must be in the form of a text file in the following format:</p>
<p align=""center"">[topic id, image id, confidence score]</p>
<p>Where: </p>
<ul>
      <li>topic id: Number of the queried topic, e.g., from 1 to 10 for the development set.</li>
      <li>image id: The image ID that answers the topic. Each image ID is mapped into moments. If there are more than one sequential images that answer the topic (i.e. the moment is more than one image in duration), then any image from within that moment is acceptable.</li>
      <li>confidence score: from 0 to 1.</li>
</ul>
<p>Sample: </p>
<p><code>
1, u1_2015-02-26_095916_1, 1.00<br />
1, u1_2015-02-26_095950_2, 1.00<br />
1, u1_2015-02-26_100028_1, 1.00<br />
...<br />
10, u3_2015-08-01_144854_1, 1.00<br />
10, u3_2015-08-01_145314_1, 1.00<br />
10, u3_2015-08-01_145345_2, 1.00<br />
10, u3_2015-08-01_145531_1, 0.80<br />
</code></p>

<h3 id=""submission-files"">Submission files</h3>
<p>The file name must be followed the rule &lt;task abbreviation&gt;_&lt;team name without spaces&gt;_&lt;run name without spaces&gt;.csv</p>
<p>Examples: </p>
<p>- LMRT_DCU_run1.csv</p>

<h3 id=""citations"">Citations</h3>

<p>Information will be posted after the challenge ends.</p>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>
<p>For each subtask, the final score is computed as an arithmetic mean of all queries. For each query, the evaluation method is applied as follows:</p>

<p>For assessing performance, classic metrics will be deployed. These metrics are:
<ul>
<li> Cluster Recall at X (CR@X) - a metric that assesses how many different clusters from the ground truth are represented among the top X results;</li>
<li> Precision at X (P@X) - measures the number of relevant photos among the top X results;</li>
<li> F1-measure at X (F1@X) - the harmonic mean of the previous two.</li>
</ul>
<p>Various cut off points are to be considered, e.g., X=5, 10, 20, 30, 40, 50. Official ranking metrics this year will be the <b>F1-measure@10</b>, which gives equal importance to diversity (via CR@10) and relevance (via P@10).</p>
<p>Participants are allowed to undertake the sub-tasks in an interactive or automatic manner. For interactive submissions, a maximum of five minutes of search time is allowed per topic. In particular, the organizers would like to emphasize methods that allow interaction with real users (via Relevance Feedback (RF), for example), i.e., beside of the best performance, the way of interaction (like number of iterations using RF), or innovation level of the method (for example, new way to interact with real users) are encouraged.</p>

###Resources

###Contact us

- Discussion Forum : [ https://www.crowdai.org/challenges/challenges/imageclef-2019-lifelog-lmrt/topics ](https://www.crowdai.org/challenges/imageclef-2019-lifelog-lmrt/topics){:target='_blank'}

We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :


<ul>
<li><a href=""https://www.uib.no/en/persons/Duc.Tien.Dang.Nguyen"" target=""_new"">Duc-Tien Dang-Nguyen</a> <b>&lt;ductien.dangnguyen(at)uib.no&gt;</b>, University of Bergen, Norway</li>
<li><a href=""https://pralab.diee.unica.it/en/LucaPiras"" target=""_new"">Luca Piras</a> <b>&lt;luca.piras(at)diee.unica.it&gt;</b>, University of Cagliari, Cagliari, Italy </li>
<li> <a href=""https://www.simula.no/people/michael"" target=""_new"">Michael Riegler</a> <b>&lt;michael(at)simula.no&gt;</b>, University of Oslo, Norway</li>
<li><a href=""http://www.fit.hcmus.edu.vn/~tmtriet/"" target=""_new"">Minh-Triet Tran</a> <b>&lt;tmtriet(at)hcmus.edu.vn&gt;</b>, University of Science, Ho Chi Minh City, Vietnam </li>
<li><a href=""http://www.itec.uni-klu.ac.at/~mlux/"" target=""_new"">Mathias Lux</a> <b>&lt;mlux(at)itec.aau.at&gt;</b>, Klagenfurt University, Austria</li>
<li> <a href=""http://www.computing.dcu.ie/~cgurrin/"" target=""_new"">Cathal Gurrin</a> <b>&lt;cgurrin(at)computing.dcu.ie&gt;</b>, Dublin City University, Ireland </li>
</ul>

### More information

You can find additional information on the challenge here:
[ https://www.imageclef.org/2019/lifelog ](https://www.imageclef.org/2019/lifelog){:target='_blank'}

###Prizes

ImageCLEF 2019 is an evaluation campaign that is being organized as part of the [ CLEF initiative ](http://clef2019.clef-initiative.eu/){:target='_blank'} labs. The campaign offers several research tasks that welcome participation from teams around the world. The results of the campaign appear in the working notes proceedings, published by CEUR Workshop Proceedings (CEUR-WS.org). Selected contributions among the participants, will be invited for publication in the following year in the Springer Lecture Notes in Computer Science (LNCS) together with the annual lab overviews.

###Datasets License

</p>
"
169,"<p><em>Note: ImageCLEF Security 2019 is divided into 3 subtasks (challenges):</em></p>

<ul>
  <li>
    <p><em>Task 1: Forged File Discovery</em></p>
  </li>
  <li>
    <p><em>Task 2: Stego Image Discovery</em></p>
  </li>
  <li>
    <p><em>Task 3: Secret Message Discovery</em></p>
  </li>
</ul>

<p><em>This challenge is about <strong>Forged File Discovery</strong>(task 1). For information on the <strong>Stego Image Discovery</strong>(task 2) challenge click <a href=""/challenges/imageclef-2019-security-stego-image-discovery"" target=""_blank""> here </a>. For information on the <strong>Secret Message Discovery</strong>(task 3) challenge click <a href=""/challenges/imageclef-2019-security-secret-message-discovery"" target=""_blank""> here </a>. All of these challenges share the same scenario. Registering for one of these challenges will automatically give you access to the other ones.</em></p>

<p><em>Note: Do not forget to read the Rules section on this page.</em></p>

<h3 id=""motivation"">Motivation</h3>

<p>File Forgery Detection (FFD) is a serious problem concerning digital forensics examiners. Fraud or counterfeits are common causes for altering files. Another example is a child predator who hides porn images by altering the image extension and in some cases by changing the image signature. Many proposals have been made to solve this problem and the most promising ones concentrate on the image content. It is also common that anyone who wants to hide any kind of information in plain sight without being perceived to use steganography. Steganography is the practice of concealing a file, message, image or video within another file, message, image, or video. The word steganography combines the Greek words steganos (στεγανός), meaning “covered” and graphein (γράφειν) meaning “writing”. The most usual cover medium for hiding data are images.</p>

<h3 id=""challenge-description"">Challenge description</h3>

<p>You are a professional digital forensic examiner collaborating with the police, who suspects that there is an ongoing fraud in the Central Bank. After obtaining a court order, police gain access to a suspect’s computer in the bank with the purpose to look for images proving the suspect guilty. However, police suspects that he has managed to change extension and signature of some images, so that they look like pdf files. Additionally, it is highly probable that the suspect has used steganography software to hide messages within some images that could reveal valuable information of his collaborators.</p>

<p>The goal of this challenge is to examine if an image has been forged. Perform detection of altered (forged) images (both extension and signature) and predict the actual type of the forged file.</p>

<h3 id=""data"">Data</h3>

<p>Training set for  forged file discovery (i.e.task 1) consists of 2400 files. 1200 of them are true pdf files and the rest seem to be pdf files but actually they are images (400 of each image type i.e. jpg , png and gif).</p>

<h3 id=""submission-instructions"">Submission instructions</h3>

<hr />
<p><em>As soon as the submission is open, you will find a “Create Submission” button on this page (just next to the tabs)</em></p>

<hr />
<p><strong>Identify Forged Images</strong></p>

<p>For the submission of the task we expect the following format:
&lt;Figure-ID&gt;;&lt;initial Image type&gt;</p>

<p>e.g.:</p>

<blockquote>
  <p>1741_01;jpg     if the document classified as a forged one, initially jpg file</p>
</blockquote>

<blockquote>
  <p>1742_01;pdf     if the document classified as a NO forged one</p>
</blockquote>

<blockquote>
  <p>1743_01;png    if the document classified as a forged one, initially png file</p>
</blockquote>

<p><strong><em>You need to respect the following constraints:</em></strong></p>

<ul>
  <li>
    <p>The separator between the figure ID and the concepts has to be a semicolon (;).</p>
  </li>
  <li>
    <p>The file to upload must be a .txt file.</p>
  </li>
  <li>
    <p>The initially  images can be jpg or gif or png or pdf.</p>
  </li>
  <li>
    <p>Each figure ID of the test set must be included in the runfile exactly once (even if there is no result).</p>
  </li>
  <li>
    <p>The result cannot be specified more than once for the same figure ID.</p>
  </li>
</ul>

<h3 id=""citations"">Citations</h3>

<p>Information will be posted after the challenge ends.</p>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>
<p>For assessing performance, classic metrics are used:
Precision, Recall and F1.</p>

<p><strong>Precision</strong>
In pattern recognition, information retrieval and binary classification, precision is the fraction of relevant instances among the retrieved instances.
Precision could be defined as the fraction of actual detected altered images among all the images detected as altered:</p>

<p>Precision = nº of actual detected altered images /Total detections of altered images</p>

<p><strong>Recall</strong>
In pattern recognition, information retrieval and binary classification, recall is the fraction of relevant instances that have been retrieved over the total amount of relevant instances.
Recall could be defined as the fraction of actual detected altered images among all the altered images:</p>

<p>Recall = nº of actual detected altered images /Total altered images</p>

<p><strong>F-measure</strong>
F-measure is the harmonic mean of precision and recall, mathematically expressed as</p>

<p>F_1=2∙(Precision ∙ Recall)/(Precision + Recall )</p>

<h3 id=""resources"">Resources</h3>

<h3 id=""contact-us"">Contact us</h3>

<ul>
  <li>Discussion Forum : <a href=""https://www.crowdai.org/challenges/imageclef-2019-security-forged-file-discovery/topics"" target=""_blank""> https://www.crowdai.org/challenges/imageclef-2019-security-forged-file-discovery/topics </a></li>
</ul>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

<ul>
  <li>
    <p>Narciso Garcia, Professor, Dr., Grupo de Tratamiento de Imágenes, Dpto. Señales, Sistemas y Radiocomunicaciones, E.T.S. Ingenieros Telecomunicación, Spain, narciso@gti.ssr.upm.es</p>
  </li>
  <li>
    <p>Ergina Kavallieratou, Associate Professor, Dr, AIlab, Department of Information &amp; Communication Systems Engineering, University of the Aegean, Greece, kavallieratou@aegean.gr</p>
  </li>
  <li>
    <p>Carlos Roberto del Blanco, Assistant Professor, Dr., Grupo de Tratamiento de Imágenes, Dpto. Señales, Sistemas y Radiocomunicaciones, E.T.S. Ingenieros de Telecomunicación, cda@gti.ssr.upm.es</p>
  </li>
  <li>
    <p>Carlos Cuevas Rodríguez, Assistant Professor, Dr., Grupo de Tratamiento de Imágenes, Dpto. Señales, Sistemas y Radiocomunicaciones, E.T.S. Ingenieros de Telecomunicación, Spain, ccr@gti.ssr.upm.es</p>
  </li>
  <li>
    <p>Nikos Vasillopoulos, Phd, Postdoc, AIlab, Department of Information &amp; Communication Systems Engineering, University of the Aegean, Greece, 
~~~ 
nvasilopoulos@aegean.gr</p>
  </li>
  <li>
    <p>Konstantinos Karampidis, Msc, Phd student, University of the Aegean, Greece, karampidis@aegean.gr</p>
  </li>
</ul>

<p><strong>For questions over the Security task e-mail: Imageclefsecurity@aegean.gr</strong></p>

<h3 id=""more-information"">More information</h3>

<p>You can find additional information on the challenge here:
<a href=""https://www.imageclef.org/2019/security"" target=""_blank""> https://www.imageclef.org/2019/security </a></p>

<h3 id=""prizes"">Prizes</h3>

<p>ImageCLEF 2019 is an evaluation campaign that is being organized as part of the <a href=""http://clef2019.clef-initiative.eu/"" target=""_blank""> CLEF initiative </a> labs. The campaign offers several research tasks that welcome participation from teams around the world. The results of the campaign appear in the working notes proceedings, published by CEUR Workshop Proceedings (CEUR-WS.org). Selected contributions among the participants, will be invited for publication in the following year in the Springer Lecture Notes in Computer Science (LNCS) together with the annual lab overviews.</p>

<h3 id=""datasets-license"">Datasets License</h3>

"
151,"<p>This challenge aims at predicting height based on genetics (DNA variation).</p>

<h3 id=""background"">Background</h3>

<p>Genetics is the study of genes, genetic variation, and heredity in living organisms. DNA is the support that allows most living organism to pass information from generation to generation. It consists in long strands of nucleotides that build a higher order structures, the chromosomes. There are four different nucleotides represented by the letters A, T, C and G that together make up the genetic code.</p>

<p>The human genome is made of &gt; 3 billion nucleotides, and each individual harbors about 4 million genetic variants (mostly single nucleotide polymorphisms, or <strong>SNPs</strong>). A specific position on a chromosome is called a genetic locus, and different versions of the same <strong>genetic locus</strong> are called <strong>alleles</strong>. Humans being <strong>diploid</strong> organisms, they have two genome copies - one inherited from each parent - and thus two alleles at each genetic locus. For one particular genetic locus, an individual is <strong>homozygous</strong> if the two alleles are identical, and <strong>heterozygous</strong> if the two alleles are different.</p>

<p>We call <strong>genotype</strong> the DNA sequence of an individual that determines a specific observable characteristic. That characteristic is called <strong>phenotype</strong>.</p>

<p><strong>Monogenic</strong> phenotypes are under the control of a single gene. For example, if hair color was a monogenic phenotype, inheriting two brown alleles of a hypothetical hair color gene would result in the brown hair phenotype. Conversely, inheriting two ginger hair alleles would result in the ginger hair phenotype.</p>

<p><img src=""https://s3.amazonaws.com/salathegroup-static/opensnp/images/image1.png"" alt=""image1"" /></p>

<p><strong>Polygenic</strong> phenotypes, on the contrary, are under the control of multiple genetic variants across the genome. If hair color was polygenic, it might for example work like the RGB (Red, Green, Blue) color model. In this case, three different genes would add their effects and interact to control hair color.</p>

<p><img src=""https://s3.amazonaws.com/salathegroup-static/opensnp/images/image2.png"" alt=""image2"" /></p>

<p><strong>Heritability</strong> of a phenotype measures how much of the observed variance of the phenotype in the population is due to genetic factors. <strong>Missing heritability</strong> represents the difference between the estimated heritability of a given phenotype, and the heritability that is explained by known genetic factors. Heritability of human height is estimated to be as high as 80%, but large genomic studies have so far only been able to explain about 25% of the observed variance. Height is a model phenotype to study complex traits, and here we want to test whether part of the missing heritability can be explained using innovative approaches to genetic datasets, including deep learning.</p>

<h2 id=""data"">Data</h2>

<p>The data comes from <a href=""https://opensnp.org/"">OpenSNP</a>, which allows customers of direct-to-customer genetic tests to publicly share their genome-wide genotyping data.</p>

<p><img src=""https://s3.amazonaws.com/salathegroup-static/opensnp/images/opensnp-logo-small.png"" alt=""openSNPLogo "" class=""img-logo"" /></p>

<p>We provide two datasets for a total of 921 samples divided into a training set of 784 sample <code class=""highlighter-rouge"">subset_cm_train.npy</code> and a test set of 137 samples in <code class=""highlighter-rouge"">subset_cm_test.npy</code>.</p>

<p>It contains a set of <strong>9,894</strong> genetic variants known to be associated with height[1] (9207 variants) and the one on Y chromosome (687 variants). This numpy file has shape <code class=""highlighter-rouge"">(784, 9894)</code> for the training set and <code class=""highlighter-rouge"">(137, 9894)</code> for the test set. 
Each genetic variant is represented by 0 (homozygous for reference) , 1 (heterozygous), 2 (homozygous for the genetic variant) or NA (missing information or absence of the position in the case of Y chromosome in women). The first 9207 rows are the genetic variants known to be associated with height, the last 687 correspond to the Y chromosome.</p>

<p>Finally, height is provided in a separate numpy file of shape <code class=""highlighter-rouge"">(784, 1)</code>  named <code class=""highlighter-rouge"">openSNP_heights.npy</code> for the training set only.</p>

<p>While we recommend to start with this simplified dataset, more advanced user might try to analyze an extended version of OpenSNP data which description is available <a href=""https://github.com/crowdAI/opensnp-challenge-starter-kit/blob/master/overview_full_dataset.md"">here</a>.</p>

<h2 id=""submission"">Submission</h2>
<div class=""language-python highlighter-rouge""><div class=""highlight""><pre class=""highlight""><code><span class=""kn"">import</span> <span class=""nn"">crowdai</span>
<span class=""n"">challenge</span> <span class=""o"">=</span> <span class=""n"">crowdai</span><span class=""o"">.</span><span class=""n"">Challenge</span><span class=""p"">(</span><span class=""s"">""OpenSNPChallenge2017""</span><span class=""p"">,</span> <span class=""s"">""YOUR_CROWDAI_API_KEY_HERE""</span><span class=""p"">)</span>

<span class=""n"">data</span> <span class=""o"">=</span> <span class=""o"">...</span> <span class=""c1"">#a list of 137 predicted heights for all the 137 corresponding data points in the test set
</span><span class=""n"">challenge</span><span class=""o"">.</span><span class=""n"">submit</span><span class=""p"">(</span><span class=""n"">data</span><span class=""p"">)</span>
<span class=""n"">challenge</span><span class=""o"">.</span><span class=""n"">disconnect</span><span class=""p"">()</span>
</code></pre></div></div>

<p>More instructions to make submissions, and starter code is available at :</p>

<p><strong>STARTER KIT</strong>  <a href=""https://github.com/crowdAI/opensnp-challenge-starter-kit""> https://github.com/crowdAI/opensnp-challenge-starter-kit </a></p>

<h2 id=""contact"">Contact</h2>
<ul>
  <li>Sharada Mohanty &lt;<a href=""mailto:sharada.mohanty@epfl.ch""> sharada.mohanty@epfl.ch </a>&gt;</li>
  <li>Olivier Naret &lt;<a href=""olivier.naret@epfl.ch""> olivier.naret@epfl.ch </a>&gt;</li>
</ul>

<h3 id=""advisors"">Advisors</h3>
<ul>
  <li>Jacques Fellay &lt;<a href=""jacques.fellay@epfl.ch""> jacques.fellay@epfl.ch </a>&gt;</li>
  <li>Marcel Salathe &lt;<a href=""marcel.salathe@epfl.ch""> marcel.salathe@epfl.ch </a>&gt;</li>
</ul>

<p><strong>Challenge Image source</strong>: <a href=""https://commons.wikimedia.org/wiki/File:Benzopyrene_DNA_adduct_1JDG.png"">https://commons.wikimedia.org/wiki/File:Benzopyrene_DNA_adduct_1JDG.png</a></p>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>
<p>The evaluation will be done based on two scores :</p>

<ul>
  <li><a href=""https://en.wikipedia.org/wiki/Coefficient_of_determination""> Co-efficient of Determination (<script type=""math/tex"">R^2</script>) </a> (Primary Score)</li>
  <li><a href=""https://en.wikipedia.org/wiki/Mean_squared_error""> Mean Squared Error </a> (Secondary Score)</li>
</ul>

<p>between the actual heights of the individuals in the test set and the submitted predictions.</p>

<p><strong>NOTE</strong> : During the challenge, the scores will be computed only on 20% of the test dataset. The final standings on the leaderboard will be decided computing the same scores on the 100% of the dataset after the challenge.</p>

<h3 id=""resources"">Resources</h3>

<p>MIT Open Course Ware can help you go further in understanding biological concepts related to this challenge.</p>

<ul>
  <li>Lesson 19 on Discovering Quantitative Trait Loci (QTLs)</li>
  <li>Lesson 20 on Human Genetics, SNPs, and Genome Wide Associate Studies</li>
</ul>

<p>The most important publications describing associations between genetic factors and human height :</p>

<ul>
  <li>based on <a href=""http://www.nature.com/ng/journal/v46/n11/full/ng.3097.html""> common SNPs </a>[1]</li>
  <li>on <a href=""http://www.nature.com/nature/journal/v542/n7640/full/nature21039.html""> rare SNPs </a>[2].</li>
</ul>

<p>To transform the VCF files it can be convenient to use <a href=""https://www.cog-genomics.org/plink2""> plink </a>.</p>

<h2 id=""references"">References</h2>

<p>1 <a href=""https://www.nature.com/ng/journal/v46/n11/abs/ng.3097.html"">Wood, Andrew R, Tonu Esko, Jian Yang, Sailaja Vedantam, Tune H Pers, Stefan Gustafsson, Audrey Y Chu, et al. “Defining the Role of Common Variation in the Genomic and Biological Architecture of Adult Human Height.” Nature Genetics 2014. doi:10.1038/ng.3097</a>.</p>

<p>2 <a href=""https://www.nature.com/nature/journal/v542/n7640/full/nature21039.html"">Marouli, Eirini, Mariaelisa Graff, Carolina Medina-Gomez, Ken Sin Lo, Andrew R. Wood, Troels R. Kjaer, Rebecca S. Fine, et al. “Rare and Low-Frequency Coding Variants Alter Human Adult Height.” Nature 2017. doi:10.1038/nature21039</a>.</p>

<h3 id=""prizes"">Prizes</h3>

<p>The winner will be invited to the 2nd Applied Machine Learning Days at EPFL in Switzerland on January  29 &amp; 30, 2018, with travel and accommodation covered.</p>

<h3 id=""datasets-license"">Datasets License</h3>

"
170,"<p><em>Note: ImageCLEF Security 2019 is divided into 3 subtasks (challenges):</em></p>

<ul>
  <li>
    <p><em>Task 1: Forged File Discovery</em></p>
  </li>
  <li>
    <p><em>Task 2: Stego Image Discovery</em></p>
  </li>
  <li>
    <p><em>Task 3: Secret Message Discovery</em></p>
  </li>
</ul>

<p><em>This challenge is about <strong>Stego Image Discovery</strong>(task 2). For information on the <strong>Forged File Discovery</strong>(task 1) challenge click <a href=""/challenges/imageclef-2019-security-forged-file-discovery"" target=""_blank""> here </a>. For information on the <strong>Secret Message Discovery</strong>(task 3) challenge click <a href=""/challenges/imageclef-2019-security-secret-message-discovery"" target=""_blank""> here </a>. All of these challenges share similar scenario. Registering for one of these challenges will automatically give you access to the other ones.</em></p>

<p><em>Note: Do not forget to read the Rules section on this page.</em></p>

<h3 id=""motivation"">Motivation</h3>

<p>File Forgery Detection (FFD) is a serious problem concerning digital forensics examiners. Fraud or counterfeits are common causes for altering files. Another example is a child predator who hides porn images by altering the image extension and in some cases by changing the image signature. Many proposals have been made to solve this problem and the most promising ones concentrate on the image content. It is also common that anyone who wants to hide any kind of information in plain sight without being perceived to use steganography. Steganography is the practice of concealing a file, message, image or video within another file, message, image, or video. The word steganography combines the Greek words steganos (στεγανός), meaning “covered” and graphein (γράφειν) meaning “writing”. The most usual cover medium for hiding data are images.</p>

<h3 id=""challenge-description"">Challenge description</h3>

<p>You are a professional digital forensic examiner collaborating with the police, who suspects that there is an ongoing fraud in the Central Bank. After obtaining a court order, police gain access to a suspect’s computer in the bank with the purpose to look for images proving the suspect guilty. However, police suspects that he has managed to change extension and signature of some images, so that they look like pdf files. Additionally, it is highly probable that the suspect has used steganography software to hide messages within some images that could reveal valuable information of his collaborators.</p>

<p>The goal of this challenge is to examine if an image could hide a text message. Identify the altered images that hide steganographic content.</p>

<h3 id=""data"">Data</h3>

<p>Training set for stego image discovery (i.e. task 2 ) consists of 1000 images of jpg format. 500 of these images are clean while the rest are stego.</p>

<h3 id=""submission-instructions"">Submission instructions</h3>

<hr />
<p><em>As soon as the submission is open, you will find a “Create Submission” button on this page (just next to the tabs)</em></p>

<hr />
<p>Please note that each group is allowed for maximum of 10 runs per task.</p>

<p>** Identify Stego Images**</p>

<p>For the submission of the task we expect the following format:</p>

<p>&lt;Figure-ID&gt;;&lt;yes/no&gt; —&gt; Figure-ID&gt;;&lt;1/0&gt;</p>

<p>e.g.:</p>

<blockquote>
  <p>1741_02;1 if the image includes stego</p>
</blockquote>

<blockquote>
  <p>1742_02;0 if the image does NOT include stego</p>
</blockquote>

<blockquote>
  <p>1743_02;1 if the image includes stego</p>
</blockquote>

<p><em><strong>You need to respect the following constraints:</strong></em></p>

<ul>
  <li>
    <p>The separator between the figure ID and the description has to be be a semicolon (;).</p>
  </li>
  <li>
    <p>The file to upload must be a .txt file.</p>
  </li>
  <li>
    <p>Each figure ID of the test set must be included in the runfile exactly once.</p>
  </li>
  <li>
    <p>The result cannot be specified more than once for the same figure ID.</p>
  </li>
</ul>

<h3 id=""citations"">Citations</h3>

<p>Information will be posted after the challenge ends.</p>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>
<p>For assessing performance, classic metrics are used:
Precision, Recall and F1.</p>

<p><strong>Precision</strong>
In pattern recognition, information retrieval and binary classification, precision is the fraction of relevant instances among the retrieved instances.
Precision could be defined as the fraction of actual detected images with hidden messages among all the detected images with hidden a message:</p>

<p>Precision= nº of actual detected images with hidden messages /Total detections of altered images with hidden messages</p>

<p><strong>Recall</strong>
In pattern recognition, information retrieval and binary classification, recall is the fraction of relevant instances that have been retrieved over the total amount of relevant instances.
Recall could be defined as the fraction of actual detected images with hidden messages among all the images with hidden a message:</p>

<p>Recall = nº of actual detected images with hidden messages /Total altered images with hidden messages</p>

<p><strong>F-measure</strong>
F-measure is the harmonic mean of precision and recall, mathematically expressed as</p>

<p>F_1=2∙(Precision ∙ Recall)/(Precision + Recall )</p>

<h3 id=""resources"">Resources</h3>

<h3 id=""contact-us"">Contact us</h3>

<ul>
  <li>Discussion Forum : <a href=""https://www.crowdai.org/challenges/imageclef-2019-security-stego-image-discovery/topics"" target=""_blank""> https://www.crowdai.org/challenges/imageclef-2019-security-stego-image-discovery/topics </a></li>
</ul>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

<ul>
  <li>
    <p>Narciso Garcia, Professor, Dr., Grupo de Tratamiento de Imágenes, Dpto. Señales, Sistemas y Radiocomunicaciones, E.T.S. Ingenieros Telecomunicación, Spain, narciso@gti.ssr.upm.es</p>
  </li>
  <li>
    <p>Ergina Kavallieratou, Associate Professor, Dr, AIlab, Department of Information &amp; Communication Systems Engineering, University of the Aegean, Greece, kavallieratou@aegean.gr</p>
  </li>
  <li>
    <p>Carlos Roberto del Blanco, Assistant Professor, Dr., Grupo de Tratamiento de Imágenes, Dpto. Señales, Sistemas y Radiocomunicaciones, E.T.S. Ingenieros de Telecomunicación, cda@gti.ssr.upm.es</p>
  </li>
  <li>
    <p>Carlos Cuevas Rodríguez, Assistant Professor, Dr., Grupo de Tratamiento de Imágenes, Dpto. Señales, Sistemas y Radiocomunicaciones, E.T.S. Ingenieros de Telecomunicación, Spain, ccr@gti.ssr.upm.es</p>
  </li>
  <li>
    <p>Nikos Vasillopoulos, Phd, Postdoc, AIlab, Department of Information &amp; Communication Systems Engineering, University of the Aegean, Greece, nvasilopoulos@aegean.gr</p>
  </li>
  <li>
    <p>Konstantinos Karampidis, Msc, Phd student, University of the Aegean, Greece, karampidis@aegean.gr</p>
  </li>
</ul>

<p><strong>For questions over the Security task e-mail: Imageclefsecurity@aegean.gr</strong></p>

<h3 id=""more-information"">More information</h3>

<p>You can find additional information on the challenge here:
<a href=""https://www.imageclef.org/2019/security"" target=""_blank""> https://www.imageclef.org/2019/security </a></p>

<h3 id=""prizes"">Prizes</h3>

<p>ImageCLEF 2019 is an evaluation campaign that is being organized as part of the <a href=""http://clef2019.clef-initiative.eu/"" target=""_blank""> CLEF initiative </a> labs. The campaign offers several research tasks that welcome participation from teams around the world. The results of the campaign appear in the working notes proceedings, published by CEUR Workshop Proceedings (CEUR-WS.org). Selected contributions among the participants, will be invited for publication in the following year in the Springer Lecture Notes in Computer Science (LNCS) together with the annual lab overviews.</p>

<h3 id=""datasets-license"">Datasets License</h3>

"
171,"<p><em>Note: ImageCLEF Security 2019 is divided into 3 subtasks (challenges):</em></p>

<ul>
  <li>
    <p><em>Task 1: Forged File Discovery</em></p>
  </li>
  <li>
    <p><em>Task 2: Stego Image Discovery</em></p>
  </li>
  <li>
    <p><em>Task 3: Secret Message Discovery</em></p>
  </li>
</ul>

<p><em>This challenge is about <strong>Secret Message Discovery</strong>(task 3). For information on the <strong>Forged File Discovery</strong>(task 1) challenge click <a href=""/challenges/imageclef-2019-security-forged-file-discovery"" target=""_blank""> here </a>. For information on the <strong>Stego Image Discovery</strong>(task 2) challenge click <a href=""/challenges/imageclef-2019-security-stego-image-discovery"" target=""_blank""> here </a>. All of these challenges share the same scenario. Registering for one of these challenges will automatically give you access to the other ones.</em></p>

<p><em>Note: Do not forget to read the Rules section on this page.</em></p>

<h3 id=""motivation"">Motivation</h3>

<p>File Forgery Detection (FFD) is a serious problem concerning digital forensics examiners. Fraud or counterfeits are common causes for altering files. Another example is a child predator who hides porn images by altering the image extension and in some cases by changing the image signature. Many proposals have been made to solve this problem and the most promising ones concentrate on the image content. It is also common that anyone who wants to hide any kind of information in plain sight without being perceived to use steganography. Steganography is the practice of concealing a file, message, image or video within another file, message, image, or video. The word steganography combines the Greek words steganos (στεγανός), meaning “covered” and graphein (γράφειν) meaning “writing”. The most usual cover medium for hiding data are images.</p>

<h3 id=""challenge-description"">Challenge description</h3>

<p>You are a professional digital forensic examiner collaborating with the police, who suspects that there is an ongoing fraud in the Central Bank. After obtaining a court order, police gain access to a suspect’s computer in the bank with the purpose to look for images proving the suspect guilty. However, police suspects that he has managed to change extension and signature of some images, so that they look like pdf files. Additionally, it is highly probable that the suspect has used steganography software to hide messages within some images that could reveal valuable information of his collaborators.</p>

<p>The goal of this challenge is to retrieve the potential message from the forged stego images. Extract the hidden messages (text) from the stego images.</p>

<h3 id=""data"">Data</h3>

<p>Training set for  secret message discovery (i.e. task 3) contains 1000 images of jpg format. 500 of them are clean while the rest contain text messages different for every 100 images.</p>

<h3 id=""submission-instructions"">Submission instructions</h3>

<hr />
<p><em>As soon as the submission is open, you will find a “Create Submission” button on this page (just next to the tabs)</em></p>

<hr />
<p><strong>Retrieve the Message</strong></p>

<p>For the submission of the task we expect the following format:</p>

<p>&lt;Figure-ID&gt;;&lt;stego&gt;</p>

<p>e.g.:</p>

<blockquote>
  <p>1743_03;abcdef</p>
</blockquote>

<p><em>if the image includes the hidden message absdef</em></p>

<blockquote>
  <p>1743_03;y5fg3687</p>
</blockquote>

<p><em>if the image includes the hidden message y5fg3687</em></p>

<p><strong><em>You need to respect the following constraints:</em></strong></p>

<ul>
  <li>
    <p>The separator between the figure ID and the description has to be a semicolon (;).</p>
  </li>
  <li>
    <p>The file to upload must be a .txt file.</p>
  </li>
  <li>
    <p>Each figure ID of the testset must be included in the runfile exactly once.</p>
  </li>
  <li>
    <p>The result cannot be specified more than once for the same figure ID.</p>
  </li>
</ul>

<h3 id=""citations"">Citations</h3>

<p>Information will be posted after the challenge ends.</p>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>
<p>For assessing performance, classic metric is used: Edit distance.</p>

<p><strong>Edit distance</strong>
Given two strings a and b on an alphabet Σ (e.g. the set of ASCII characters), the edit distance d(a,b) is the minimum-weight series of edit operations (Insertion, Deletion, Substitution) that transforms a into b.</p>

<h3 id=""resources"">Resources</h3>

<h3 id=""contact-us"">Contact us</h3>

<ul>
  <li>Discussion Forum : <a href=""https://www.crowdai.org/challenges/imageclef-2019-security-secret-message-discovery/topics"" target=""_blank""> https://www.crowdai.org/challenges/imageclef-2019-security-secret-message-discovery/topics </a></li>
</ul>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

<ul>
  <li>
    <p>Narciso Garcia, Professor, Dr., Grupo de Tratamiento de Imágenes, Dpto. Señales, Sistemas y Radiocomunicaciones, E.T.S. Ingenieros Telecomunicación, Spain, narciso@gti.ssr.upm.es</p>
  </li>
  <li>
    <p>Ergina Kavallieratou, Associate Professor, Dr, AIlab, Department of Information &amp; Communication Systems Engineering, University of the Aegean, Greece, kavallieratou@aegean.gr</p>
  </li>
  <li>
    <p>Carlos Roberto del Blanco, Assistant Professor, Dr., Grupo de Tratamiento de Imágenes, Dpto. Señales, Sistemas y Radiocomunicaciones, E.T.S. Ingenieros de Telecomunicación, cda@gti.ssr.upm.es</p>
  </li>
  <li>
    <p>Carlos Cuevas Rodríguez, Assistant Professor, Dr., Grupo de Tratamiento de Imágenes, Dpto. Señales, Sistemas y Radiocomunicaciones, E.T.S. Ingenieros de Telecomunicación, Spain, ccr@gti.ssr.upm.es</p>
  </li>
  <li>
    <p>Nikos Vasillopoulos, Phd, Postdoc, AIlab, Department of Information &amp; Communication Systems Engineering, University of the Aegean, Greece, nvasilopoulos@aegean.gr</p>
  </li>
  <li>
    <p>Konstantinos Karampidis, Msc, Phd student, University of the Aegean, Greece, karampidis@aegean.gr</p>
  </li>
</ul>

<p><strong>For questions over the Security task e-mail: Imageclefsecurity@aegean.gr</strong></p>

<h3 id=""more-information"">More information</h3>

<p>You can find additional information on the challenge here:
<a href=""https://www.imageclef.org/2019/security"" target=""_blank""> https://www.imageclef.org/2019/security</a></p>

<h3 id=""prizes"">Prizes</h3>

<p>ImageCLEF 2019 is an evaluation campaign that is being organized as part of the <a href=""http://clef2019.clef-initiative.eu/"" target=""_blank""> CLEF initiative </a> labs. The campaign offers several research tasks that welcome participation from teams around the world. The results of the campaign appear in the working notes proceedings, published by CEUR Workshop Proceedings (CEUR-WS.org). Selected contributions among the participants, will be invited for publication in the following year in the Springer Lecture Notes in Computer Science (LNCS) together with the annual lab overviews.</p>

<h3 id=""datasets-license"">Datasets License</h3>

"
172,"<p><em>Note: ImageCLEF Tuberculosis 2019 is divided into 2 subtasks (challenges). This challenge is about <strong>CT report</strong>. For information on the <strong>Severity Scoring</strong> challenge click <a href=""/challenges/imageclef-2019-tuberculosis-severity-scoring"" target=""_blank""> here </a>. Both challenges share the same dataset, so registering for one of these challenges will automatically give you access to the other one.</em></p>

<p><em>Note: Do not forget to read the Rules section on this page.</em></p>

<h3 id=""motivation"">Motivation</h3>

<p>Tuberculosis (TB) is a bacterial infection caused by a germ called Mycobacterium tuberculosis. About 130 years after its discovery, the disease remains a persistent threat and a leading cause of death worldwide according to WHO. This bacteria usually attacks the lungs, but it can also damage other parts of the body. Generally, TB can be cured with antibiotics. However, the different types of TB require different treatments, and therefore the detection of the TB type and the evaluation of the severity stage are two important tasks.</p>

<h3 id=""challenge-description"">Challenge description</h3>

<p>In this subtasks the participants will have to generate an automatic report based on the CT image.
This report should include the following information in binary form (0 or 1): Left lung affected, right lung affected, presence of calcifications, presence of caverns, pleurisy, lung capacity decrease.</p>

<h3 id=""data"">Data</h3>

<p>In this edition, both subtasks (SVR and CTR) use the same dataset containing 335 chest CT scans of TB patients along with a set of clinically relevant metadata. 218 patients are used for training and 117 for test. The selected metadata includes the following binary measures: disability, relapse, symptoms of TB, comorbidity, bacillary, drug resistance, higher education, ex-prisoner, alcoholic, smoking.</p>

<p>For all patients we provide 3D CT images with slice size of 512*512 pixels and number of slices varying from about 50 to 400. All the CT images are stored in NIFTI file format with .nii.gz file extension (g-zipped .nii files). This file format stores raw voxel intensities in Hounsfield units (HU) as well the corresponding image metadata such as image dimensions, voxel size in physical units, slice thickness, etc. A freely-available tool called <a href=""https://www.creatis.insa-lyon.fr/rio/vv"" target=""_blank""> “VV” </a> can be used for viewing image files. Currently, there are various tools available for reading and writing NIFTI files. Among them there are <a href=""https://de.mathworks.com/matlabcentral/fileexchange/8797-tools-for-nifti-and-analyze-image/content/load_nii.m"" target=""_blank""> load_nii </a> and <a href=""https://de.mathworks.com/matlabcentral/fileexchange/8797-tools-for-nifti-and-analyze-image/content/save_nii.m"" target=""_blank""> save_nii </a> functions for Matlab and <a href=""http://niftilib.sourceforge.net/"" target=""_blank""> Niftilib </a> library for C, Java, Matlab and Python.</p>

<p>We also provide automatic extracted masks of the lungs. This material can be downloaded together with the patients CT images. The details of this segmentation can be found <a href=""http://publications.hevs.ch/index.php/publications/show/1871"" target=""_blank""> here </a>.
In case the participants use these masks in their experiments, please refer to the section “Citations” in the <a href=""https://www.imageclef.org/2019/medical/tuberculosis"" target=""_blank""> ImageCLEF TB 2019 </a>
website to find the appropriate citation for this lung segmentation technique.</p>

<p><strong>Remarks on the automatic lung segmentation:</strong></p>

<p>The segmentations were manually analysed based on statistics on number of lungs found and size ratio between right-left lung. Only those segmentations with anomalies on these statistics were visualized. The code used to segment the patients was adapted for the cases with unsatisfactory segmentation. After this proceeding, all patients with anomalies presented a satisfactory mask.</p>

<h3 id=""submission-instructions"">Submission instructions</h3>

<hr />
<p><em>As soon as the submission is open, you will find a “Create Submission” button on this page (just next to the tabs)</em></p>

<hr />

<p>Submit a plain text file named with the prefix <strong>CTR</strong> (e.g. CTRfree-text.txt) with the following format:</p>

<p>&lt;Patient-ID&gt;,&lt;Probability of “left lung affected”&gt;,&lt;Probability of “right lung affected”&gt;,&lt;Probability of “presence of calcifications”&gt;,&lt;Probability of “presence of caverns”&gt;,&lt;Probability of “pleurisy”&gt;,&lt;Probability of “lung capacity decrease”&gt;</p>

<p>e.g.:</p>

<div class=""highlighter-rouge""><div class=""highlight""><pre class=""highlight""><code>CTR_TST_001,0.93,0.2,0.655,0.01,0.3645,0.98
CTR_TST_002,0.54,0,1,0.25,0.2,0.598,0
CTR_TST_003,0.1,0.50,0.0,1.0,0.999,0.46
CTR_TST_004,0.245,0.12,0.23,0.34,0.45,0.68
CTR_TST_005,0.7,0.1,0,0,0,0
</code></pre></div></div>

<p>You need to respect the following constraints:</p>

<ul>
  <li>Patient-IDs must be part of the predefined Patient-IDs</li>
  <li>All patient-IDs must be present in the runfiles</li>
  <li>Only use numbers between 0 and 1 for the probabilities. Use the dot (.) as a decimal point (no commas accepted)</li>
</ul>

<h3 id=""citations"">Citations</h3>

<p>Information will be posted after the challenge ends.</p>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>
<p>This task is considered a multi-binary classification problem (6 binary findings). Measures including AUC and accuracy will be used to evaluate the task.
The ranking of this task will be done first by <strong>average AUC</strong> and then by <strong>min AUC</strong> (both over the 6 CT findings).</p>

<h3 id=""resources"">Resources</h3>

<h3 id=""contact-us"">Contact us</h3>

<ul>
  <li>Discussion Forum : <a href=""https://www.crowdai.org/challenges/imageclef-2019-tuberculosis-ct-report/topics"" target=""_blank""> https://www.crowdai.org/challenges/imageclef-2019-tuberculosis-ct-report/topics </a></li>
</ul>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

<p>Yashin Dicente Cid &lt;yashin.dicente(at)hevs.ch&gt;, University of Applied Sciences Western Switzerland, Sierre, Switzerland
Vitali Liauchuk &lt;vitali.liauchuk(at)gmail.com&gt;, Institute for Informatics, Minsk, Belarus
Vassili Kovalev &lt;vassili.kovalev(at)gmail.com&gt;, Institute for Informatics, Minsk, Belarus
Henning Müller &lt;henning.mueller(at)hevs.ch&gt;, University of Applied Sciences Western Switzerland, Sierre, Switzerland</p>

<h3 id=""more-information"">More information</h3>

<p>You can find additional information on the challenge here:
<a href=""https://www.imageclef.org/2019/medical/tuberculosis"" target=""_blank""> https://www.imageclef.org/2019/medical/tuberculosis</a></p>

<h3 id=""prizes"">Prizes</h3>

<p>ImageCLEF 2019 is an evaluation campaign that is being organized as part of the <a href=""http://clef2019.clef-initiative.eu/"" target=""_blank""> CLEF initiative </a> labs. The campaign offers several research tasks that welcome participation from teams around the world. The results of the campaign appear in the working notes proceedings, published by CEUR Workshop Proceedings (CEUR-WS.org). Selected contributions among the participants, will be invited for publication in the following year in the Springer Lecture Notes in Computer Science (LNCS) together with the annual lab overviews.</p>

<h3 id=""datasets-license"">Datasets License</h3>

"
152,"<p>Can AI generate music that we humans find beautiful, perhaps even moving? Let’s find out!</p>

<p>In this challenge, participants are tasked to generate an AI model that learns on a large data set of music (in the form of MIDI files), and is then capable of producing its own music. Concretely, the model must produce a music piece in response to a short “seed” MIDI file that is given as input.</p>

<p>There are two special aspects of this challenge, apart from the extremely interesting application. First, the results of the models will be evaluated by humans, with an ELO-style system where volunteers are given two randomly paired pieces of generated music, and choose the one they like better. Second, <strong>the top five models will at the end each generate a piece of music that will be performed live on stage at the Applied Machine Learning Days</strong>!</p>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>
<p>The grader expects a MIDI file of a total length of <code class=""highlighter-rouge"">3600</code> seconds (when played at <code class=""highlighter-rouge"">120 bpm</code>). The MIDI file has to be a <code class=""highlighter-rouge"">type 0</code> MIDI file (a maximum of 1 track), and in the case of multiple tracks, only the first track will be considered. There are no challenge-specific restrictions on the number of channels being used in the MIDI file.
The grader splits the MIDI file into <code class=""highlighter-rouge"">120 chunks</code> of approximately <code class=""highlighter-rouge"">30 seconds</code> each, and each submission is represented by this pool of 120 chunks.
During this post-processing step, all meta events from the MIDI file will be removed except the PPQ meta event (or <code class=""highlighter-rouge"">ticks per beat</code>), hence the officially supported MIDI events will only be the <code class=""highlighter-rouge"">note_on</code> and <code class=""highlighter-rouge"">note_off</code> events; where <code class=""highlighter-rouge"">note_off</code> event can be optionally replaced by a <code class=""highlighter-rouge"">note_on</code> event with a <em>velocity</em> of <code class=""highlighter-rouge"">0</code>. All the MIDI parsing is done using the <a href=""https://mido.readthedocs.io/en/latest/"">MIDO</a> library; and you are requested to ensure that your submitted file is estimated to be of <code class=""highlighter-rouge"">3600 +/- 10 seconds</code> by <code class=""highlighter-rouge"">mido.MidiFile('your_file_path').length</code>.</p>

<p>A separate <a href=""https://www.crowdai.org/challenges/ai-generated-music-challenge/dynamic_contents"">evaluation interface</a> is made available, where all the participants (and other external volunteers) can hear two randomly sampled chunks and then vote for the one they like better (more details on the sampling mechanism is provided in the following sections). These randomly sampled chunks will be played with the <a href=""https://github.com/mudcube/MIDI.js/blob/master/examples/soundfont/acoustic_grand_piano-mp3.js"">SoundFont of an acoustic grand piano</a> at <code class=""highlighter-rouge"">120 bpm</code>.</p>

<p>These binary comparisons will be used to compute an individual score for every submission, which evolves over time as it gets more and more evaluations in the evaluation interface. The scoring mechanism follows the <a href=""https://www.microsoft.com/en-us/research/project/trueskill-ranking-system/"">TrueSkill</a> ranking system, and hence is modeled by $ \mu $ (a quantitative estimate of the preference of a general population towards a particular song) and $ \sigma $ (the confidence of the system in this estimate). The actual score on the leaderboard is computed by taking a conservative estimate of the modeled score, and hence is represented by :
$ \mu - k * \sigma $
where $k$ is the ratio of the default $\mu$ and $\sigma$ values and is represented by:  $(\mu=25) / (\sigma=8.334)$.</p>

<p>The <a href=""https://www.crowdai.org/challenges/ai-generated-music-challenge/submissions"">submissions tab</a> will report the values for $\mu$, $\sigma$ and the number of evaluations completed for every submission; and the leaderboard will use the conservative estimate of $\mu - k * \sigma$ as the primary score, and $\mu$ as the secondary score.</p>

<p>To ensure that the top-10 selected participants are not overfitting on the training set; <strong>the top-10</strong>  submissions at the end of the challenge, will be divided into quantized chunks of $\tau(=5)$ seconds each (at 120 bpm) with a sliding window of stride $s$, and a normalised dynamic time warp (DTW) distance will be computed against $\tau(=5)$  second chunks from all the MIDI files listed in the <a href=""https://www.crowdai.org/challenges/ai-generated-music-challenge/dataset_files"">Datasets</a>. 
With $DTW(x, y)$ representing the DTW between two $\tau(=5)$ second quantized chunks, the normalized DTW will be computed by :</p>

<p>$NDTW(x,y) = \frac{127 \times T(\tau=5) - DTW(x,y) }{127 \times T(\tau=5)}$</p>

<p>where,
$T(\tau)$ represents the number of ticks in a time period of $\tau$ seconds.</p>

<p>All matching chunks pairs with $NTDW &lt; 0.3$ will be manually verified, and in case the chunks are found to be similar, then the submissions will be disqualified.
Given the subjective nature of the evaluation, the organisers will reserve the right to both adjust the threshold of $0.3$ and also to decide if the flagged chunks are indeed similar because of the model overfitting, or because of the said participant trying to cheat by stitching together MIDI snippets from the training data.</p>

<p><strong>Starter Kit</strong> : A starter kit to help you get started on the submission procedure is made available at: <a href=""https://github.com/crowdAI/crowdai-ai-generate-music-starter-kit"">https://github.com/crowdAI/crowdai-ai-generate-music-starter-kit</a>.</p>

<p><strong>Comin Soon</strong> : A Getting Started guide on music generation from MIDI files using LSTMs.</p>

<h3 id=""resources"">Resources</h3>

<p><strong>Starter Kit</strong> : A starter kit to help you get started on the submission procedure is made available at : <a href=""https://github.com/crowdAI/crowdai-ai-generate-music-starter-kit"">https://github.com/crowdAI/crowdai-ai-generate-music-starter-kit</a>.</p>

<p>Some other projects to help you quickly get started on MIDI composition:</p>

<ul>
  <li><a href=""https://github.com/brannondorsey/midi-rnn"">https://github.com/brannondorsey/midi-rnn</a></li>
  <li><a href=""https://github.com/jisungk/deepjazz"">https://github.com/jisungk/deepjazz</a></li>
  <li><a href=""https://magenta.tensorflow.org/performance-rnn"">Google Magenta : Performance RNN</a></li>
  <li><a href=""https://richardyang40148.github.io/TheBlog/midinet_arxiv_demo.html"">MIDINet</a></li>
</ul>

<h3 id=""contact"">Contact:</h3>

<ul>
  <li>Technical issues : <a href=""https://gitter.im/crowdAI/AI-Generated-Music-Challenge"">https://gitter.im/crowdAI/AI-Generated-Music-Challenge</a></li>
  <li>Discussion Forum : <a href=""https://www.crowdai.org/challenges/ai-generated-music-challenge/topics"">https://www.crowdai.org/challenges/ai-generated-music-challenge/topics</a></li>
</ul>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

<ul>
  <li>Sharada Prasanna Mohanty <a href=""mailto:sharada.mohanty@epfl.ch"" target=""_blank"">sharada.mohanty@epfl.ch</a></li>
  <li>Florian Colombo <a href=""mailto:florian.colombo@epfl.ch"" target=""_blank"">florian.colombo@epfl.ch</a></li>
</ul>

<h3 id=""prizes"">Prizes</h3>

<h3 id=""datasets-license"">Datasets License</h3>

<p>abcd</p>
"
147,"<h1 id=""learning-how-to-walk"">Learning how to walk</h1>

<p>Our movement originates in the brain. Many neurological disorders, such as Cerebral Palsy, Multiple Sclerosis, or strokes can lead to problems with walking. Treatments are often symptomatic, and it’s often hard to predict outcomes of surgeries. Understanding underlying mechanisms is key to improvement of treatments. This motivates our efforts to model the motor control unit of the brain.</p>

<p>In this challenge, your task is to model the motor control unit in a virtual environment. You are given a musculoskeletal model with 18 muscles to control. At every 10ms you send signals to these muscles to activate or deactivate them. The objective is to walk as far as possible in 5 seconds.</p>

<p>For modelling physics we use OpenSim - a biomechanical physics environment for musculoskeletal simulations. You can read more datails <a href=""https://github.com/stanfordnmbl/osim-rl"">here</a>.</p>

<p><img src=""https://github.com/stanfordnmbl/osim-rl/blob/master/demo/training.gif?raw=true"" alt=""HUMAN environment"" /></p>

<p><strong>NOTE</strong> : There have been a few changes to the API of the grading server. Please update your <code class=""highlighter-rouge"">osim-rl</code> installation by :
<code class=""highlighter-rouge"">pip install git+https://github.com/kidzik/osim-rl.git</code> 
and update your submission script by referring to : (https://github.com/stanfordnmbl/osim-rl/blob/master/scripts/submit.py#L43)[https://github.com/stanfordnmbl/osim-rl/blob/master/scripts/submit.py#L43]
In the meantime if you run into scary looking error messages when using your previous submission scripts, please do not panic !! :D :D !!</p>

<h2 id=""credits"">Credits</h2>

<p>This challenge wouldn’t be possible without:</p>

<ul>
  <li><a href=""https://github.com/opensim-org/opensim-core"">OpenSim</a></li>
  <li><a href=""https://nmbl.stanford.edu/"">Stanford Neuromuscular Biomechanics Lab</a></li>
  <li><a href=""http://mobilize.stanford.edu/"">Stanford Mobilize Center</a></li>
  <li><a href=""https://gym.openai.com/"">OpenAI gym</a></li>
  <li><a href=""https://github.com/openai/gym-http-api"">OpenAI http client</a></li>
  <li><a href=""https://github.com/matthiasplappert/keras-rl"">keras-rl</a></li>
  <li>and many other teams, individuals and projects</li>
</ul>

<p>For more details and queries please contact</p>

<ul>
  <li><a href=""http://kidzinski.com/"">Łukasz Kidziński</a></li>
  <li><a href=""mailto:sharada.mohanty@epfl.ch"">S.P. Mohanty</a></li>
  <li><a href=""mailto: info@crowdai.org"">CrowdAI</a></li>
</ul>

<h2 id=""partners"">Partners</h2>
<p><img src=""https://avatars0.githubusercontent.com/u/7243467?v=3&amp;s=200"" alt=""Stanford"" class=""img-logo"" />
<img src=""https://dnczkxd1gcfu5.cloudfront.net/images/challenge_partners/image_file/35/group_1.jpg"" alt=""EPFL"" class=""img-logo"" /></p>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>
<p>Your task is to build a function <code class=""highlighter-rouge"">f</code> which takes current state <code class=""highlighter-rouge"">observation</code> (31 dimensional vector) and returns muscle activations <code class=""highlighter-rouge"">action</code> (18 dimensional vector) in a way that maximizes the reward.</p>

<p>The trial ends either if the pelvis of the model goes below <code class=""highlighter-rouge"">0.7</code> meter or if you reach <code class=""highlighter-rouge"">500</code> iterations (corresponding to <code class=""highlighter-rouge"">5</code> seconds in the virtual environment). Let <code class=""highlighter-rouge"">N</code> be the length of the trial. Your total reward is simply the position of the pelvis on the <code class=""highlighter-rouge"">x</code> axis after <code class=""highlighter-rouge"">N</code> steps. The value is given in centimeters.</p>

<p>After each iteration you get a reward equal to the change of the <code class=""highlighter-rouge"">x</code> axis of pelvis during this iteration.</p>

<p>You can test your model on your local machine. For submission, you will need to interact with the remote environment: crowdAI sends you the current <code class=""highlighter-rouge"">observation</code> and you need to send back the action you take in the given state.</p>

<h3 id=""resources"">Resources</h3>

<p>Please refer to the <em>Getting Started</em> guide in the Dataset section of the challenge, for more details on how to access the challenge environments, and also for a basic tutorial on how to make your first submission.</p>

<h3 id=""prizes"">Prizes</h3>

<p>The winner will be invited to the 2nd Applied Machine Learning Days at EPFL in Switzerland on January  29 &amp; 30, 2018, with travel and accommodation covered.</p>

<h3 id=""datasets-license"">Datasets License</h3>

"
183,"<p>The facility management of the KIT collects data for controlling and accounting. All buildings
at the campus north and south are equipped with sensors that measure, among other things,
power or water consumption. An important aspect in energy controlling is a reliable forecast.
Such a forecast is crucial for system operators. Power plants are shut down or restarted on a
regular basis to balance the demand fluctuation in the energy grid. In this task, we will focus
on forecasting the consumption of a few selected buildings on the campus north. With two
years worth of data we seek to forecast the following three months in the third year.</p>

<h2 id=""data"">Data</h2>

<p>The data is collected by the monitoring system and stored in a database. The data for each
sensor is a time series. A time series is a sequence of values $x_t$ where $t$ corresponds to the
points in time when a value is actually measured. In our case, a value is recorded every 15
minutes.</p>

<p>We have exported the data exactly as collected by the system, i.e., raw measurement data.
The data has some special charactistics that we explain in the following. Most sensors are
so called meters: They accumulate the consumption which results in a monotonously increasing
value. In order to obtain the actual consumption for a time slot one has to calculate
the difference to the previous measurement. This procedure is called differentiating in time
series analysis. Additionally, the following phenomena may occur:</p>

<ul>
  <li>Meter resets: The individual meter values are sometimes reset to zero (at random times)</li>
  <li>Outliers: Sometimes no value or extremely high values are transmitted. No value may</li>
  <li>No update: Sometimes an old value is repeated until a point where the meter transmits
a new value that is considerably higher. This visually appears as a step in the data.</li>
</ul>

<p>We have selected ten buildings from the campus north. They have different amounts and
types of sensors. The data spans two years (2015 and 2016) and the goal is to forecast the first
three months of 2017.</p>

<h3 id=""file-description"">File Description</h3>
<p>The archive contains the files train, meta and test. Any timestamp is in the following format,
where the first part is the date and the second part the exact time, including milliseconds
and time zone: yyyy-MM-dd’T’HH:mm:ss,SSSZZZZ</p>

<p>A detailed description of the files is given in the <em>Task.pdf</em> in the download section.</p>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>
<p>The task of this challenge is to calculate a forecast for the first three months in 2017.
We use the mean absolute percentage error (MAPE) for the evaluation.
The sensors have different value ranges but MAPE can handle this and summarize the errors of all sensors.
For MAPE, a lower value is better.
Given the $n$ actual values $a_t$ and $n$ forecast values $f_t$, we calculate:
\begin{equation}
MAPE = \dfrac{100}{n} \sum\limits_{t=1}^n\left|\dfrac{a_t - f_t}{a_t}\right|
\end{equation}</p>

<p>Irregularities in the actual data values $a_t$ as previously described have been taken care of. I.e., gaps from missing values were filled by linear interpolation.
Furthermore, we have checked that the test data does not contain any value outliers, i.e., measurement errors, or meter resets.
The MAPE score will be calculated using these corrected actual values.</p>

<h3 id=""resources"">Resources</h3>

<h3 id=""prizes"">Prizes</h3>

<h3 id=""datasets-license"">Datasets License</h3>

"
105,"
<p><img src=""https://aicrowd-static.s3.eu-central-1.amazonaws.com/novartis/dsai-2019/header2.jpg"" alt="""" /></p>

<h1 id=""introduction"">Introduction</h1>

<p>Which of pharma’s biggest challenges can we tackle with data science &amp; AI if we put our collective minds together? Can you help find one of Pharma’s holy grails?  Today, predicting the Probability of Success (PoS) of a development program is based on simple industry benchmarks. Probability of Success is used to support decision making at the highest level: Should we invest potentially hundreds of dollars in a compound and run a Phase 3 trial? Data can guide those decisions and there are several approaches to do so. This is a challenge of such importance that we have teamed up with one of the world’s foremost experts, Prof. Andrew Lo of MIT’s Sloan School of Management.</p>

<h1 id=""objective"">Objective</h1>

<ul>
  <li><strong>The objective:</strong> There are several questions for you to work on, <a href=""#detailed-evaluation-criteria"">details here</a>. You are expected to submit your <a href=""https://gitlab.aicrowd.com"">programs</a> and a presentation (details coming).</li>
</ul>

<h1 id=""data-sources"">Data Sources</h1>

<ul>
  <li><strong>Data sources:</strong> This challenge focuses on publically available data which contains historical information on the trial and program level.  The data will be available in Aridhia.  You may want to connect or upload new data.  If you wish to make this available for the leaderboard evaluation, please contact Nick Kelley.</li>
</ul>

<p>Data dictionaries and some query file and look up table names - as well as future API instructions for data models and linking which we’ve just been approved for will be in the <a href=""https://www.aicrowd.com/challenges/novartis-dsai-challenge/dataset_files"">resources section.</a>   Master variables description table will be <a href=""https://aicrowd-production.s3.eu-central-1.amazonaws.com/dataset_files/challenge_105/a6f093f1-d939-46a0-9798-a2b15da023b9_Master%20data%20variables%20description.xlsx?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAJ6IZH6GWKDCCDFAQ%2F20191114%2Feu-central-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20191114T170920Z&amp;X-Amz-Expires=604800&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Signature=4f8dcafdfff8bed32b256491e0b936b4b37a5ae4714eb8f25d3abb1ca3f2585e"">kept and updated here.</a></p>

<h1 id=""getting-started"">Getting Started</h1>

<ul>
  <li><strong>How to get started:</strong>  The <a href=""https://aicrowd-production.s3.eu-central-1.amazonaws.com/dataset_files/challenge_105/79cc4066-e5fe-4379-ac12-eff71d351c16_README%20-%20Workspace%20Setup.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAJ6IZH6GWKDCCDFAQ%2F20191112%2Feu-central-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20191112T172455Z&amp;X-Amz-Expires=604800&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Signature=28f084fc866adbffe52786e2df443201de55cd22c29e59a80cbcb4f8a7f869c1"">Walk-through for workspace set up</a> and for <a href=""https://aicrowd-production.s3.eu-central-1.amazonaws.com/dataset_files/challenge_105/ad3eee39-fd80-411b-a9db-5dde7fdded5f_README%20-%20Installing%20Software%2001.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAJ6IZH6GWKDCCDFAQ%2F20191114%2Feu-central-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20191114T081434Z&amp;X-Amz-Expires=604800&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Signature=2eca295b8e543d638a0955e0cdba30a4ed6dae7674b989825b8592be833176cb"">Software Installation</a> is available in the <a href=""https://www.aicrowd.com/challenges/novartis-dsai-challenge/dataset_files"">resources section</a>.  This is being merged with the <a href=""https://gitlab.aicrowd.com/novartis/novartis-dsai-challenge-starter-kit"">Starter kit and guide available here</a>.</li>
</ul>

<p><strong>New</strong> Installing software - new guidance updated in the <a href=""https://www.aicrowd.com/challenges/novartis-dsai-challenge/dataset_files"">Resources page</a> - see <code class=""highlighter-rouge"">README - Installing Software 01.pdf</code></p>

<p>note: <a href=""https://teams.microsoft.com/l/team/19%3a40c91df597c94e54bd8103e627a06de6%40thread.skype/conversations?groupId=d60d820c-2323-4fea-b2cd-41e1d89db164&amp;tenantId=f35a6974-607f-47d4-82d7-ff31d7dc53a5"">Live Platform Support with Aridhia in MS Teams</a></p>

<p>In summary, this is what you’ll need to do at least once per team:</p>

<ol>
  <li>Set up your workspace environment based on how you work</li>
  <li>Get the <a href=""https://gitlab.aicrowd.com/novartis/novartis-dsai-challenge-starter-kit"">gitlab project starter code</a> into your workspace</li>
  <li>Have some fun:  Use our rstudio and/or jupyter python tutorials from the starter kit to load data, train, and save a model</li>
  <li>Get on the leader board by making your first test submission!</li>
</ol>

<p><strong>Platforms:</strong> <a href=""https://www.aridhia.com/"">Aridhia</a> supplies the environment and compute used for the data challenge, AIcrowd provides you the leaderboard and discussions.
<strong>Leaderboard:</strong> The <a href=""https://www.aicrowd.com/challenges/novartis-dsai-challenge/leaderboards"">leaderboard</a> will focus on one of the questions: to predict the probability of approval.  Details and rules can be found <a href=""https://www.aicrowd.com/challenges/novartis-dsai-challenge/leaderboards"">here</a>.  The evaluation committee will evaluate all other challenges outside of the leaderboard.</p>

<h1 id=""prizes"">Prizes</h1>

<ul>
  <li><strong>Prizes:</strong> There will be five prize categories: Leaderboard Performance, Data Wrangling, Innovation, and Insights: general, trial, and program level insights. Note that your presentation is the face of your work, present yourself well!</li>
</ul>

<h1 id=""timeline"">Timeline</h1>

<ul>
  <li><strong>Timeframe:</strong>  The challenge will end 20 December 2019 at 18.00 CET.</li>
</ul>

<p><img src=""https://aicrowd-static.s3.eu-central-1.amazonaws.com/novartis/dsai-2019/roadmap.jpg"" alt="""" /></p>

<h1 id=""discussion"">Discussion</h1>

<ul>
  <li><strong>Questions &amp; Discussion:</strong>  If you have additional questions, please submit them in our <a href=""https://discourse.aicrowd.com/c/novartis-dsai-challenge"">forums</a>.</li>
</ul>

<h1 id=""contact"">Contact</h1>
<p>note: please use our <a href=""https://discourse.aicrowd.com/c/novartis-dsai-challenge"">forums</a> where possible
- Novartis: nicholas.kelley@novartis.com
- Aridhia platform: rodrigo.barnes@aridhia.com
- AI Crowd:  mohanty@aicrowd.com</p>

<h1 id=""detailed-evaluation-criteria"">Detailed Evaluation Criteria</h1>

<p><strong>Data challenge awards, challenges, and evaluation criteria</strong></p>

<p>Teams may attempt any or all of the challenges below, which are grouped into the following categories:</p>

<ul>
  <li>The Leaderboard and most predictive model</li>
  <li>General insights and learning from the model, including specific questions the business team has for clinical trials and over-arching clinical programs</li>
  <li>Data wrangling: enabling better predictions and insights by linking more data</li>
</ul>

<p>The core dataset is composed of two historical trial and drug data sets from Informa which have been linked together.  (details in resources)  Please note that your solutions to the challenges may be improved by leveraging more information than is contained in the core dataset. Teams are encouraged to explore the impact of adding additional information to the core dataset, and can make this available centrally to all participants for leaderboard evaluations.  (see rules)</p>

<p><strong>The «Leaderboard»:  Model Performance and Predictive Power</strong>
The challenge: Predict the chance of obtaining regulatory approval while imagining you are planning your Phase 3 program. Regulatory outcomes for programs are contained in the “outcome” column of the data frame which is mapped from the variable Dev Status.</p>

<ul>
  <li><strong>Evaluation criterion:</strong> We want to emulate a real world decision making scenario after a Phase 2 trial:whether or not to invest in a Phase 3 program. To that aim, we will train on only Phase 2 data before 2015 and try to predict the Regulatory Approval outcome as described above. The performance of your algorithm on a hold-out dataset will be measured using the log-loss metric and used to rank your solution.</li>
</ul>

<p><strong>Data wrangling / engineering</strong></p>

<ul>
  <li>
    <p><strong>The challenge:</strong> Bring and link new data sets which allow additional features to be incorporated in algorithms either to provide new scientific insights (see these challenges listed below) or  for the Leaderboard competition of predicting the chance of obtaining regulatory approval. External publicly available data or data which is provided can be used.
In order to use such data for the Leaderboard and methods competition, you will need to publish the new data and a high level description to the central workspace for all participants.  Participants using new data will be asked to demonstrate the performance boost of including it.</p>
  </li>
  <li>
    <p><strong>Evaluation criteria:</strong></p>

    <ul>
      <li>Gains in performance (as assessed by your team or by the Leaderboard) made by using the new data to train the algorithm</li>
      <li>Innovation of your data wrangling / engineering approach</li>
      <li>Novelty of your approach for validating your own algorithm</li>
    </ul>
  </li>
</ul>

<p><strong>Presentation and visualization</strong></p>

<ul>
  <li>
    <p><strong>The challenge:</strong> Communicate your data, insights and results to non-data-scientists</p>
  </li>
  <li>
    <p><strong>Evaluation criteria:</strong></p>
    <ul>
      <li>Clarity, transparency, scientific integrity of information</li>
      <li>Teams will be shortlisted based on their submission and invited to give a presentation to the evaluation committee, including leadership representation from portfolio strategy,  biostatistics,the Digital Office and of course  Prof. Andrew Lo.</li>
    </ul>
  </li>
</ul>

<p><strong>General insights:</strong></p>

<ul>
  <li><strong>The challenge:</strong>
    <ul>
      <li>Explain which drivers are most important for predictions, and/or predictions in certain areas</li>
      <li>Explore and visualise how various features relate to one another</li>
      <li>Show for a given trial/program prediction which specific features were most influential in its prediction</li>
      <li>Bonus – try to give an example of how the insights above would support portfolio decision making and help to balance risk across programs / trials</li>
    </ul>
  </li>
  <li><strong>Additional evaluation criteria:</strong>
    <ul>
      <li>Communicate your insights in as intuitive and accessible a way as possible</li>
    </ul>
  </li>
</ul>

<p><strong>Scientific program level insights:</strong></p>

<ul>
  <li><strong>The challenge:</strong>
    <ul>
      <li>Can predicting trial success improve your prediction of regulatory approval?</li>
      <li>What is the chance of obtaining regulatory approval in the following scenarios, and what are the drivers for successful regulatory approval for each of them?
        <ul>
          <li>without Phase 2, with Phase 3</li>
          <li>with Phase 2, without Phase 3</li>
          <li>without Phase 2, without Phase 3</li>
        </ul>
      </li>
      <li>Can you predict reason(s) for failure of the program, as provide by the variable “Trial Outcome”.</li>
      <li>Can you predict regulatory approval, imagining you are prior to starting Phase 2?. What are the key drivers?</li>
    </ul>
  </li>
  <li><strong>Evaluation criteria:</strong>
    <ul>
      <li>Innovation of your ML approach and interpretability</li>
      <li>Novelty of your approach for validating your own algorithm</li>
      <li>Approach for handling additional or missing data</li>
    </ul>
  </li>
</ul>

<p><strong>Scientific trial level insights</strong></p>

<ul>
  <li><strong>The challenge</strong>
    <ul>
      <li>Identify the most important drivers of success for phase 2 (phase 3) trials and describe their impact. These drivers should be known at the time of planning phase 2 (phase 3). [variable: trial outcome]</li>
      <li>What features predict that a trial will be terminated due to safety/adverse effects?</li>
      <li>How does the importance of different features in predicting trial success vary across phases?</li>
      <li>What are the recommendations for designing dose-finding studies (phase 2 trial with 3 and more doses) which would ultimately increase the chance of approval?</li>
      <li>Imagine you are planning a phase 3 trial. Can you predict the duration of the trial? What are the key drivers of Phase 3 trial duration – as provided by the column “duration” in the core data.</li>
      <li>Can you predict reason(s) for failure in a study (efficacy, safety, operational, …) as outlined in the Trial Outcome column/variable?</li>
    </ul>
  </li>
  <li><strong>Evaluation criteria</strong>
    <ul>
      <li>Innovation of your ML approach and interpretability</li>
      <li>Novelty of your approach for validating your own algorithm</li>
      <li>Approach for handling additional or missing data</li>
    </ul>
  </li>
</ul>

<p><strong>Outside the box thinking and Innovation:</strong></p>

<ul>
  <li>The challenge: Surprise us!</li>
  <li>The evaluation: There are no predetermined criteria for this prize. An evaluation team will identify the winner.</li>
</ul>

"
153,"<p>Like never before, the web has become a place for sharing creative work - such as music - among a global community of artists and art lovers. While music and music collections predate the web, the web enabled much larger scale collections. Whereas people used to own a handful of vinyls or CDs, they nowadays have instant access to the whole of published musical content via online platforms. Such dramatic increase in the size of music collections created two challenges: (i) the need to automatically organize a collection (as users and publishers cannot manage them manually anymore), and (ii) the need to automatically recommend new songs to a user knowing his listening habits. An underlying task in both those challenges is to be able to group songs in semantic categories.</p>

<p>Music genres are categories that have arisen through a complex interplay of cultures, artists, and market forces to characterize similarities between compositions and organize music collections. Yet, the boundaries between genres still remain fuzzy, making the problem of music genre recognition (MGR) a nontrivial task (Scaringella 2006). While its utility has been debated, mostly because of its ambiguity and cultural definition, it is widely used and understood by end-users who find it useful to discuss musical categories (McKay 2006). As such, it is one of the most researched areas in the Music Information Retrieval (MIR) field (Sturm 2012).</p>

<p>The task of this challenge, one of the four official challenges of the <a href=""https://www2018.thewebconf.org/program/challenges-track/"">Web Conference (WWW2018) challenges track</a>, is to recognize the musical genre of a piece of music of which only a recording is available. Genres are broad, e.g. pop or rock, and each song only has one target genre. The data for this challenge comes from the recently published <a href=""https://github.com/mdeff/fma"">FMA dataset</a> (Defferrard 2017), which is a dump of the <a href=""https://freemusicarchive.org"">Free Music Archive (FMA)</a>, an interactive library of high-quality and curated audio which is freely and openly available to the public.</p>

<h2 id=""results"">Results</h2>

<p>You can find the final results and the ranking on the <a href=""https://github.com/crowdAI/crowdai-musical-genre-recognition-starter-kit"">repository</a> and in the <a href=""https://doi.org/10.5281/zenodo.1243501"">slides used to announce them</a>.</p>

<p>In the interest of reproducibility and transparency for interested
researchers, you’ll find below links to the source code repositories of all
systems submitted by the participants for the second round of the challenge.</p>

<ol>
  <li>Transfer Learning of Artist Group Factors to Musical Genre Classification
    <ul>
      <li>Jaehun Kim (<a href=""https://www.crowdai.org/participants/jaehun"">@jaehun</a>), TU Delft and Minz Won (<a href=""https://www.crowdai.org/participants/minzwon"">@minzwon</a>), Universitat Pompeu Fabra</li>
      <li>Code: <a href=""https://gitlab.crowdai.org/minzwon/WWWMusicalGenreRecognitionChallenge"">https://gitlab.crowdai.org/minzwon/WWWMusicalGenreRecognitionChallenge</a></li>
      <li>Paper: <a href=""https://doi.org/10.1145/3184558.3191823"">https://doi.org/10.1145/3184558.3191823</a></li>
    </ul>
  </li>
  <li>Ensemble of CNN-based Models using various Short-Term Input
    <ul>
      <li>Hyungui Lim (<a href=""https://www.crowdai.org/participants/hglim"">@hglim</a>), <a href=""http://cochlear.ai"">http://cochlear.ai</a></li>
      <li>Code: <a href=""https://gitlab.crowdai.org/hglim/WWWMusicalGenreRecognitionChallenge"">https://gitlab.crowdai.org/hglim/WWWMusicalGenreRecognitionChallenge</a></li>
    </ul>
  </li>
  <li>Detecting Music Genre Using Extreme Gradient Boosting
    <ul>
      <li>Benjamin Murauer (<a href=""https://www.crowdai.org/participants/benjamin_murauer"">@benjamin_murauer</a>), Universität Innsbruck</li>
      <li>Code: <a href=""https://gitlab.crowdai.org/Benjamin_Murauer/WWWMusicalGenreRecognitionChallenge"">https://gitlab.crowdai.org/Benjamin_Murauer/WWWMusicalGenreRecognitionChallenge</a></li>
      <li>Paper: <a href=""https://doi.org/10.1145/3184558.3191822"">https://doi.org/10.1145/3184558.3191822</a></li>
    </ul>
  </li>
  <li>ConvNet on STFT spectrograms
    <ul>
      <li>Daniyar Chumbalov (<a href=""https://www.crowdai.org/participants/check"">@check</a>), EPFL and Philipp Pushnyakov (<a href=""https://www.crowdai.org/participants/gg12"">@gg12</a>), Moscow Institute of Physics and Technologies (MIPT)</li>
      <li>Code: <a href=""https://gitlab.crowdai.org/gg12/WWWMusicalGenreRecognitionChallenge"">https://gitlab.crowdai.org/gg12/WWWMusicalGenreRecognitionChallenge</a></li>
    </ul>
  </li>
  <li><a href=""https://arxiv.org/pdf/1610.02357"">Xception</a> on mel-scaled spectrograms
    <ul>
      <li><a href=""https://www.crowdai.org/participants/viper"">@viper</a> and <a href=""https://www.crowdai.org/participants/algohunt"">@algohunt</a></li>
      <li>Code: <a href=""https://gitlab.crowdai.org/viper/WWWMusicalGenreRecognitionChallenge"">https://gitlab.crowdai.org/viper/WWWMusicalGenreRecognitionChallenge</a></li>
    </ul>
  </li>
  <li>Audio <a href=""https://arxiv.org/abs/1707.01629"">Dual Path Networks</a> on mel-scaled spectrograms
    <ul>
      <li>Sungkyun Chang (<a href=""https://www.crowdai.org/participants/mimbres"">@mimbres</a>), Seoul National University</li>
      <li>Code: <a href=""https://gitlab.crowdai.org/mimbres/WWWMusicalGenreRecognitionChallenge"">https://gitlab.crowdai.org/mimbres/WWWMusicalGenreRecognitionChallenge</a></li>
    </ul>
  </li>
</ol>

<p>The repositories should be self-contained and easily executable. You can
execute any of the systems on your own mp3s by following those steps:</p>

<ol>
  <li>Clone the git repository.</li>
  <li><a href=""Round2_packaging_guidelines.md#building-a-docker-image"">Build a docker image with <code class=""highlighter-rouge"">repo2docker</code></a></li>
  <li><a href=""Round2_packaging_guidelines.md#executing-the-docker-image"">Execute the docker image</a></li>
</ol>

<p>You can find more details in the <a href=""https://doi.org/10.5281/zenodo.1243501"">slides used to announce the
results</a> and in the <a href=""https://arxiv.org/abs/1803.05337"">overview paper</a>. The <a href=""https://arxiv.org/abs/1803.05337"">overview paper</a> summarizes our experience running a challenge with open data for musical genre recognition. Those notes motivate the task and the challenge design, show some statistics about the submissions, and present the results. Please cite our paper in your scholarly work if you want to reference this challenge.</p>

<div class=""highlighter-rouge""><div class=""highlight""><pre class=""highlight""><code>@inproceedings{fma_crowdai_challenge,
  title = {Learning to Recognize Musical Genre from Audio},
  author = {Defferrard, Micha\""el and Mohanty, Sharada P. and Carroll, Sean F. and Salath\'e, Marcel},
  booktitle = {WWW '18 Companion: The 2018 Web Conference Companion},
  year = {2018},
  url = {https://arxiv.org/abs/1803.05337},
}
</code></pre></div></div>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>
<p>To avoid overfitting and cheating, the challenge will happen in <strong>two rounds</strong>. The final ranking will be based on results from the second round.
In the first round, participants are provided a test set of 35,000 clips of 30 seconds each, and they have to submit their predictions for all the 35,000 clips. The platform evaluates the predictions and ranks the participant upon submission.
In the second round, all the participants will have to wrap their models in a Docker container. We will evaluate those against a new unseen test set. These 30s clips will be sampled (at least in part) from new contributions to the <a href=""https://freemusicarchive.org"">Free Music Archive</a>.</p>

<p>Details of how to package your code as <a href=""https://mybinder.readthedocs.io/en/latest/"">Binder</a> compatible repositories, please read the documentation here : <a href=""https://github.com/crowdAI/crowdai-musical-genre-recognition-starter-kit/blob/master/Round2_Packaging_Guidelines.md"">https://github.com/crowdAI/crowdai-musical-genre-recognition-starter-kit/blob/master/Round2_Packaging_Guidelines.md</a></p>

<p>The primary metric for evaluation will be the Mean <a href=""http://scikit-learn.org/stable/modules/model_evaluation.html#log-loss"">Log Loss</a>, and the secondary metric for the evaluation with be the Mean <a href=""https://en.wikipedia.org/wiki/F1_score"">F1-Score</a>.</p>

<p>The <strong>Mean Log Loss</strong> is defined by</p>

<p>$ L = - \frac{1}{N} \sum_{n=1}^N \sum_{c=1}^{C} y_{nc} \ln(p_{nc}), $</p>

<p>where</p>

<ul>
  <li>$N=35000$ is the number of examples in the test set,</li>
  <li>$C=16$ is the number of class labels, i.e. genres,</li>
  <li>$y_{nc}$ is a binary value indicating if the n-th instance belongs to the c-th label,</li>
  <li>$p_{nc}$ is the probability according to your submission that the n-th instance belongs to the c-th label,</li>
  <li>$\ln$ is the natural logarithmic function.</li>
</ul>

<p>The $F_1$ score for a particular class $c$ is given by</p>

<p>$ F_1^c = 2\frac{p^c r^c}{p^c + r^c}, $</p>

<p>where</p>

<ul>
  <li>$p^c = \frac{tp^c}{tp^c + fp^c}$ is the precision for class $c$,</li>
  <li>$r^c = \frac{tp^c}{tp^c + fn^c}$ is the recall for class $c$,</li>
  <li>$tp^c$ refers to the number of True Positives for class $c$,</li>
  <li>$fp^c$ refers to the number of False Positives for class $c$,</li>
  <li>$fn^c$ refers to the number of False Negatives for class $c$.</li>
</ul>

<p>The final <strong>Mean $F_1$ Score</strong> is then defined as</p>

<p>$ F_1 = \frac{1}{C} \sum_{c=1}^{C} F_1^c. $</p>

<p>The participants have to submit a CSV file with the following header:</p>

<p><code class=""highlighter-rouge"">
file_id,Blues,Classical,Country,Easy Listening,Electronic,Experimental,Folk,Hip-Hop,Instrumental,International,Jazz,Old-Time / Historic,Pop,Rock,Soul-RnB,Spoken
</code></p>

<p>Each row is then an entry for every file in the test set (in the sorted order of the <code class=""highlighter-rouge"">file_id</code>s). The first column in every row represents the <code class=""highlighter-rouge"">file_id</code> (which is the name of the test file without its <code class=""highlighter-rouge"">.mp3</code> extension) and the rest of the $C=16$ columns are the predicted probabilities for each class in the order mentioned in the above CSV header.</p>

<h3 id=""resources"">Resources</h3>

<p>Please refer to the <a href=""https://www.crowdai.org/challenges/www-2018-challenge-learning-to-recognize-musical-genre/dataset_files"">dataset page</a> for more information about the training and test data, as well as download links.</p>

<p>The <a href=""https://github.com/crowdAI/crowdai-musical-genre-recognition-starter-kit"">starter kit</a> includes code to handle the data an make a submission. Moreover, it features some examples and baselines.</p>

<p>You are encouraged to check out the <a href=""https://github.com/mdeff/fma"">FMA dataset GitHub repository</a> for Jupyter notebooks showing how to use the data, exploring it, and training baseline models. This challenge uses the <code class=""highlighter-rouge"">rc1</code> version of the data, make sure to checkout that version of the code. The associated <a href=""https://arxiv.org/abs/1612.01840"">paper</a> describes the data.</p>

<p>Additional resources:</p>

<ul>
  <li><a href=""https://github.com/ybayle/awesome-deep-learning-music"">A list of scientific articles about deep learning applied to music</a></li>
  <li><a href=""https://arxiv.org/abs/1709.04396"">A Tutorial on Deep Learning for Music Information Retrieval</a></li>
  <li><a href=""https://github.com/faroit/awesome-python-scientific-audio"">A list of python software/tools for scientific research in audio/music</a></li>
  <li><a href=""https://github.com/tuwien-musicir/DL_MIR_Tutorial"">Deep learning tutorial for genre recognition with Keras</a></li>
  <li><a href=""https://github.com/mlachmish/MusicGenreClassification"">Music Genre Classification with Deep Learning in TensorFlow</a></li>
</ul>

<p>Public contact channels:</p>

<ul>
  <li><a href=""https://gitter.im/crowdAI/WWW-Music-Genre-Recognition-Challenge"">Gitter channel</a></li>
  <li><a href=""https://www.crowdai.org/challenges/www-2018-challenge-learning-to-recognize-musical-genre/topics"">Discussion forum</a></li>
</ul>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at:</p>

<ul>
  <li><a href=""mailto:michael.defferrard@epfl.ch"" target=""_blank"">michael.defferrard@epfl.ch</a></li>
  <li><a href=""mailto:sharada.mohanty@epfl.ch"" target=""_blank"">sharada.mohanty@epfl.ch</a></li>
</ul>

<h3 id=""prizes"">Prizes</h3>

<p>The winner will be invited to present their solution to the 3rd <a href=""https://www.appliedmldays.org"">Applied Machine Learning Days</a> at EPFL in Switzerland in January 2019, with travel and accommodation covered (up to $2000).</p>

<p>Moreover, all participants are invited to submit a paper to the <a href=""https://www2018.thewebconf.org/program/challenges-track/"">Web Conference (WWW2018) challenges track</a>. The paper should describe the proposed solution and self-assessments of its performance. Papers must be submitted in PDF on <a href=""https://easychair.org/conferences/?conf=www2018-fma-challenge"">EasyChair</a> for peer-review. The template to use is <a href=""https://www.acm.org/publications/proceedings-template"">ACM</a>, selecting the “sigconf” sample (as for the main conference). Submissions should not exceed five pages including any diagrams or appendices, plus unlimited pages of references. As the challenge is run publicly, reviews are not double-blind and papers should not be anonymized. Accepted papers will be published in the official satellite proceedings of the conference. As the challenge will continue after the submission deadline, authors of accepted papers will have the opportunity to submit a camera-ready version which will incorporate their latest tweaks. The event at the conference will be like a workshop, where participants present their solutions and we announce the winners.</p>

<h2 id=""timeline"">Timeline</h2>

<p>Below is the timeline of the challenge:</p>

<ul>
  <li>2017-12-07 Challenge start.</li>
  <li>2018-02-09 Paper submission deadline.</li>
  <li>2018-02-14 Paper acceptance notification.</li>
  <li>2018-03-01 End of the first round. No new participants can enroll.</li>
  <li>2018-04-08 Participants have to submit a docker container for the second round.</li>
  <li>2018-04-27 Announcement of winners and presentation of accepted papers at the conference.</li>
</ul>

<h3 id=""datasets-license"">Datasets License</h3>

"
201,"
<h3 id=""extract-roads-from-satellite-images"">Extract roads from satellite images</h3>

<p>For this problem, we provide a set of satellite/aerial images acquired from GoogleMaps. We also provide ground-truth images where each pixel is labeled as {road, background}. Your goal is to train a classifier to segment roads in these images, i.e. assign a label {road=1, background=0} to each pixel. Please see detailed instructions on the course github.</p>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>
<p>Your algorithm is evaluated according to the following criterion:</p>

<ul>
  <li><a href=""https://en.wikipedia.org/wiki/F1_score""> F1 score </a> (this combines the two numbers of precision and recall)</li>
</ul>

<h3 id=""resources"">Resources</h3>

<h3 id=""prizes"">Prizes</h3>

<h3 id=""datasets-license"">Datasets License</h3>

"
184,"<p>The facility management of the KIT collects data for controlling and accounting. All buildings
at the campus north and south are equipped with sensors that measure, among other things,
power or water consumption. An important aspect in energy controlling is a reliable forecast.
Such a forecast is crucial for system operators. Power plants are shut down or restarted on a
regular basis to balance the demand fluctuation in the energy grid. In this task, we will focus
on forecasting the consumption of a few selected buildings on the campus north. With two
years worth of data we seek to forecast the following three months in the third year.</p>

<h2 id=""data"">Data</h2>

<p>The data is collected by the monitoring system and stored in a database. The data for each
sensor is a time series. A time series is a sequence of values $x_t$ where $t$ corresponds to the
points in time when a value is actually measured. In our case, a value is recorded every 15
minutes.</p>

<p>We have exported the data exactly as collected by the system, i.e., raw measurement data.
The data has some special charactistics that we explain in the following. Most sensors are
so called meters: They accumulate the consumption which results in a monotonously increasing
value. In order to obtain the actual consumption for a time slot one has to calculate
the difference to the previous measurement. This procedure is called differentiating in time
series analysis. Additionally, the following phenomena may occur:</p>

<ul>
  <li>Meter resets: The individual meter values are sometimes reset to zero (at random times)</li>
  <li>Outliers: Sometimes no value or extremely high values are transmitted. No value may</li>
  <li>No update: Sometimes an old value is repeated until a point where the meter transmits
a new value that is considerably higher. This visually appears as a step in the data.</li>
</ul>

<p>We have selected 100 buildings from the campus north. They have different amounts and
types of sensors. The training data spans five years (2012-2016). The goal is to predict weeks in 2017. In total you have to predict 13 weeks in the year 2017 following the pattern:
| 3 week test_train | 1 week test predict |</p>

<h3 id=""file-description"">File Description</h3>
<p>The archive contains the files train, meta and test. Any timestamp is in the following format,
where the first part is the date and the second part the exact time, including the time zone: yyyy-MM-dd HH:mm:ssZZZZ</p>

<p>A detailed description of the files is given in the full task description.</p>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>
<p>We use the mean absolute scaled error (MASE) for the evaluation.
The sensors have different value ranges but MASE can handle this and summarize the errors of all sensors.
For MASE, a lower value is better.
MASE is a scaled error meaning that we the error of the forecast is rated relatively to a naive forecast.
In our case the naive forecast is a repetition of the week immediately before the week that needs to be forecast (i.e. week 3 in the test train in each of the 13 splits).
A MASE value below 1 stands for a prediction better than the baseline and a value above 1 a worse one.</p>

<p>Per sensor $i$ we first calculate the absolute scaled error: Given the $n$ actual values $a_{i, t}$, $n$ forecast values $f_{i, t}$ and the error of the naive forecast for this sensor $\text{err-naive}_{i}$, we calculate:</p>

<p>\begin{equation}
ASE_i = \dfrac{\sum\limits_{t=1}^n\left|a_{i, t} - f_{i, t}\right|}{\text{err-naive}_{i}}
\end{equation}</p>

<p>Afterwards we aggregate the individual ASE of each sensor by taking the mean, where $N$ is the number of sensors
\begin{equation}
MASE = \dfrac{1}{N}\sum\limits_{i=1}^N ASE_i
\end{equation}</p>

<h3 id=""resources"">Resources</h3>

<h3 id=""prizes"">Prizes</h3>

<h3 id=""datasets-license"">Datasets License</h3>

"
148,"<p><strong>THIS CHALLENGE IS OVER, BUT THERE IS <a href=""https://www.crowdai.org/challenges/nips-2018-ai-for-prosthetics-challenge"">A NEW ONE AT NIPS 2018</a></strong></p>

<p>Updates for participants: <strong>Please read about the latest changes and the logistics of the second round <a href=""https://github.com/stanfordnmbl/osim-rl/tree/master/docs"">here</a> and <a href=""https://www.crowdai.org/topics/important-announcement-round-2-submission/discussion"">here</a> and <a href=""https://www.crowdai.org/topics/important-announcement-round-2-updates-submission-screencast/discussion"">here</a>(last update November 6th).</strong></p>

<p>Welcome to <strong>Learning to Run</strong>, one of the 5 official challenges in the <a href=""https://nips.cc/Conferences/2017/CompetitionTrack"">NIPS 2017 Competition Track</a>. In this competition, you are tasked with developing a controller to enable a physiologically-based human model to navigate a complex obstacle course as quickly as possible. You are provided with a human musculoskeletal model and a physics-based simulation environment where you can synthesize physically and physiologically accurate motion. Potential obstacles include external obstacles like steps, or a slippery floor, along with internal obstacles like muscle weakness or motor noise. You are scored based on the distance you travel through the obstacle course in a set amount of time.</p>

<p>Our objectives are to:</p>

<ul>
  <li>bring Deep Reinforcement Learning to solve problems in medicine,</li>
  <li>promote open-source tools in RL research (the physics simulator, the RL environment, and the competition platform are all open-source),</li>
  <li>encourage RL research in computationally complex environments, with stochasticity and highly-dimensional action spaces.</li>
</ul>

<p>Follow the instructions in the <a href=""https://www.crowdai.org/challenges/nips-2017-learning-to-run/dataset_files"">Getting Started guide in the Dataset section of the challenge</a> and visit our <a href=""https://github.com/stanfordnmbl/osim-rl"">github repo</a> to get started!</p>

<p><strong>First Prize – NVIDIA DGX Station™</strong></p>

<p><a href=""https://s3-eu-west-1.amazonaws.com/kidzinski/opensim-ami/nvidia-station.png""><img src=""https://s3-eu-west-1.amazonaws.com/kidzinski/opensim-ami/nvidia-station.png"" alt=""NVIDIA Station"" width=""240px"" /></a></p>

<p>NVIDIA DGX Station™ is the Fastest Personal Supercomputer for Researchers and Data Scientists.</p>

<p><strong>Computing support – Amazon AWS cloud credits</strong></p>

<p><strong>Amazon AWS has generously agreed to support participants of the challenge with $30,000 worth of cloud credits.</strong> The top 100 performers as per the leaderboard on August 13th, 2017, 23:59:59 UTC, received $300 AWS cloud credits.</p>

<h2 id=""partners"">Partners</h2>

<p><img src=""https://s3.amazonaws.com/salathegroup-static/nips/logos/Stanford.png"" alt=""stanford"" class=""img-logo"" />
<img src=""https://s3.amazonaws.com/salathegroup-static/nips/logos/epfl.png"" alt=""epfl"" class=""img-logo"" />
<img src=""https://s3.amazonaws.com/salathegroup-static/nips/logos/Berkeley.png"" alt=""berkley"" class=""img-logo"" />
<a href=""http://mobilize.stanford.edu/""><img src=""https://s3.amazonaws.com/salathegroup-static/nips/logos/mobilize.png"" alt=""stanford mobilize"" class=""img-logo"" /></a></p>

<h2 id=""sponsors"">Sponsors</h2>

<p><a href=""https://aws.amazon.com/""><img src=""https://upload.wikimedia.org/wikipedia/commons/thumb/1/1d/AmazonWebservices_Logo.svg/2000px-AmazonWebservices_Logo.svg.png"" alt=""Amazon AWS"" class=""img-logo"" /></a>
<a href=""https://nvidia.com/""><img src=""https://vignette1.wikia.nocookie.net/logopedia/images/3/38/Nvidia_logo.png/revision/latest?cb=20120829072950"" alt=""NVIDIA"" class=""img-logo"" /></a>
<a href=""http://www.tri.global/""><img src=""https://s3-eu-west-1.amazonaws.com/kidzinski/nips-challenge/tri1.png"" alt=""TRI"" class=""img-logo"" /></a></p>

<h2 id=""media"">Media</h2>

<p><a href=""https://techcrunch.com/2017/08/07/dueling-ais-compete-in-learning-to-walk-secretly-manipulating-images-and-more-at-nips/""><img src=""https://seeklogo.com/images/T/techcrunch-logo-B444826970-seeklogo.com.png"" alt=""TechCrunch"" class=""img-logo"" /></a>
<a href=""http://news.stanford.edu/2017/08/07/virtual-competitors-vie-different-kind-athletic-title/""><img src=""https://cehg.stanford.edu/sites/default/files/styles/large-scaled/public/c876e3f31ce0c5ba771fbdccdcb3c1dc.png?itok=-83R2NJW"" alt=""Stanford News"" class=""img-logo"" /></a>
<a href=""http://insights.globalspec.com/article/6167/watch-computer-generated-skeletons-run-for-cerebral-palsy""><img src=""https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/50fa0a860a431c503132b1fa0cac8377_logo%20%283%29.png"" alt=""IEEE"" class=""img-logo"" /></a></p>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>
<p>Your task is to build a function f which takes the current state observation (a 41 dimensional vector) and returns the muscle excitations action (18 dimensional vector) in a way that maximizes the reward. Your total reward is the position of the pelvis on the x axis after the last iteration minus a penalty for using ligament forces. Ligaments are tissues which prevent your joints from bending too much - overusing these tissues leads to injuries, so we want to avoid it. The penalty in the total reward is equal to the sum of forces generated by ligaments over the trial, divided by 1000. For details on evaluation please refer to the <a href=""https://www.crowdai.org/challenges/nips-2017-learning-to-run/dataset_files"">Getting Started guide in the Dataset section of the challenge</a>.</p>

<h3 id=""resources"">Resources</h3>

<p>Please refer to the <em>Getting Started</em> guide in the Dataset section of the challenge, for more details on how to access the challenge environments, and also for a basic tutorial on how to make your first submission.</p>

<p>We are in the process of compiling the book chapter for the Book on the NIPS Challenge Track this year. But in the meantime, here are some interesting articles and blog posts written by participants :</p>

<ul>
  <li><a href=""https://medium.com/mlreview/our-nips-2017-learning-to-run-approach-b80a295d3bb5"">https://medium.com/mlreview/our-nips-2017-learning-to-run-approach-b80a295d3bb5</a></li>
  <li><a href=""https://arxiv.org/abs/1711.06922"">https://arxiv.org/abs/1711.06922</a></li>
  <li><a href=""https://medium.com/@scitator/run-skeleton-run-3rd-place-solution-for-nips-2017-learning-to-run-207f9cc341f8"">https://medium.com/@scitator/run-skeleton-run-3rd-place-solution-for-nips-2017-learning-to-run-207f9cc341f8</a></li>
</ul>

<h2 id=""contact-us"">Contact Us</h2>

<ul>
  <li>Gitter Channel : <a href=""https://gitter.im/crowdAI/NIPS-Learning-To-Run-Challenge"">crowdAI/NIPS-Learning-To-Run-Challenge</a></li>
  <li>Technical issues : <a href=""https://github.com/stanfordnmbl/osim-rl/issues"" target=""_blank"">https://github.com/stanfordnmbl/osim-rl/issues </a></li>
  <li>Discussion Forum : <a href=""https://www.crowdai.org/challenges/nips-2017-learning-to-run/topics"">https://www.crowdai.org/challenges/nips-2017-learning-to-run/topics</a></li>
</ul>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organisers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

<ul>
  <li><a href=""mailto:lukasz.kidzinski@stanford.edu"" target=""_blank""> lukasz.kidzinski@stanford.edu
 </a></li>
  <li><a href=""mailto:sharada.mohanty@epfl.ch"" target=""_blank""> sharada.mohanty@epfl.ch
 </a></li>
</ul>

<h3 id=""prizes"">Prizes</h3>

<p>1st - NVIDIA DGX Station™</p>

<p>2nd - NVIDIA Titan Xp</p>

<p>3rd - NVIDIA Titan Xp</p>

<p>Additionally:</p>

<ul>
  <li>Invitation to publish articles in the NIPS competition book.</li>
  <li>Invitation to the 2nd <a href=""https://www.appliedmldays.org"">Applied Machine Learning Days</a> at EPFL in Switzerland on January 29 &amp; 30, 2018, with travel and accommodation covered.</li>
  <li>Invitation to give a research talk at Stanford, with travel and accommodation covered.</li>
  <li>Reimbursement of travel and accommodation at NIPS 2017</li>
</ul>

<p>[<img src=""https://s3-eu-west-1.amazonaws.com/kidzinski/opensim-ami/nvidia-station.png"" alt=""NVIDIA Station"" width=""240px"" />]</p>

<p>NVIDIA DGX Station™ is the Fastest Personal Supercomputer for Researchers and Data Scientists” with the following benefits:</p>

<ul>
  <li>Revolutionary form factor - designed for the desk, whisper-quiet</li>
  <li>Start experimenting in hours, not weeks, powered by DGX Stack</li>
  <li>Productivity that goes from desk  to data center to cloud</li>
  <li>Breakthrough performance and precision – powered by Volta</li>
</ul>

<h3 id=""datasets-license"">Datasets License</h3>

"
186,"<p>Welcome to the <em>Adversarial Vision Challenge</em>, one of the official challenges in the <a href=""https://nips.cc"" target=""_blank""> NIPS 2018 </a> competition track. In this competition you can take on the role of an attacker or a defender (or both). As a defender you are trying to build a visual object classifier that is as robust to image perturbations as possible. As an attacker, your task is to find the smallest possible image perturbations that will fool a classifier.</p>

<p>The overall goal of this challenge is to facilitate measurable progress towards robust machine vision models and more generally applicable adversarial attacks. As of right now, modern machine vision algorithms are extremely susceptible to small and almost imperceptible perturbations of their inputs (so-called <em>adversarial examples</em>). This property reveals an astonishing difference in the information processing of humans and machines and raises security concerns for many deployed machine vision systems like autonomous cars. Improving the robustness of vision algorithms is thus important to close the gap between human and machine perception and to enable safety-critical applications.</p>

<p><img src=""https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/ba0e38f517ff26613f4c225aed4f3b51_competition_illustration.png"" alt=""Illustration Adversarial Examples"" style=""width: 800px"" /></p>

<h2 id=""competition-tracks"">Competition tracks</h2>

<p>There will be three tracks in which you and your team can compete:</p>

<ul>
  <li><a href=""https://www.crowdai.org/organizers/bethgelab/challenges/nips-2018-adversarial-vision-challenge-robust-model-track"" target=""_blank"">Robust Model Track</a></li>
  <li><a href=""https://www.crowdai.org/organizers/bethgelab/challenges/nips-2018-adversarial-vision-challenge-untargeted-attack-track"" target=""_blank"">Untargeted Attacks Track</a></li>
  <li><a href=""https://www.crowdai.org/organizers/bethgelab/challenges/nips-2018-adversarial-vision-challenge-targeted-attack-track/"" target=""_blank"">Targeted Attacks Track</a></li>
</ul>

<p>In this track your task is to build and train a robust model on <a href=""https://tiny-imagenet.herokuapp.com/"" target=""_blank"">tiny ImageNet</a>. The attacks will try to find small image perturbations that change the prediction of your model to the wrong class. The larger these perturbations are the better is your score (see below).</p>

<h2 id=""evaluation-criterion"">Evaluation criterion</h2>

<p>Models are scored as follows (higher is better):</p>

<ul>
  <li>Let M be the model and S be the set of samples.</li>
  <li>We apply the five best untargeted attacks on M for each sample in S.</li>
  <li>For each sample we record the minimum adversarial L2 distance (MAD) across the attacks.</li>
  <li>If a model misclassifies a sample then the minimum adversarial distance is registered as zero for this sample.</li>
  <li>The final model score is the median MAD across all samples.</li>
  <li>The higher the score, the better.</li>
</ul>

<p>The top-5 attacks against which submissions are evaluated are fixed for two weeks at a time after which we evaluate all current submissions to determine the new top-5 attacks for the upcoming two weeks.</p>

<h2 id=""timeline"">Timeline</h2>

<p><em>(tentative)</em>.<br />
* <strong>June 25th, 2018</strong>  : Challenge begins   <br />
* <strong>November 1st</strong> : Final submission date <br />
* <strong>November 15th</strong> : Winners Announced</p>

<h2 id=""submissions"">Submissions</h2>

<p>To make a submission, please follow the instructions in this GitLab repository:
<a href=""https://gitlab.crowdai.org/adversarial-vision-challenge/nips18-avc-model-template"" target=""_blank""> https://gitlab.crowdai.org/adversarial-vision-challenge/nips18-avc-model-template </a></p>

<p>Fork the above <strong>template repository</strong> in GitLab and follow the instructions stated in the README.md.
You need to have a crowdAI-account and sign in to GitLab using this account.
In the README you will also find links to multiple fully functional examples.</p>

<h2 id=""organizing-team"">Organizing Team</h2>

<p>The organizing team comes from multiple groups — <a href=""https://www.uni-tuebingen.de/en/university.html"">University of Tübingen</a>, <a href=""https://research.google.com/teams/brain/"">Google Brain</a>, <a href=""https://www.epfl.ch/index.en.html"">EPFL</a> and <a href=""http://www.psu.edu/"">Pennsylvania State University</a>.</p>

<p>The Team consists of:  <br />
*  <a href=""https://twitter.com/wielandbr"" target=""_blank""> Wieland Brendel </a> <br />
*  <a href=""https://jonasrauber.de"" target=""_blank""> Jonas Rauber </a> <br />
*  <a href=""https://twitter.com/alexey2004"" target=""_blank""> Alexey Kurakin </a> <br />
*  <a href=""https://twitter.com/NicolasPapernot"" target=""_blank""> Nicolas Papernot </a> <br />
*  <a href=""https://twitter.com/beveliqi"" target=""_blank""> Behar Veliqi </a> <br />
*  <a href=""https://twitter.com/MeMohanty"" target=""_blank""> Sharada P. Mohanty </a>  <br />
*  <a href=""https://twitter.com/marcelsalathe"" target=""_blank""> Marcel Salathé </a> <br />
*  <a href=""https://twitter.com/MatthiasBethge"" target=""_blank""> Matthias Bethge </a></p>

<h2 id=""sponsors"">Sponsors</h2>

<table>
  <tbody>
    <tr>
      <td><img src=""https://upload.wikimedia.org/wikipedia/commons/thumb/9/93/Amazon_Web_Services_Logo.svg/200px-Amazon_Web_Services_Logo.svg.png"" alt=""Amazon AWS"" style=""width: 120px"" /></td>
      <td><img src=""https://www.dgincubation.co.jp/wp-content/uploads/portfolio/paperspace-eyecatch.png"" alt=""Paperspace"" style=""width: 240px"" /></td>
    </tr>
  </tbody>
</table>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>

<h3 id=""resources"">Resources</h3>

<h2 id=""contact-us"">Contact Us</h2>

<ul>
  <li>Gitter Channel : <a href=""https://gitter.im/crowdAI/nips-2018-adversarial-vision-challenge"">crowdAI/nips-2018-adversarial-vision-challenge</a></li>
  <li>Discussion Forum : <a href=""https://www.crowdai.org/challenges/nips-2018-adversarial-vision-challenge/topics"">https://www.crowdai.org/challenges/nips-2018-adversarial-vision-challenge/topics</a></li>
</ul>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

<ul>
  <li><a href=""mailto:wieland.brendel@bethgelab.org"" target=""_blank""> wieland.brendel@bethgelab.org
 </a></li>
  <li><a href=""mailto:behar.veliqi@bethgelab.org"" target=""_blank"">behar.veliqi@bethgelab.org</a></li>
  <li><a href=""mailto:sharada.mohanty@epfl.ch"" target=""_blank""> sharada.mohanty@epfl.ch
 </a></li>
</ul>

<h3 id=""prizes"">Prizes</h3>

<ul>
  <li><strong>$15.000 worth of Paperspace cloud compute credits</strong>: The top-20 teams in each track (defense, untargeted attack, targeted attack) as of 28. September will receive 250$ each.</li>
</ul>

<h3 id=""datasets-license"">Datasets License</h3>

"
187,"<p>Welcome to the <em>Adversarial Vision Challenge</em>, one of the official challenges in the <a href=""https://nips.cc"" target=""_blank""> NIPS 2018 </a> competition track. In this competition you can take on the role of an attacker or a defender (or both). As a defender you are trying to build a visual object classifier that is as robust to image perturbations as possible. As an attacker, your task is to find the smallest possible image perturbations that will fool a classifier.</p>

<p>The overall goal of this challenge is to facilitate measurable progress towards robust machine vision models and more generally applicable adversarial attacks. As of right now, modern machine vision algorithms are extremely susceptible to small and almost imperceptible perturbations of their inputs (so-called <em>adversarial examples</em>). This property reveals an astonishing difference in the information processing of humans and machines and raises security concerns for many deployed machine vision systems like autonomous cars. Improving the robustness of vision algorithms is thus important to close the gap between human and machine perception and to enable safety-critical applications.</p>

<p><img src=""https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/ba0e38f517ff26613f4c225aed4f3b51_competition_illustration.png"" alt=""Illustration Adversarial Examples"" style=""width: 800px"" /></p>

<h2 id=""competition-tracks"">Competition tracks</h2>

<p>There will be three tracks in which you and your team can compete:</p>

<ul>
  <li><a href=""https://www.crowdai.org/organizers/bethgelab/challenges/nips-2018-adversarial-vision-challenge-robust-model-track"" target=""_blank"">Robust Model Track</a></li>
  <li><a href=""https://www.crowdai.org/organizers/bethgelab/challenges/nips-2018-adversarial-vision-challenge-untargeted-attack-track"" target=""_blank"">Untargeted Attacks Track</a></li>
  <li><a href=""https://www.crowdai.org/organizers/bethgelab/challenges/nips-2018-adversarial-vision-challenge-targeted-attack-track/"" target=""_blank"">Targeted Attacks Track</a></li>
</ul>

<p>This track is very similar to the  <a href=""https://www.crowdai.org/organizers/bethgelab/challenges/nips-2018-adversarial-vision-challenge-untargeted-attack-track"" target=""_blank"">untargeted attacks</a> track. The only difference is that here an adversarial perturbation is not defined as making the model predict <em>any</em> wrong label but it has to get the model to predict a <em>particular</em> (wrong) label.</p>

<h2 id=""evaluation-criterion"">Evaluation criterion</h2>

<p>Attacks are scored as follows (lower is better):</p>

<ul>
  <li>Let A be the attack and S be the set of samples.</li>
  <li>We apply attack A against the best five models for each sample in S.</li>
  <li>If an attack fails to produce a (targeted) adversarial for a given sample, then we register a worst case distance (distance of the sample to a uniform grey image).</li>
  <li>The final attack score is the median L2 distance across samples.</li>
</ul>

<p>The top-5 models against which submissions are evaluated are fixed for two weeks at a time after which we evaluate all current submissions to determine the new top-5 models for the upcoming two weeks.</p>

<h2 id=""submissions"">Submissions</h2>

<p>To make a submission, please follow the instructions in this GitLab repository:
<a href=""https://gitlab.crowdai.org/adversarial-vision-challenge/nips18-avc-attack-template"" target=""_blank""> https://gitlab.crowdai.org/adversarial-vision-challenge/nips18-avc-attack-template </a></p>

<p>Fork the above <strong>template repository</strong> in GitLab and follow the instructions stated in the README.md.
You need to have a crowdAI-account and sign in to GitLab using this account.
In the README you will also find links to multiple fully functional examples.</p>

<h2 id=""timeline"">Timeline</h2>

<p><em>(tentative)</em>.<br />
* <strong>June 25th, 2018</strong>  : Challenge begins   <br />
* <strong>November 1st</strong> : Final submission date <br />
* <strong>November 15th</strong> : Winners Announced</p>

<h2 id=""organizing-team"">Organizing Team</h2>

<p>The organizing team comes from multiple groups — <a href=""https://www.uni-tuebingen.de/en/university.html"">University of Tübingen</a>, <a href=""https://research.google.com/teams/brain/"">Google Brain</a>, <a href=""https://www.epfl.ch/index.en.html"">EPFL</a> and <a href=""http://www.psu.edu/"">Pennsylvania State University</a>.</p>

<p>The Team consists of:  <br />
*  <a href=""https://twitter.com/wielandbr"" target=""_blank""> Wieland Brendel </a> <br />
*  <a href=""https://jonasrauber.de"" target=""_blank""> Jonas Rauber </a> <br />
*  <a href=""https://twitter.com/alexey2004"" target=""_blank""> Alexey Kurakin </a> <br />
*  <a href=""https://twitter.com/NicolasPapernot"" target=""_blank""> Nicolas Papernot </a> <br />
*  <a href=""https://twitter.com/beveliqi"" target=""_blank""> Behar Veliqi </a> <br />
*  <a href=""https://twitter.com/MeMohanty"" target=""_blank""> Sharada P. Mohanty </a>  <br />
*  <a href=""https://twitter.com/marcelsalathe"" target=""_blank""> Marcel Salathé </a> <br />
*  <a href=""https://twitter.com/MatthiasBethge"" target=""_blank""> Matthias Bethge </a></p>

<h2 id=""sponsors"">Sponsors</h2>

<table>
  <tbody>
    <tr>
      <td><img src=""https://upload.wikimedia.org/wikipedia/commons/thumb/9/93/Amazon_Web_Services_Logo.svg/200px-Amazon_Web_Services_Logo.svg.png"" alt=""Amazon AWS"" style=""width: 120px"" /></td>
      <td><img src=""https://www.dgincubation.co.jp/wp-content/uploads/portfolio/paperspace-eyecatch.png"" alt=""Paperspace"" style=""width: 240px"" /></td>
    </tr>
  </tbody>
</table>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>

<h3 id=""resources"">Resources</h3>

<h2 id=""contact-us"">Contact Us</h2>

<ul>
  <li>Gitter Channel : <a href=""https://gitter.im/crowdAI/nips-2018-adversarial-vision-challenge"">crowdAI/nips-2018-adversarial-vision-challenge</a></li>
  <li>Discussion Forum : <a href=""https://www.crowdai.org/challenges/nips-2018-adversarial-vision-challenge/topics"">https://www.crowdai.org/challenges/nips-2018-adversarial-vision-challenge/topics</a></li>
</ul>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

<ul>
  <li><a href=""mailto:wieland.brendel@bethgelab.org"" target=""_blank""> wieland.brendel@bethgelab.org
 </a></li>
  <li><a href=""mailto:behar.veliqi@bethgelab.org"" target=""_blank"">behar.veliqi@bethgelab.org</a></li>
  <li><a href=""mailto:sharada.mohanty@epfl.ch"" target=""_blank""> sharada.mohanty@epfl.ch
 </a></li>
</ul>

<h3 id=""prizes"">Prizes</h3>

<ul>
  <li><strong>$15.000 worth of Paperspace cloud compute credits</strong>: The top-20 teams in each track (defense, untargeted attack, targeted attack) as of 28. September will receive 250$ each.</li>
</ul>

<h3 id=""datasets-license"">Datasets License</h3>

"
202,"<p>See detailed instructions on the course github, including the PDF project description.</p>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>
<p>Your submission will be evaluated in terms of classification error (accuracy).</p>

<h3 id=""resources"">Resources</h3>

<h3 id=""prizes"">Prizes</h3>

<h3 id=""datasets-license"">Datasets License</h3>

"
188,"<p>Welcome to the <em>Adversarial Vision Challenge</em>, one of the official challenges in the <a href=""https://nips.cc"" target=""_blank""> NIPS 2018 </a> competition track. In this competition you can take on the role of an attacker or a defender (or both). As a defender you are trying to build a visual object classifier that is as robust to image perturbations as possible. As an attacker, your task is to find the smallest possible image perturbations that will fool a classifier.</p>

<p>The overall goal of this challenge is to facilitate measurable progress towards robust machine vision models and more generally applicable adversarial attacks. As of right now, modern machine vision algorithms are extremely susceptible to small and almost imperceptible perturbations of their inputs (so-called <em>adversarial examples</em>). This property reveals an astonishing difference in the information processing of humans and machines and raises security concerns for many deployed machine vision systems like autonomous cars. Improving the robustness of vision algorithms is thus important to close the gap between human and machine perception and to enable safety-critical applications.</p>

<p><img src=""https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/ba0e38f517ff26613f4c225aed4f3b51_competition_illustration.png"" alt=""Illustration Adversarial Examples"" style=""width: 800px"" /></p>

<h2 id=""competition-tracks"">Competition tracks</h2>

<p>There will be three tracks in which you and your team can compete:</p>

<ul>
  <li><a href=""https://www.crowdai.org/organizers/bethgelab/challenges/nips-2018-adversarial-vision-challenge-robust-model-track"" target=""_blank"">Robust Model Track</a></li>
  <li><a href=""https://www.crowdai.org/organizers/bethgelab/challenges/nips-2018-adversarial-vision-challenge-untargeted-attack-track"" target=""_blank"">Untargeted Attacks Track</a></li>
  <li><a href=""https://www.crowdai.org/organizers/bethgelab/challenges/nips-2018-adversarial-vision-challenge-targeted-attack-track/"" target=""_blank"">Targeted Attacks Track</a></li>
</ul>

<p>In this track you build an attack algorithm that breaks the defenses. For each model and each given image your attack tries to find the smallest perturbation that makes the model predict a wrong class label (so-called <em>adversarial perturbations</em>). Your attack will be able to craft model-specific adversarials by asking the model for its prediction on self-defined inputs (up to 1000 times / image). The smaller the adversarial perturbations are that your attack finds (on average), the better is your score (the exact scoring formula will be published soon).</p>

<h2 id=""evaluation-criterion"">Evaluation criterion</h2>

<p>Attacks are scored as follows (lower is better):</p>

<ul>
  <li>Let A be the attack and S be the set of samples.</li>
  <li>We apply attack A against the best five models for each sample in S.</li>
  <li>If an attack fails to produce a (targeted) adversarial for a given sample, then we register a worst case distance (distance of the sample to a uniform grey image).</li>
  <li>The final attack score is the median L2 distance across samples.</li>
</ul>

<p>The top-5 models against which submissions are evaluated are fixed for two weeks at a time after which we evaluate all current submissions to determine the new top-5 models for the upcoming two weeks.</p>

<h2 id=""submissions"">Submissions</h2>

<p>To make a submission, please follow the instructions in this GitLab repository:
<a href=""https://gitlab.crowdai.org/adversarial-vision-challenge/nips18-avc-attack-template"" target=""_blank""> https://gitlab.crowdai.org/adversarial-vision-challenge/nips18-avc-attack-template </a></p>

<p>Fork the above <strong>template repository</strong> in GitLab and follow the instructions stated in the README.md.
You need to have a crowdAI-account and sign in to GitLab using this account.
In the README you will also find links to multiple fully functional examples.</p>

<h2 id=""timeline"">Timeline</h2>

<p><em>(tentative)</em>.<br />
* <strong>June 25th, 2018</strong>  : Challenge begins   <br />
* <strong>November 1st</strong> : Final submission date <br />
* <strong>November 15th</strong> : Winners Announced</p>

<h2 id=""organizing-team"">Organizing Team</h2>

<p>The organizing team comes from multiple groups — <a href=""https://www.uni-tuebingen.de/en/university.html"">University of Tübingen</a>, <a href=""https://research.google.com/teams/brain/"">Google Brain</a>, <a href=""https://www.epfl.ch/index.en.html"">EPFL</a> and <a href=""http://www.psu.edu/"">Pennsylvania State University</a>.</p>

<p>The Team consists of:  <br />
*  <a href=""https://twitter.com/wielandbr"" target=""_blank""> Wieland Brendel </a> <br />
*  <a href=""https://jonasrauber.de"" target=""_blank""> Jonas Rauber </a> <br />
*  <a href=""https://twitter.com/alexey2004"" target=""_blank""> Alexey Kurakin </a> <br />
*  <a href=""https://twitter.com/NicolasPapernot"" target=""_blank""> Nicolas Papernot </a> <br />
*  <a href=""https://twitter.com/beveliqi"" target=""_blank""> Behar Veliqi </a> <br />
*  <a href=""https://twitter.com/MeMohanty"" target=""_blank""> Sharada P. Mohanty </a>  <br />
*  <a href=""https://twitter.com/marcelsalathe"" target=""_blank""> Marcel Salathé </a> <br />
*  <a href=""https://twitter.com/MatthiasBethge"" target=""_blank""> Matthias Bethge </a></p>

<h2 id=""sponsors"">Sponsors</h2>

<table>
  <tbody>
    <tr>
      <td><img src=""https://upload.wikimedia.org/wikipedia/commons/thumb/9/93/Amazon_Web_Services_Logo.svg/200px-Amazon_Web_Services_Logo.svg.png"" alt=""Amazon AWS"" style=""width: 120px"" /></td>
      <td><img src=""https://www.dgincubation.co.jp/wp-content/uploads/portfolio/paperspace-eyecatch.png"" alt=""Paperspace"" style=""width: 240px"" /></td>
    </tr>
  </tbody>
</table>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>

<h3 id=""resources"">Resources</h3>

<h2 id=""contact-us"">Contact Us</h2>

<ul>
  <li>Gitter Channel : <a href=""https://gitter.im/crowdAI/nips-2018-adversarial-vision-challenge"">crowdAI/nips-2018-adversarial-vision-challenge</a></li>
  <li>Discussion Forum : <a href=""https://www.crowdai.org/challenges/nips-2018-adversarial-vision-challenge/topics"">https://www.crowdai.org/challenges/nips-2018-adversarial-vision-challenge/topics</a></li>
</ul>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

<ul>
  <li><a href=""mailto:wieland.brendel@bethgelab.org"" target=""_blank""> wieland.brendel@bethgelab.org
 </a></li>
  <li><a href=""mailto:behar.veliqi@bethgelab.org"" target=""_blank"">behar.veliqi@bethgelab.org</a></li>
  <li><a href=""mailto:sharada.mohanty@epfl.ch"" target=""_blank""> sharada.mohanty@epfl.ch
 </a></li>
</ul>

<h3 id=""prizes"">Prizes</h3>

<ul>
  <li><strong>$15.000 worth of Paperspace cloud compute credits</strong>: The top-20 teams in each track (defense, untargeted attack, targeted attack) as of 28. September will receive 250$ each.</li>
</ul>

<h3 id=""datasets-license"">Datasets License</h3>

"
189,"<p>Use cases for machine-type communications are developing very rapidly. There has been enormous interest in integrating connectivity solutions with sensors, actuators, meters, cars, appliances, and so on. The Internet of Things (IoT) is thus being created and constantly expanded. IoT consists of a number of networks that may have different design objectives. For example, some networks only intend to cover local area (e.g. one single home) whereas some networks offer wide-area coverage.</p>

<p>Narrowband Internet of Things (NB-IoT) is a new cellular technology for providing wide-area coverage for the Internet of Things (IoT). NB-IoT is developed to enable a wide range of devices and services to be connected using cellular telecommunications bands i.e. “narrow” bands. NB-IoT focuses specifically on indoor coverage, low cost, long battery life, and enabling a large number of connected devices.</p>

<p>Through its very low power requirements NB-IoT aims to help connect every kind of device to the Internet of Things. However, if a network coverage cannot be provided to a sufficient level, e.g. as in the case of basements or deep buildings, a connection might not be feasible at all.</p>

<p><img src=""https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/2d5c37a0050bb2791eb828f384c96e39_Picture1.png"" alt=""Picture1.png"" /></p>

<h1 id=""task"">Task</h1>

<p>Information about network coverage is especially important for network service providers to maximize their reach to their customers. Rohde &amp; Schwarz already helps those companies to evaluate the quality of their infrastructure, providing multiple tools for a wide range of technologies. However, as the Internet of Things takes off, new challenges appear along with it. Measurements for this kind of network have to be taken where IoT devices will be deployed and as many of those will be deployed in commercial and private areas, naturally with restricted access, this becomes a too complex task to be handled manually. Using AI to predict if NB-IoT connections are possible from outside places of interest would remove the overhead that is typically associated with the manual measurements that are usually taken. This would greatly improve the ability to provide meaningful data for network service providers and thus, improve the network connections around us all!</p>

<p>Participants of the challenge will be given two sets of NB-IoT measurement data from different places of interest and their surrounding area along with a values indicating if connections could be established. In addition to the raw measurement data, a descriptor of the place of interest, e.g. “basement”, “garage”, will also be provided. Participants are required to write a script that produces an AI capable of predicting if this connection is possible on the basis of the first mentioned dataset. Participants are then required to use their AI on the second dataset which only contains measurement from outside the place of interest, adding the same type of connection identifier as in the first set based on their predictions. The resulting AI is supposed to predict whether a connection is possible by evaluating data measured outside of the places of interest, e.g. will an IoT device be able to connect inside a building judging from street data.</p>

<p>In short, your task is to:</p>

<ol>
  <li>Create an AI that predicts if a mobile will be to connect to the network at given signal conditions</li>
  <li>Train the AI on signal propagation, leading to a signal propagation model covering the area of interest</li>
  <li>Combine both into a model at which point what radio conditions can be expected and if the mobile is expected to be able to connect</li>
</ol>

<p><img src=""https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/48830cbfba9c44963a6d5da5eb7d7d0e_46503_06.jpg"" alt=""46503_06.jpg"" /></p>

<p><em>Rohde &amp; Schwarz measurement devices for network quality testing</em></p>

<p><strong>Script - this could be useful to add to the starter kit</strong>
There is also a very early version of a script that plots the data by GPS coordinates.</p>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>
<p>Participants will be given two datasets. One larger training set contains measurement data from outside and within the places of interest. It also contains an identifier of whether a NB-IoT connection could be established inside or not. The second set only contains data from outside the places of interest. Participants are required to produce an AI capable of predicting if a connection is possible inside the place from the measurement data outside. Predictions could also be mapped out to a heat map as in the picture below.</p>

<p><img src=""https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/b1b9170c9f698d90e96e6f0b0860a9be_Screen%20Shot%202018-03-13%20at%2011.01.17.png"" alt=""Screen Shot 2018-03-13 at 11.01.17.png"" /></p>

<p>The script creating the AI will be committed for Rohde &amp; Schwarz to run. After internal tests with the submitted script, a winner will be chosen based upon the precision of the resulting AI on the second dataset.</p>

<h3 id=""resources"">Resources</h3>

<h3 id=""prizes"">Prizes</h3>

<h3 id=""datasets-license"">Datasets License</h3>

"
190,"<blockquote>
  <p>UPDATE: Results of the final evaluation are up to date. They have been re-evaluted (detailed explanation can be found <a href=""https://groups.google.com/forum/#!topic/vizdoom/UQXrW6rJupM"">here</a>).</p>
</blockquote>

<h1 id=""results-of-the-final-evaluation"">Results of the final evaluation</h1>

<p>From 51 teams submitted 204 agents, we’ve selected 4 best submissions from 4 different teams according to <a href=""https://www.crowdai.org/challenges/visual-doom-ai-competition-2018-singleplayer-track-1/leaderboards"">leaderboard</a> on 10 new unknown easy maps.</p>

<h2 id=""winner-tsailhttpswwwcrowdaiorgparticipantstsail-tsinghua-university--tencent-ai-lab"">Winner: <a href=""https://www.crowdai.org/participants/tsail"">TSAIL</a> (Tsinghua University &amp; Tencent AI Lab)</h2>

<h2 id=""st-runner-up-doomnethttpswwwcrowdaiorgparticipantsdoomnet-andrey-kolishchak"">1st Runner-Up: <a href=""https://www.crowdai.org/participants/doomnet"">DoomNet</a> (Andrey Kolishchak)</h2>

<h2 id=""nd-runner-up-viplabhttpswwwcrowdaiorgparticipantsviplab-from-tsinghua-universitybrbr"">2nd Runner-Up: <a href=""https://www.crowdai.org/participants/viplab"">VIPLAB</a> (from Tsinghua University)<br /><br /></h2>

<table>
  <thead>
    <tr>
      <th style=""text-align: left"">Team</th>
      <th style=""text-align: left"">Bot</th>
      <th style=""text-align: right"">Map 1</th>
      <th style=""text-align: right"">2</th>
      <th style=""text-align: right"">3</th>
      <th style=""text-align: right"">4</th>
      <th style=""text-align: right"">5</th>
      <th style=""text-align: right"">6</th>
      <th style=""text-align: right"">7</th>
      <th style=""text-align: right"">8</th>
      <th style=""text-align: right"">9</th>
      <th style=""text-align: right"">10</th>
      <th style=""text-align: right"">Total time (m)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style=""text-align: left""><strong><a href=""https://www.crowdai.org/participants/tsail"">TSAIL</a></strong></td>
      <td style=""text-align: left""><strong>TSAIL</strong></td>
      <td style=""text-align: right"">3.90</td>
      <td style=""text-align: right"">5.00</td>
      <td style=""text-align: right"">0.72</td>
      <td style=""text-align: right"">0.34</td>
      <td style=""text-align: right"">0.82</td>
      <td style=""text-align: right"">0.32</td>
      <td style=""text-align: right"">4.06</td>
      <td style=""text-align: right"">5.00</td>
      <td style=""text-align: right"">5.00</td>
      <td style=""text-align: right"">0.20</td>
      <td style=""text-align: right"">25.34</td>
    </tr>
    <tr>
      <td style=""text-align: left""><strong><a href=""https://www.crowdai.org/participants/doomnet"">DoomNet</a></strong></td>
      <td style=""text-align: left""><strong>DoomNet</strong></td>
      <td style=""text-align: right"">0.97</td>
      <td style=""text-align: right"">5.00</td>
      <td style=""text-align: right"">0.95</td>
      <td style=""text-align: right"">1.97</td>
      <td style=""text-align: right"">5.00</td>
      <td style=""text-align: right"">0.33</td>
      <td style=""text-align: right"">5.00</td>
      <td style=""text-align: right"">5.00</td>
      <td style=""text-align: right"">5.00</td>
      <td style=""text-align: right"">0.65</td>
      <td style=""text-align: right"">29.86</td>
    </tr>
    <tr>
      <td style=""text-align: left""><strong><a href=""https://www.crowdai.org/participants/viplab"">VIPLAB</a></strong></td>
      <td style=""text-align: left""><strong>agent_viplab</strong></td>
      <td style=""text-align: right"">2.45</td>
      <td style=""text-align: right"">5.00</td>
      <td style=""text-align: right"">5.00</td>
      <td style=""text-align: right"">0.86</td>
      <td style=""text-align: right"">2.42</td>
      <td style=""text-align: right"">0.48</td>
      <td style=""text-align: right"">5.00</td>
      <td style=""text-align: right"">5.00</td>
      <td style=""text-align: right"">5.00</td>
      <td style=""text-align: right"">0.33</td>
      <td style=""text-align: right"">31.54</td>
    </tr>
    <tr>
      <td style=""text-align: left""><a href=""https://www.crowdai.org/participants/ddangelo"">ddangelo</a></td>
      <td style=""text-align: left"">DoomGai</td>
      <td style=""text-align: right"">5.00</td>
      <td style=""text-align: right"">5.00</td>
      <td style=""text-align: right"">5.00</td>
      <td style=""text-align: right"">5.00</td>
      <td style=""text-align: right"">0.65</td>
      <td style=""text-align: right"">1.00</td>
      <td style=""text-align: right"">5.00</td>
      <td style=""text-align: right"">5.00</td>
      <td style=""text-align: right"">5.00</td>
      <td style=""text-align: right"">0.68</td>
      <td style=""text-align: right"">37.33</td>
    </tr>
  </tbody>
</table>

<h1 id=""visual-doom-ai-competition-at-cig2018httpsprojectdkemaastrichtuniversitynlcig2018-singleplayer-track-1"">Visual Doom AI Competition at <a href=""https://project.dke.maastrichtuniversity.nl/cig2018/"">CIG2018</a> Singleplayer Track (1)</h1>

<p>This competition is run on <a href=""https://project.dke.maastrichtuniversity.nl/cig2018"">Computational Intelligence and Games Conference 2018</a>.</p>

<p>The task here is to create an Artificial Intelligence agent that is able to finish Doom game levels (not the original levels but randomly generated ones) using data normally available to human players (mostly visual data) without any auxilliary information. The agent has to use the <a href=""https://github.com/mwydmuch/ViZDoom"">ViZDoom framework</a> that gives real-time access to the screen.</p>

<p><strong>Track 1</strong> challenges agents to beat  <strong>single player</strong> levels as fast as possible. Levels vary in difficulty with time so the entry threshold is low - you do not need sophisticated knowledge to start and you can learn on the run!</p>

<p>Multiplayer (track 2) challenge page is available <a href=""https://www.crowdai.org/organizers/poznan-university-of-technology/challenges/visual-doom-ai-competition-2018-multiplayer-track-2"">here</a>.</p>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>
<p>The task is to create a bot that completes randomly generated levels of Doom, filled with monsters and resources.
<a href=""https://github.com/mwydmuch/PyOblige"">PyOblige</a> random generator is provided for training. The final evaluation will also use maps generated by this generator.</p>

<p>Bots will be ranked by the time of completion of a set of levels. The competition times will be summed up. Death will reset progress without resetting the timer. Each level will have limited time to complete, that will also be the maximum time for a level (lowest score).</p>

<p>Generated maps may contain various monsters, hazardous surfaces (acid and lava), weapons and items, doors (that need to be opened with the USE key). Difficulty levels will be moderated as the competition progresses. At the beginning, test evaluation will take place on really simple maps. If the submitted controllers beat them with ease, the difficulty will be increased.</p>

<p>During the public evaluation period bots will be scored after submission and the leaderboard will be updated. During final 2 weeks, the scores will be reset and kept secret. Only a list of participants will be published.</p>

<h3 id=""resources"">Resources</h3>

<h2 id=""repository-with-a-sample-random-submission"">Repository with a sample (random) submission</h2>
<p><a href=""https://github.com/crowdAI/vizdoom2018-singleplayer-starter-kit"" target=""_blank"">https://github.com/crowdAI/vizdoom2018-singleplayer-starter-kit  </a></p>

<h2 id=""contact-us"">Contact Us</h2>

<ul>
  <li>Gitter Channel : <a href=""https://gitter.im/crowdAI/vizdoom2018"">crowdAI/vizdoom2018</a></li>
  <li>Technical Issues : <a href=""https://github.com/crowdAI/vizdoom2018-singleplayer-starter-kit/issues"">https://github.com/crowdAI/vizdoom2018-singleplayer-starter-kit/issues</a></li>
  <li>Discussion Forum : <a href=""https://www.crowdai.org/challenges/visual-doom-ai-competition-2018-track-1/topics"">https://www.crowdai.org/challenges/visual-doom-ai-competition-2018-track-1/topics</a></li>
</ul>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

<ul>
  <li><a href=""mailto:mkempka@.cs.put.poznan.pl"" target=""_blank"">mkempka@cs.put.poznan.pl</a></li>
  <li><a href=""mailto:mwydmuch@cs.put.poznan.pl"" target=""_blank"">mwydmuch@cs.put.poznan.pl</a></li>
  <li><a href=""mailto:sharada.mohanty@epfl.ch"" target=""_blank""> sharada.mohanty@epfl.ch
 </a></li>
</ul>

<h3 id=""prizes"">Prizes</h3>

<p>Top 3 places at the end of the challenge (August 12, 2018 - 23:59 UTC) will be awarded <strong>500USD/300USD/200USD</strong> from IEEE CIS  if eligible. For more information on awarding policy please consult <a href=""https://cis.ieee.org/student-games-based-competition/awarding-policy-and-winners.html"">this website</a></p>

<p>We are also looking for sponsors!</p>

<p>Want to help or support us in any way? <a href=""http://vizdoom.cs.put.edu.pl/authors"" target=""_blank""> Contact us! </a></p>

<h3 id=""datasets-license"">Datasets License</h3>

<p>Example</p>
"
191,"<blockquote>
  <p>UPDATE: Added results of the final evaluation.</p>
</blockquote>

<h1 id=""results"">Results</h1>
<p>From 33 teams submitted 152 agents, we’ve selected 3 best submissions from 3 different teams according to <a href=""https://www.crowdai.org/challenges/visual-doom-ai-competition-2018-multiplayer-track-2/leaderboards"">leaderboard</a> for final evaluation on 10 new unknown maps.
We’ve added two best bots from 2017’s edition and the best bot from 2016’s edition.</p>

<h2 id=""winner-bwbellhttpswwwcrowdaiorgparticipantsbwbell-ben-bell"">Winner: <a href=""https://www.crowdai.org/participants/bwbell"">bwbell</a> (Ben Bell)</h2>

<h2 id=""st-runner-up-tsailabhttpswwwcrowdaiorgparticipantstsailab-9ff41213-0276-407f-9678-ad8625f7419a-tsinghua-university--tencent-ai-lab"">1st Runner-Up: <a href=""https://www.crowdai.org/participants/tsailab-9ff41213-0276-407f-9678-ad8625f7419a"">TSAILAB</a> (Tsinghua University &amp; Tencent AI Lab)</h2>

<h2 id=""nd-runner-up-michaelkraxhttpswwwcrowdaiorgparticipantsmichaelkrax-michael-kraxbrbr"">2nd Runner-Up: <a href=""https://www.crowdai.org/participants/michaelkrax"">michaelkrax</a> (Michael Krax)<br /><br /></h2>

<table>
  <thead>
    <tr>
      <th style=""text-align: left"">Team</th>
      <th style=""text-align: left"">Bot</th>
      <th style=""text-align: right"">Map 1</th>
      <th style=""text-align: right"">2</th>
      <th style=""text-align: right"">3</th>
      <th style=""text-align: right"">4</th>
      <th style=""text-align: right"">5</th>
      <th style=""text-align: right"">6</th>
      <th style=""text-align: right"">7</th>
      <th style=""text-align: right"">8</th>
      <th style=""text-align: right"">9</th>
      <th style=""text-align: right"">10</th>
      <th style=""text-align: right"">Total frags</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style=""text-align: left""><strong><a href=""https://www.crowdai.org/participants/bwbell"">bwbell</a></strong></td>
      <td style=""text-align: left""><strong>Marv2in</strong></td>
      <td style=""text-align: right"">19</td>
      <td style=""text-align: right"">15</td>
      <td style=""text-align: right"">53</td>
      <td style=""text-align: right"">31</td>
      <td style=""text-align: right"">21</td>
      <td style=""text-align: right"">51</td>
      <td style=""text-align: right"">33</td>
      <td style=""text-align: right"">34</td>
      <td style=""text-align: right"">32</td>
      <td style=""text-align: right"">53</td>
      <td style=""text-align: right""><strong>342</strong></td>
    </tr>
    <tr>
      <td style=""text-align: left""><strong><a href=""https://www.crowdai.org/participants/tsailab-9ff41213-0276-407f-9678-ad8625f7419a"">TSAILAB</a></strong></td>
      <td style=""text-align: left""><strong>AWM</strong></td>
      <td style=""text-align: right"">19</td>
      <td style=""text-align: right"">21</td>
      <td style=""text-align: right"">30</td>
      <td style=""text-align: right"">33</td>
      <td style=""text-align: right"">39</td>
      <td style=""text-align: right"">27</td>
      <td style=""text-align: right"">22</td>
      <td style=""text-align: right"">12</td>
      <td style=""text-align: right"">19</td>
      <td style=""text-align: right"">24</td>
      <td style=""text-align: right""><strong>246</strong></td>
    </tr>
    <tr>
      <td style=""text-align: left""><strong><a href=""https://www.crowdai.org/participants/michaelkrax"">michaelkrax</a></strong></td>
      <td style=""text-align: left""><strong>CVFighter</strong></td>
      <td style=""text-align: right"">26</td>
      <td style=""text-align: right"">18</td>
      <td style=""text-align: right"">21</td>
      <td style=""text-align: right"">21</td>
      <td style=""text-align: right"">30</td>
      <td style=""text-align: right"">40</td>
      <td style=""text-align: right"">16</td>
      <td style=""text-align: right"">24</td>
      <td style=""text-align: right"">9</td>
      <td style=""text-align: right"">29</td>
      <td style=""text-align: right""><strong>234</strong></td>
    </tr>
    <tr>
      <td style=""text-align: left""><em>Terminators</em></td>
      <td style=""text-align: left""><em>Arnold4</em> (1st in 2017)</td>
      <td style=""text-align: right"">13</td>
      <td style=""text-align: right"">-4</td>
      <td style=""text-align: right"">16</td>
      <td style=""text-align: right"">15</td>
      <td style=""text-align: right"">15</td>
      <td style=""text-align: right"">9</td>
      <td style=""text-align: right"">16</td>
      <td style=""text-align: right"">24</td>
      <td style=""text-align: right"">8</td>
      <td style=""text-align: right"">17</td>
      <td style=""text-align: right""><em>128</em></td>
    </tr>
    <tr>
      <td style=""text-align: left""><em>TSAIL</em></td>
      <td style=""text-align: left""><em>YanShi</em> (2nd in 2017)</td>
      <td style=""text-align: right"">6</td>
      <td style=""text-align: right"">8</td>
      <td style=""text-align: right"">14</td>
      <td style=""text-align: right"">18</td>
      <td style=""text-align: right"">11</td>
      <td style=""text-align: right"">8</td>
      <td style=""text-align: right"">13</td>
      <td style=""text-align: right"">12</td>
      <td style=""text-align: right"">6</td>
      <td style=""text-align: right"">21</td>
      <td style=""text-align: right""><em>117</em></td>
    </tr>
    <tr>
      <td style=""text-align: left""><em>IntelAct</em></td>
      <td style=""text-align: left""><em>IntelAct</em> (1st in 2016)</td>
      <td style=""text-align: right"">2</td>
      <td style=""text-align: right"">-2</td>
      <td style=""text-align: right"">13</td>
      <td style=""text-align: right"">12</td>
      <td style=""text-align: right"">11</td>
      <td style=""text-align: right"">13</td>
      <td style=""text-align: right"">14</td>
      <td style=""text-align: right"">6</td>
      <td style=""text-align: right"">6</td>
      <td style=""text-align: right"">20</td>
      <td style=""text-align: right""><em>95</em></td>
    </tr>
  </tbody>
</table>

<h1 id=""visual-doom-ai-competition-at-cig2018httpsprojectdkemaastrichtuniversitynlcig2018-multiplayer-track-2"">Visual Doom AI Competition at <a href=""https://project.dke.maastrichtuniversity.nl/cig2018/"">CIG2018</a> Multiplayer Track (2)</h1>

<p>This competition is run on <a href=""https://project.dke.maastrichtuniversity.nl/cig2018"">Computational Intelligence and Games Conference 2018</a>.</p>

<p>The task here is to create an Artificial Intelligence agent that is able to compete with other agents in Doom deathmatches using only data available to regular players without any auxilliary information. The agent has to use the <a href=""https://github.com/mwydmuch/ViZDoom"">ViZDoom framework</a> to connect to the game.</p>

<p><strong>Track 2</strong> is a full on Doom deathmatch (like in previous years) on unknown maps. Agents will compete in multiplayer games and the best <strong>frag</strong> collector will emerge victorious.</p>

<p>Singleplayer (track 1) challenge page is available <a href=""https://www.crowdai.org/organizers/poznan-university-of-technology/challenges/visual-doom-ai-competition-2018-singleplayer-track-1"">here</a>.</p>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>
<p>The task is to create bots that fight against each other in a regular deathmatch, where different weapons and items are available. The bots will be ranked by the number of <a href=""http://doomwiki.org/wiki/Frag"">frags</a>, where the number of frags for this competition is defined as:
<strong>frags = number of killed opponents - number of suicides</strong></p>

<p><a href=""https://github.com/Marqt/ViZDoom/blob/master/scenarios/cig.wad"">5 maps</a> are provided for training and more maps can be found at <a href=""https://www.doomworld.com/idgames/"">Doomworld</a>. The final evaluation will take place on several (secret) testing maps.</p>

<p>During the public evaluations phase bots will play multiple matches with different opponents and the leaderboard will be updated accordingly.</p>

<p>During final 2 weeks the leaderboard will be hidden. Multiple matches on various maps will be played similarly to the public evaluation period. Best bots (probably 8 of them) will take part in a final matches that will determine the top ranking. Finalists will most likely be published before publication of final results at CIG</p>

<h3 id=""resources"">Resources</h3>

<h2 id=""repository-with-a-sample-random-submission"">Repository with a sample (random) submission</h2>
<p><a href=""https://github.com/crowdAI/vizdoom2018-multiplayer-starter-kit"">https://github.com/crowdAI/vizdoom2018-multiplayer-starter-kit</a></p>

<h2 id=""contact-us"">Contact Us</h2>

<ul>
  <li>Gitter Channel : <a href=""https://gitter.im/crowdAI/vizdoom2018"">crowdAI/vizdoom2018</a></li>
  <li>Technical Issues : <a href=""https://github.com/crowdAI/vizdoom2018-multiplayer-starter-kit/issues"">https://github.com/crowdAI/vizdoom2018-multiplayer-starter-kit/issues</a></li>
  <li>Discussion Forum : <a href=""https://www.crowdai.org/challenges/visual-doom-ai-competition-2018-track-1/topics"">https://www.crowdai.org/challenges/visual-doom-ai-competition-2018-track-1/topics</a></li>
</ul>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

<ul>
  <li><a href=""mailto:mkempka@.cs.put.poznan.pl"" target=""_blank"">mkempka@cs.put.poznan.pl</a></li>
  <li><a href=""mailto:mwydmuch@cs.put.poznan.pl"" target=""_blank"">mwydmuch@cs.put.poznan.pl</a></li>
  <li><a href=""mailto:sharada.mohanty@epfl.ch"" target=""_blank""> sharada.mohanty@epfl.ch
 </a></li>
</ul>

<h3 id=""prizes"">Prizes</h3>

<p><strong>UPDATE</strong></p>

<p><strong>The prizes have been updated, as follows:</strong></p>

<p><strong>Top-3 places</strong>: will be awarded <strong>500USD/300USD/200USD</strong> from IEEE CIS  if eligible. For more information on awarding policy please consult <a href=""https://cis.ieee.org/student-games-based-competition/awarding-policy-and-winners.html"">this website</a></p>

<p>Want to help or support us in any way? <a href=""http://vizdoom.cs.put.edu.pl/authors"" target=""_blank""> Contact us! </a></p>

<h3 id=""datasets-license"">Datasets License</h3>

"
192,"
<h3 id=""evaluation-criteria"">Evaluation criteria</h3>

<h3 id=""resources"">Resources</h3>

<h3 id=""prizes"">Prizes</h3>

<h3 id=""datasets-license"">Datasets License</h3>

"
203,"<p>For this choice of project task, you are supposed to predict good recommendations, e.g. of movies to users. We have acquired ratings of 10000 users for 1000 different items (think of movies). All ratings are integer values between 1 and 5 stars. No additional information is available on the movies or users.</p>

<p>All information of the task and some baselines are provided in <a href=""https://github.com/epfml/ML_course/blob/344456134d9b217798c4050e62c3be4d3de96c1c/labs/ex10/exercise10.pdf"">Exercise 10</a></p>

<p>Please see also detailed instructions on the course github.</p>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>
<p>Your collaborative filtering algorithm is evaluated according to the prediction error, measured by root-mean-squared error (RMSE).</p>

<h3 id=""resources"">Resources</h3>

<h3 id=""prizes"">Prizes</h3>

<h3 id=""datasets-license"">Datasets License</h3>

"
193,"
<blockquote>
  <p>Please consider citing the following paper, if you find this work useful : 
Diego Perez-Liebana, Katja Hofmann, Sharada Prasanna Mohanty, Noburu Kuno, Andre Kramer, Sam Devlin, Raluca D. Gaina: <strong>“The Multi-Agent Reinforcement Learning in MalmÖ (MARLÖ) Competition”, 2019, Challenges in Machine Learning (NIPS Workshop), 2018</strong>  <a href=""http://arxiv.org/abs/1901.08129"">http://arxiv.org/abs/1901.08129</a></p>
</blockquote>

<blockquote>
  <p>We are accepting submissions for Round 2. Please feel free to make a submission using the <a href=""https://github.com/crowdAI/marlo-multi-agent-starter-kit/"" target=""_blank"">instructions here</a>.</p>
</blockquote>

<blockquote>
  <p><strong>Due to the delay of opening submission system, we extended the Entry Period to December 31, 2018. Please take a look at the part of Winner Selection for detail.</strong></p>
</blockquote>

<h2 id=""what-is-the-challenge"">What is the Challenge?</h2>

<p>Learning to Play: The Multi-Agent Reinforcement Learning in MalmO Competition (“Challenge”) is a new challenge that proposes research on Multi-Agent Reinforcement Learning using multiple games. Participants would create learning agents that will be able to play multiple 3D games as deﬁned in the MalmO platform. The aim of the competition is to encourage AI research on more general approaches via multi-player games. For this, the Challenge will consist of not one but several games, each one of them with several tasks of varying difficulty and settings. Some of these tasks will be public and participants will be able to train on them. Others, however, will be private, only used to determine the ﬁnal rankings of the competition.</p>

<p>Organizer (Microsoft, Queen Mary University of London and crowdAI) will make the Challenge tasks and sample code available via GitHub on or before the Challenge start date. Entries will be judged by Organizer according to the criteria outlined in the “Winner Selection” section below.</p>

<p>The following prizes will be awarded: (1) the top 7 team will be awarded a MARLO Travel Grant with a maximum value of <code class=""highlighter-rouge"">$2,500</code> USD to join a relevant academic conference or workshop, additionally, the 1st winning team will be awarded a second MARLO Travel Grant with a maximum value of <code class=""highlighter-rouge"">$2,500</code> USD to join the <a href=""https://www.appliedmldays.org/"" target=""_blank"">Applied Machine Learning Days 2019</a> and (2) three winning teams will be awarded Microsoft Azure Sponsorship with a maximum value of <code class=""highlighter-rouge"">$10,000</code> USD for the 1st place, <code class=""highlighter-rouge"">$5,000</code> USD for the 2nd place and <code class=""highlighter-rouge"">$3,000</code> USD for the 3rd place. Please see the “Challenge Prizes” section below for further details.</p>

<h2 id=""what-are-the-start-and-end-dates"">What are the start and end dates?</h2>

<p>The Challenge starts at 00:01 Pacific Standard Time, on July 27, 2018, and the Qualify Round ends at 23:59 Pacific Standard Time, on December 31, 2018 (“Entry Period”). Entries must be received within the Entry Period to be eligible.</p>

<p>The Kick-off Tournament is held in <a href=""https://marlo-ai.github.io/"" target=""_blank"">MARLO workshop</a> of AIIDE’18, the 14th <a href=""https://sites.google.com/ncsu.edu/aiide-2018/"" target=""_blank"">AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment</a> held at the University of Alberta in Edmonton, AB, Canada on November 14, 2018.</p>

<p>The Final Tournament will be held offline in a week after deadline to decide winners.</p>

<h2 id=""games-and-tasks"">Games and Tasks</h2>

<p>One of the main features of this competition is that agents play in multiple games. Therefore, several tasks are proposed for this contest. For the purpose of this document and the competition itself, we deﬁne:</p>

<ul>
  <li>
    <p>Game: each one of the diﬀerent scenarios in which agents play.</p>
  </li>
  <li>
    <p>Task: each instance of a game. Tasks, within a game, may be diﬀerent to each other in terms of level layout, size, diﬃculty and other game-dependent settings.</p>
  </li>
</ul>

<p><img src=""https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/ce3722de6fc358b980aab61893b6ffa0_Figure1.png"" alt=""Figure1.png"" />
Figure 1: Sketch of how games and tasks are organized in the Challenge.</p>

<p>As can be seen, tasks will be of public nature and accessible by the participants, while others are secret and will be used to evaluate the submitted entries at the end of the competition.</p>

<p>Tasks are distributed across sets:
<img src=""https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/1cf1be0ffe8090ed993adfcb63580163_Figure2.png"" alt=""Figure2.png"" />
Figure 2: On the left: Build Battle, where players need to recreate a structure (in this case, the structure is shown on the ground). On the right: Pig Chase, where players collaborate to corner the pig.</p>

<h2 id=""competition"">Competition</h2>

<p>To participate in the Challenge, you will first need to register as a user on crowdai.org and entry the Challenge. Then you start playing with the Challenge by simply cloning the <a href=""https://github.com/crowdAI/marLo"" target=""_blank"">starter-kit of the Challenge on GitHub</a>. Your participation in the Challenge happens through a self-hosted GitLab instance of crowdAI. You need to create a “private” repository with your initial code (or simply code copied from the starter kit). You can make a submission by creating a new git tag and pushing the tag on your repository. crowdAI bot identifies your push tag, automatically clone your repository into a docker image and run an evaluation. After evaluation, the <a href=""https://www.crowdai.org/challenges/marlo-2018/leaderboards"" target=""_blank"">leaderboard</a> is updated. You can find more detail about the actual submission process in the starter-kit.</p>

<p>Your code (and also the corresponding issues for every submission) will stay private during the Challenge. You are encouraged to add an opensource license of your choice. At the end of the Challenge, you will be provided a time period, within which you can decide to keep your code private and not making it publicly available. Else your repository will be made public by default. If the participant has not already added a License to the repository, a MIT license will be automatically added to the repository before making it public. Those who object to having their code made public, will be excluded from the automated process of making all the Challenge specific repositories public.</p>

<p>We are not responsible for entries that we do not receive for any reason, or for entries that we receive but are not decipherable for any reason. We will automatically disqualify any incomplete or illegible entries.</p>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>
<p>## Winner Selection</p>

<h3 id=""qualify-round"">Qualify Round</h3>

<p><strong>Round 1 – until October 21, 2018</strong></p>

<ul>
  <li>Warm up round. Submitted agent to work for a single task.</li>
</ul>

<p><strong>Round 2 – October 21, 2018 – December 31, 2018</strong></p>

<ul>
  <li>Submitted agent to work with fixed random agent as its opponent.</li>
</ul>

<p>At the close of the Entry Period (on December 31, 2018), the Organizer will select 8-32 qualifying teams from eligible entries based upon the overall performance on the Competition tasks (i.e. game score) to invite them to the Final Tournament.</p>

<p>The decisions of the Organizer are final and binding. If we do not receive a sufficient number of entries meeting the entry requirements, we may, at our discretion, select fewer qualifying teams to the tournament.</p>

<h3 id=""kick-off-tournament"">Kick-off Tournament</h3>

<p>Kick-off Tournament is organized in a form of live competition at the workshop in AIIDE 2018. A few Invited teams are broken into groups. Teams on each group play among themselves as the same format with Final Tournament as below.</p>

<h3 id=""final-tournament"">Final Tournament</h3>

<p>The Final Tournament is organized offline in a week after the deadline of December 31, 2018. The 8-32 invited teams are broken into 8 groups. Teams on each group play among themselves (in which we here called a “league”) to determine a ranking, and the top 2 teams of these players progresses to the next round. Each league (P players in a group) is played across the same N games, with T repetitions played per game. Each game has its own leaderboard of agents ranked by the quality of the players. These leaderboards award ranking points to the entries, following a Formula 1-like scheme: 25 points for the 1st ranked entry, 18 for the 2nd, 15, 12, 10, 8, 6, 4, 2 and 1 for positions in the ranking from 3rd to 10th respectively. No points are awarded for positions 11th and below. The winner of the league is determined by adding all ranking points obtained in the diﬀerent games of the challenge.
<img src=""https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/495f17c83f2f2a4f5deb6d6663d6a4dc_Figure3.png"" alt=""Figure3.png"" />
Figure 3: MARLO Final Tournament</p>

<p>If any team member is a potential winner and is 18 years of age or older, but is considered a minor in their place of legal residence, we may require that person’s parent or legal guardian to sign all required forms on their behalf. If that person does not complete the required forms as instructed and/or return the required forms within the time period listed on the winner notification message, we may disqualify the relevant team and select a runner-up.</p>

<p>If your team is confirmed as a winner of the Challenge:</p>

<ul>
  <li>
    <p>It may not designate another party as the winner. If your team is unable or unwilling to accept its prize, we may award it to a runner-up; and</p>
  </li>
  <li>
    <p>Team members will be solely responsible for any taxes that may be payable in connection with the award of any prize.</p>
  </li>
</ul>

<h3 id=""resources"">Resources</h3>

<p>Please visit the starter-kit page first for our comprehensive instruction to start this competition.</p>

<ul>
  <li>Starter-kit: <a href=""https://github.com/crowdAI/marLo"" target=""_blank"">https://github.com/crowdAI/marLo</a></li>
</ul>

<p>The original resource of Project Malmo platform is available on the GitHub.</p>

<ul>
  <li>GitHub: <a href=""https://github.com/Microsoft/malmo"" target=""_blank"">https://github.com/Microsoft/malmo</a></li>
  <li>Project Malmo website: <a href=""https://www.microsoft.com/en-us/research/project/project-malmo/"" target=""_blank"">https://www.microsoft.com/en-us/research/project/project-malmo/</a></li>
</ul>

<p>Follow twitter for the latest information of Project Malmo</p>

<ul>
  <li>Twitter: <a href=""https://twitter.com/Project_Malmo"" target=""_blank"">@Project_Malmo</a></li>
</ul>

<h2 id=""contact-us"">Contact Us</h2>

<ul>
  <li>Gitter Channel : <a href=""https://gitter.im/Microsoft/malmo"" target=""_blank""> https://gitter.im/Microsoft/malmo </a></li>
  <li>Discussion Forum : <a href=""https://www.crowdai.org/challenges/marlo-2018/topics"" target=""_blank""> https://www.crowdai.org/challenges/marlo-2018/topics </a></li>
</ul>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at:</p>

<ul>
  <li><a href=""mailto:sharada.mohanty@epfl.ch"" target=""_blank"">sharada.mohanty@epfl.ch</a></li>
  <li><a href=""mailto:malmoadm@microsoft.com"" target=""_blank"">malmoadm@microsoft.com</a></li>
</ul>

<h3 id=""prizes"">Prizes</h3>

<h3 id=""marlo-travel-grant-prize"">MARLO Travel Grant Prize</h3>

<p>The top 7 teams (3 Progress Award at Kick-off Tournament and the rest of 4 Award at Final Tournament) will be awarded a MARLO Travel Grant with a maximum value of <code class=""highlighter-rouge"">$2,500</code> USD for the team members to join a relevant conference to publish their competition result. The conference is within a year after finishing the final tournament and it is decided later based on mutual agreement between Organizer and the winning team. Additionally, the 1st winning team will be awarded a second MARLO Travel Grant with a maximum value of <code class=""highlighter-rouge"">$2,500</code> USD to join the <a href=""https://www.appliedmldays.org/"" target=""_blank"">Applied Machine Learning Days 2019</a>. The winner of Progress Award at Kick-off Tournament and the winner of Award at the Final Tournament can be overlapped as long as the Organizer identify some improvements about the submission made by the relevant teams. Organizer will reimburse the following reasonably and necessarily incurred travel expenses of team members in attending the conference: (1) economy class roundtrip airfares from your nearest airport to the city of the conference; and (2) accommodation up to 3 days. Organizer will not cover meals during your attendance at the conference.</p>

<h3 id=""microsoft-azure-sponsorship-prize"">Microsoft Azure Sponsorship Prize</h3>

<p>Three teams will each win a Microsoft Azure Sponsorship at Final Tournament with a maximum value of <code class=""highlighter-rouge"">$10,000</code> USD for the 1st place, <code class=""highlighter-rouge"">$5,000</code> USD for the 2nd place and <code class=""highlighter-rouge"">$3,000</code> USD for the 3rd place. The Microsoft Azure Sponsorship will be awarded to the team lead nominated by the relevant team. Team leads will be solely responsible for the allocation of the grant between the relevant team members. The award of a Microsoft Azure Sponsorship will be subject to each team member agreeing to comply with such terms and conditions of use or other requirements that Microsoft may impose.</p>

<h3 id=""azure-for-students"">Azure for Students</h3>

<p>Azure for Students gets you started with $100 in Azure credits to be used within the first 12 months plus select free services (subject to change) without requiring a credit card at sign-up. This is not mandatory for the Challenge. We recommend you take a look at the portal site if you would get additional computing resources: <a href=""https://azure.microsoft.com/en-us/free/students/"" target=""_blank"">https://azure.microsoft.com/en-us/free/students/</a>.</p>

<h3 id=""winners-list-and-sponsor"">Winner’s List and Sponsor</h3>

<p>We will notify the winning teams by November 17, 2018. We will also post a leaderboard detailing the top 32 team entries online on crowdAI and under Microsoft Research website. Please note that we will only post team names and we will not post the names of any individual team members. This list will remain posted for a period of at least 12 calendar months.</p>

<p>If your team’s entry is in a public repository we may also post a link to that repository. Please check your GitLab and GitHub settings to ensure that this will not result in individual team members becoming identifiable unless this is intended by the individual(s) in question.</p>

<p>This promotion is sponsored by Microsoft Corporation, One Microsoft Way, Redmond, WA 98052-6399, USA.</p>

<h2 id=""timeline"">Timeline</h2>

<ul>
  <li>July 27th, 2018: Competition Open</li>
  <li>October 21st, 2018: Qualifying Round 1 submission deadline</li>
  <li>November 14th, 2018: Kick-off Tournament</li>
  <li>December 31st, 2018: Qualifying Round 2 submission deadline</li>
  <li>Early January 2019: Final Tournament</li>
</ul>

<h2 id=""organizing-team"">Organizing Team</h2>

<p>The organizing team comes from multiple groups — Queen Mary University of London, École Polytechnique Fédérale de Lausanne, and Microsoft Research.</p>

<p>The organizing team consists of:</p>

<ul>
  <li>Diego Perez-Liebana (Queen Mary University of London)</li>
  <li>Raluca D. Gaina (Queen Mary University of London)</li>
  <li>Daniel Ionita (Queen Mary University of London)</li>
  <li>Sharada Prasanna Mohanty (École Polytechnique fédérale de Lausanne)</li>
  <li>Sam Devlin (Microsoft Research)</li>
  <li>Andre Kramer (Microsoft Research)</li>
  <li>Sean Kuno (Microsoft Research)</li>
  <li>Katja Hofmann (Microsoft Research)</li>
</ul>

<h2 id=""sponsors"">Sponsors</h2>

<ul>
  <li>Microsoft</li>
</ul>

<h3 id=""datasets-license"">Datasets License</h3>

"
194,"<p>SBB Swiss Federal Railways manages one of the most densely-used mixed-traffic railway networks in the world. Every day we transport over 1.2 million people and carry 210’000 ton-kilometers of freight on more than 10’000 trains, all while achieving world-leading punctuality metrics.</p>

<p>While average utilisation of our infrastructure (measured in average number of trains per kilometer of track) is already second-to-none, traffic is expected to increase even further. There are also financial incentives to use the available infrastructure to its maximum. However, before trains can run, they must first be accepted into the timetable.</p>

<p>Generating a railway timetable is known to be an NP-hard problem. While scheduling few trains is easy, complications quickly explode with increasing number of trains. The most important factors contributing to complexity are interdepencies between trains (such as connections) and the inability of trains to overtake one another on the same track.</p>

<p>Our goal with this challenge is to solicit ingenious ways to tackle the timetable generation/optimization problem. Do you see a suitable algorithm? A promising AI-approach? A powerful heuristic? We can’t wait to see it in action!</p>

<p>We provide you with a set of sample problem instances consisting of a list of trains to be scheduled, their commercial requirements to be respected and a set of routes they can take through the network. Your challenge is to come up with a timetable for these problem instances.</p>

<p><img src=""https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/1f9a8f0b83d464ebc5933ac806ca0c7e_dark-evening-light-trails-434415.jpg"" alt=""dark-evening-light-trails-434415.jpg"" /></p>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>
<p>Each train in a problem instance will define a latest desired arrival times at its stops. It will also define its importance, relative to other trains.</p>

<p>The evaluation criterion for each individual solution will be the weighted sum of all delays. The delay is zero if all trains are scheduled such that they arrive at each of their stops no later than desired. There is also the possibility that some routes for a train are more desirable than others, in which case a solution incurs additional “routing penalty” if an undesired route is chosen.</p>

<p>The overall evaluation criterion will be the sum of each individual problem instance. Missing or invalid solutions are penalized with a large constant, so that it is generally better to find valid solutions to as many instances as possible, even if these solutions have large delays/routing penalties.</p>

<p>If you would like to know the precise formulation, head over to our <a href=""https://github.com/crowdAI/train-schedule-optimisation-challenge-starter-kit"">Starter Kit</a>, in particular the formal <a href=""https://github.com/crowdAI/train-schedule-optimisation-challenge-starter-kit/blob/master/documentation/business_rules.md#objective-function"">definition of the objective function</a>. Be advised that that definition is rather technical. It is important to understand the meaning behind it.</p>

<h3 id=""resources"">Resources</h3>

<p>This Challenge is a little bit special in that to provide solutions, you need to understand a few things about the data format of the problem instances and the solutions, as well as the rules that a solution must adhere to.</p>

<p>We have put together a Starter Kit that contains the necessary documentation in, hopefully, easily understandable form, works through some step-by-step examples and provides some utility scripts that help you get startet. Please head over to the <a href=""https://github.com/crowdAI/train-schedule-optimisation-challenge-starter-kit/#starter-kit-repo-for-the-sbb-train-schedule-optimisation-challenge-on-crowdai"">Starter Kit</a>, where the README should guide you through the content available there.</p>

<p>In case of questions about the material, please do not hesitate to contact us.</p>

<p>Here are some interesting blog posts:</p>

<ul>
  <li><a href=""https://medium.com/crowdai/can-you-make-swiss-trains-even-more-punctual-ec9aa73d6e35"">Can You Make Swiss Trains Even More Punctual?</a></li>
</ul>

<h2 id=""contact-us"">Contact Us</h2>

<h3 id=""for-challenge-related-questions-technical-andor-content-questions"">For Challenge-related questions (technical and/or content questions)</h3>

<ul>
  <li>Gitter Channel : <a href=""https://gitter.im/crowdAI/sbb-challenges"">crowdAI/sbb-challenges</a></li>
  <li>Technical Issues : <a href=""https://github.com/crowdAI/train-schedule-optimisation-challenge-starter-kit/issues"">https://github.com/crowdAI/train-schedule-optimisation-challenge-starter-kit/issues</a></li>
  <li>Discussion Forum : <a href=""https://www.crowdai.org/challenges/train-schedule-optimisation-challenge/topics"">https://www.crowdai.org/challenges/train-schedule-optimisation-challenge/topics</a></li>
</ul>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

<ul>
  <li><a href=""mailto:julian.jordi@sbb.ch"" target=""_blank"">julian.jordi@sbb.ch</a></li>
  <li><a href=""mailto:sharada.mohanty@epfl.ch"" target=""_blank""> sharada.mohanty@epfl.ch
 </a></li>
</ul>

<h3 id=""for-press-inquiries"">For Press inquiries</h3>
<p>Please contact SBB Media Relations at <a href=""mailto:press@sbb.ch"">press@sbb.ch</a></p>

<h3 id=""prizes"">Prizes</h3>

<p>The top three submissions will be awarded the following cash prizes (in Swiss Francs):</p>

<ul>
  <li><strong>CHF 7’000.- for first prize</strong></li>
  <li><strong>CHF 5’000.- for second prize</strong></li>
  <li><strong>CHF 3’500.- for third prize</strong></li>
</ul>

<p>In addition, we allow for the possibility of awarding several travel grants to the <a href=""https://www.appliedmldays.org/"">Applied Machine Learning Days 2019</a> in Lausanne, Switzerland. Participants with promising solutions may be invited to present their solutions in person.</p>

<p>Note that the travel grants are not automatically awarded to the top-rated submissions. It may be possible that a submission does not score very highly because, for example, only half of the problem instances could be solved. However, the approach used demonstrates an original and promising idea that SBB would like to expand upon. In this case, SBB would reserve the right to award a travel grant to such a submission.</p>

<p>In case more than one submission receives the same score, the tie will be broken as follows
1. preference to the submission which requires less total computation time
2. preference to the submission which requires less computing resources
3. if neither of these can be adequately measured or distinguished, the organizers break the ties or decide to award joint prizes.</p>

<p><img src=""https://crowdai-shared.s3.eu-central-1.amazonaws.com/markdown_editor/3f97ddda4fe1141ff8eb79d883aa3fb5_train.png"" alt=""train.png"" /></p>

<h3 id=""datasets-license"">Datasets License</h3>

"
195,"<p>Anonymisation of data is a long term scientific question that is not yet solved. With the rising of open data, we’re in need of good anonymisation algorithms. This is why we held the first international Data Anonymisation and Re-identification Competition (DARC).</p>

<p>This competition take place in two rounds :
    - The first round is dedicated to the anonymization of the ground truth data.
    - The second to the re-identification of other player’s anonymized data.</p>

<h2 id=""dataset"">Dataset</h2>
<p>You can download the datasets in the <a href=""https://www.org/challenges/data-anonymization-and-re-identification-competition-darc/dataset_file"">Datasets Section</a>. You will find <code class=""highlighter-rouge"">ground_truth.csv</code>, which is the transactional histroy of 4000 users from the UCI dataset <code class=""highlighter-rouge"">Online Retail</code>.</p>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>
<p>You will be evaluated on two aspects : <strong>utility</strong> and <strong>re-identification rate</strong>.</p>

<p><strong>Utility</strong> is computed by taking the maximum of 6 metrics (named $E_i$). Each metrics return a score between 0 and 1, 0 being the best score.</p>

<p><strong>Re-identification</strong>  is computed by taking the maximum of 6 metrics (named $S_i$). Each metrics return a score between 0 and 1, 0 being the best score.</p>

<p>For more information about the metrics , please read the <a href=""https://doi.org/10.5281/zenodo.1890775"">rules</a></p>

<p>The score of a file is computed as follow : $\frac{Utility + Re-identification}{2}$</p>

<h3 id=""resources"">Resources</h3>

<h3 id=""prizes"">Prizes</h3>

<h3 id=""datasets-license"">Datasets License</h3>

"
196,"
<h3 id=""evaluation-criteria"">Evaluation criteria</h3>

<h3 id=""resources"">Resources</h3>

<h3 id=""prizes"">Prizes</h3>

<h3 id=""datasets-license"">Datasets License</h3>

"
198,"<p><strong>Starter Kit</strong> : <a href=""https://github.com/crowdAI/league-of-nations-archives-digitization-challenge-starter-kit"">https://github.com/crowdAI/league-of-nations-archives-digitization-challenge-starter-kit</a></p>

<p>The documents in the archives of the <a href=""https://www.unog.ch/80256EDD006AC19C/(httpPages)/242056AEA671DEF780256EF30037A2A8?OpenDocument"" target=""lon_history""> League of Nations </a> (1919-1946) contain important historical information about the origins of the United Nations that are relevant to understanding the UN and its many roles today, as well being a critical resource for the history of international relations during the interwar period. The official documents in particular are a vital source for researchers, as they provide the official output of the various bodies of the League, from the well-known League Council and Assembly, to the most technical sub-committees and conferences, including minutes, final reports, and official working papers.</p>

<p>Because these data exist only in the form of paper archives at the <a href=""https://www.unog.ch/library"" target=""un_library_page""> UN Library in Geneva </a>, the digital transformation of the documents represents an important challenge. Digitization of these archives will enable far wider access to these documents, which are currently only accessible to researchers who can visit the archives in person. Furthermore, while digitization itself is obviously critical, providing intellectual access to the documents is no less necessary.</p>

<p>Currently, there are no comprehensive indexes or catalogues of the official documents collection, and it is therefore necessary to provide basic points of access to the materials, including the title, document symbol, date, and language of each document. This is currently done in a largely manual process, which is extremely labour intensive and relatively slow.</p>

<p>The documents themselves unfortunately present further challenges to rendering their contents accessible. There is a wide variety in the formats and layouts of title pages, titles may be complex and multi-faceted, and documents exist in multiple languages, including bi- and multi-lingual texts. Moreover, the quality of the printing and reproduction methods used has often resulted in poor quality text, for which it can be difficult to achieve good results using Optical Character Recognition (OCR) analyses.</p>

<p>The training dataset contains more than 4500 documents in English or French. 
More details can be found in the <a href=""https://www.crowdai.org/challenges/league-of-nations-archives-digitization-challenge/dataset_files"">Dataset</a> of the challenge.</p>

<p><strong>The challenge is to identify the language in each of the documents accurately using any model for prediction.</strong></p>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>
<p>The primary metric for evaluation will be the Mean <a href=""https://en.wikipedia.org/wiki/F1_score"">F1-Score</a>, and the secondary metric for the evaluation with be the Mean <a href=""http://scikit-learn.org/stable/modules/model_evaluation.html#log-loss"">Log Loss</a></p>

<p>The <strong>Mean Log Loss</strong> is defined by</p>

<p>$ L = - \frac{1}{N} \sum_{n=1}^N \sum_{c=1}^{C} y_{nc} \ln(p_{nc}), $</p>

<p>where</p>

<ul>
  <li>$N= 14216$ is the number of examples in the test set,</li>
  <li>$C=2$ is the number of class labels, i.e. languages : [en, fr],</li>
  <li>$y_{nc}$ is a binary value indicating if the n-th instance belongs to the c-th label,</li>
  <li>$p_{nc}$ is the probability according to your submission that the n-th instance belongs to the c-th label,</li>
  <li>$\ln$ is the natural logarithmic function.</li>
</ul>

<p>The $F_1$ score for a particular class $c$ is given by</p>

<p>$ F_1^c = 2\frac{p^c r^c}{p^c + r^c}, $</p>

<p>where</p>

<ul>
  <li>$p^c = \frac{tp^c}{tp^c + fp^c}$ is the precision for class $c$,</li>
  <li>$r^c = \frac{tp^c}{tp^c + fn^c}$ is the recall for class $c$,</li>
  <li>$tp^c$ refers to the number of True Positives for class $c$,</li>
  <li>$fp^c$ refers to the number of False Positives for class $c$,</li>
  <li>$fn^c$ refers to the number of False Negatives for class $c$.</li>
</ul>

<p>The final <strong>Mean $F_1$ Score</strong> is then defined as</p>

<p>$ F_1 = \frac{1}{C} \sum_{c=1}^{C} F_1^c. $</p>

<p>The participants have to submit a CSV file with the following header:</p>

<p><code class=""highlighter-rouge"">
filename,en,fr
</code></p>

<p>Each row is then an entry for every file in the test set (in the sorted order of the <code class=""highlighter-rouge"">filename</code>s). The first column in every row represents the <code class=""highlighter-rouge"">filename</code> (which is the name of the test file with its ‘.jpg’ extension) and the rest of the $C=2$ columns are the predicted probabilities for each class in the order mentioned in the above CSV header.</p>

<p>A sample row would look like. :</p>

<p><code class=""highlighter-rouge"">
58dc3fae0f039187c6615393b31c4978.jpg, 0.765, 0.235
</code></p>

<p>which means that for the image in the file <code class=""highlighter-rouge"">58dc3fae0f039187c6615393b31c4978.jpg</code>, you are <code class=""highlighter-rouge"">76.5%</code> confident that it belongs to the class <code class=""highlighter-rouge"">en</code> and <code class=""highlighter-rouge"">23.5%</code> confident that it belongs to the class <code class=""highlighter-rouge"">fr</code>.</p>

<p>An example of a French document will be:
<img src=""https://preview.ibb.co/g2dV0p/C_1930_345_410_0022_result.jpg"" alt="" french_lon "" /></p>

<p>An example of an English document will be:
<img src=""https://preview.ibb.co/knpdLp/C_1930_345_410_0026_result.jpg"" alt="" english_lon "" /></p>

<h3 id=""resources"">Resources</h3>

<ul>
  <li>
    <p>The starter kit for making submissions can be found at : <a href=""https://github.com/crowdAI/league-of-nations-archives-digitization-challenge-starter-kit"">https://github.com/crowdAI/league-of-nations-archives-digitization-challenge-starter-kit</a></p>
  </li>
  <li>
    <p>More information about the background of the project can also be found at: <a href=""https://www.zooniverse.org/projects/nshreyasvi/league-of-nations-in-the-digital-age"" target=""zooniversecampaign""> Crowdsourcing Campaign </a></p>
  </li>
</ul>

<h2 id=""contact-us"">Contact Us</h2>

<p>Use one of the public channels:</p>

<ul>
  <li>Gitter Channel : <a href=""https://gitter.im/crowdAI/un_library_challenge"">crowdAI/un_library_challenge</a></li>
  <li>Discussion Forum : <a href=""https://www.crowdai.org/challenges/league-of-nations-archives-digitization-challenge/topics"">https://www.crowdai.org/challenges/league-of-nations-archives-digitization-challenge/topics</a></li>
</ul>

<p>We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers. In extreme cases, if there are any queries or comments that you would like to make using a private communication channel, then you can send us an email at :</p>

<ul>
  <li><a href=""mailto:sharada.mohanty@epfl.ch"" target=""_blank"">sharada.mohanty@epfl.ch</a></li>
</ul>

<h3 id=""prizes"">Prizes</h3>

<p>The winning participant will be extended a travel grant by <a href=""http://www.citizencyberlab.org/"" target=""citizencyberlabwebsite"">Citizen Cyberlab </a> to attend the <a href=""https://www.itu.int/en/ITU-T/AI/2018/Pages/default.aspx"" target=""aiforgoodwebsite""> AI for Good summit </a> 2019 in Geneva, Switzerland.
The travel grant covers the travel and accommodation expenses up to <strong>CHF 1500</strong>.</p>

<h3 id=""datasets-license"">Datasets License</h3>

"
199,"<p>Over the past few years, several methods for trajectory-based activity forecasting have been proposed. However, most techniques have been evaluated on a limited number of sequences. Further, these methods have either been evaluated on different subsets of the available data, using different evaluation scripts, or on contrasting coordinate systems (2D, 3D). These inconsistencies have made it difficult to objectively compare forecasting techniques. One potential solution involves creating a standardized benchmark to serve as an objective measure of performance; despite their potential pitfalls, benchmarks hold great promise in addressing such comparison issues. There have been a limited number of attempts at trajectory forecasting benchmarks, such as the ETH and the UCY datasets. However, a common technique for presenting forecasting results requires both a standard dataset and evaluation metrics. We introduce TrajNet, a new, large scale trajectory-based activity benchmark, that uses a unified evaluation system to test gathered state-of-the-art methods on various trajectory-based activity forecasting datasets. Our benchmark not only covers a wide range of datasets, but also includes various types of targets, from pedestrians to bikers, skateboarders, cars, buses, and golf cars, that navigate in a real world outdoor environment.</p>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>
<p>Multiples evaluations criteria are taken into account. First, the average L2 distance between the prediction and the ground truth allows seeing the accuracy along the paths. This measure is divided into linear and non-linear trajectories. Then, the final L2 displacement shows how well the model will predict the last coordinates. Finally, a new measure is added to what has been presented in papers treating this subject, the collision measure. Indeed, real pedestrians are always trying to avoid collisions, a well-calibrated model should avoid these collisions.</p>

<p>The results will be presented with the following measures:</p>

<p><strong>Overall Rank</strong>:</p>

<ul>
  <li><strong>Better</strong>: lower</li>
  <li><strong>Perfect</strong>: 1</li>
  <li><strong>Description</strong>: This is the rank of each trajectory predictor averaged over all other present evaluation measures. The closer this value is to 1, the better.</li>
</ul>

<p><strong>Overall Average</strong>:</p>

<ul>
  <li><strong>Better</strong>: lower</li>
  <li><strong>Perfect</strong>: 0.0</li>
  <li><strong>Description</strong>: This is the performance of each trajectory predictor averaged over all other present evaluation measures. The closer this value is to 0.0, the better.</li>
</ul>

<p><strong>Final Disp. L2</strong>:</p>

<ul>
  <li><strong>Better</strong>: lower</li>
  <li><strong>Perfect</strong>: 0.0</li>
  <li><strong>Description</strong>: Trajectory prediction accuracy, calculated by taking an L2 average over the displacement at the final position in each of the trajectories in all the datasets.</li>
</ul>

<p><strong>Mean Disp. L2</strong>:</p>

<ul>
  <li><strong>Better</strong>: lower</li>
  <li><strong>Perfect</strong>: 0.0</li>
  <li><strong>Description</strong>: Trajectory prediction accuracy, calculated by taking an L2 average over the displacement at each of the positions in all of the trajectories in all the datasets.</li>
</ul>

<p><strong>Collision</strong>:</p>

<ul>
  <li><strong>Better</strong>: lower</li>
  <li><strong>Perfect</strong>: 0</li>
  <li><strong>Description</strong>: Trajectory prediction collision, calculated by looking at the distance between points at the same frame. If this distance is lower than twice the pedestrian radius (0.2 by default) then, plus one is added to this measure.</li>
</ul>

<h2 id=""data-description"">Data Description</h2>

<p>The files contain two different types of data:</p>

<p><strong>1 - “scene” which helps to define the trajectory of one pedestrian</strong></p>

<p>{“scene”: {“id”: 266, “p”: 254, “s”: 10238, “e”: 10358, “fps”: 2.5, “tag”: 2}}</p>

<p>The “id” is the scene id, the “p” refers to the pedestrian ID and the “s” and the “e” correspond to the starting and ending frames number, at which the pedestrian “p” appears. Furthermore, the fps corresponds to the frame rate between two consecutive frames of the dataset (in our case it’s almost always 2.5). Finally, the tag corresponds to the trajectory type. It can be a value between 0 and 4:</p>

<ul>
  <li>0 corresponds to the undetermined type. It is only the case for the test set
<img src=""https://raw.githubusercontent.com/rodolphefarrando/data-preprocessing-for-traj-pred/master/figure/0_biwi_md.png"" alt=""Static"" /></li>
  <li>If the total euclidian distance of the trajectory is less than 1 meter then the trajectory type is 1
<img src=""https://upload.wikimedia.org/wikipedia/en/7/7d/Lenna_%28test_image%29.png"" alt=""Lena"" /></li>
  <li>Type 2 corresponds to linear trajectories. If a pedestrian makes a change of direction of more than 15 degrees in the prediction part of the trajectory, then it is considered as nonlinear. Otherwise, the trajectory is linear.</li>
  <li>Type 3 is defined as “forced non-linear trajectories”. That is, a pedestrian changing its direction because there is someone in front of him.</li>
  <li>Finally type 4 corresponds to non-linear trajectories without any apparent reasons</li>
</ul>

<p><strong>2 - “track” which complete the scene data, with the coordinates of the pedestrian</strong></p>

<p>{“track”: {“f”: 10238, “p”: 248, “x”: 13.2, “y”: 5.85, “pred_number”: 0}}</p>

<p>“f” is the frame id, “p” is again the pedestrian id and x and y are the coordinates of the pedestrian at frame f. The pred_number is only useful if you are performing a nearest neighbors search. You can add the number of your prediction in this feature.</p>

<p>To have your predictions evaluated, you need to submit a .zip file containing <strong>the exact same names</strong> as the files provided in the dataset. The evaluator will provide a grade for each metric previously mentioned. The files may contain multiple predictions if your model is performing a nearest neighbors search. The maximum number of nearest neighbors is limited to 3.</p>

<h3 id=""resources"">Resources</h3>

<p>In this section, you will find help to submit the correct type of files. You need to complete the tests file provided in the data. There a predefined number of “scene” in the test files, and for each “scene” the 12 last coordinates of the pedestrian of interest are missing. Once you completed the files, you have to submit them with the (<a href=""http://ndjson.org/"" target=""_blank""> .ndjson </a>) extension and they have to have the same format as the provided ones.</p>

<p>A <a href=""https://gitlab.crowdai.org/RodolpheFarrando/trajnetchallenge-guide"" target=""_blank"">guide for the challenge</a> is available.</p>

<p>Other GitHub links also provide some tools and example.</p>

<p><a href=""https://github.com/vita-epfl/trajnetbaselines"" target=""_blank"">Baseline algorithms for TrajNet</a></p>

<p><a href=""https://github.com/vita-epfl/trajnettools"" target=""_blank"">Tools for Trajnet </a></p>

<p><a href=""https://www.google.com"" target=""_blank"">Google </a></p>

<h3 id=""prizes"">Prizes</h3>

<h3 id=""datasets-license"">Datasets License</h3>

"
200,"<p>See detailed instruction see also the <a href=""https://github.com/epfml/ML_course/raw/master/projects/project1/project1_description.pdf"">Project 1 PDF description</a> available on the <a href=""https://mlo.epfl.ch/page-146520.html"">ML course web site</a>.</p>

<h2 id=""file-descriptions"">File descriptions</h2>

<p><strong>train.csv</strong> - Training set of 250000 events. The file starts with the ID column, then the label column (the y you have to predict), and finally 30 feature columns.  <br />
<strong>test.csv</strong> - The test set of around 568238 events - Everything as above, except the label is missing.  <br />
<strong>sample-submission.csv</strong> - a sample submission file in the correct format. The sample submission always predicts -1, that is ‘background’.</p>

<p>For detailed information on the semantics of the features, labels, and weights, see the technical documentation from the LAL website on the task. Note that here for the EPFL course, we use a simpler evaluation metric instead (classification error).</p>

<p><strong>Some details to get started:</strong></p>

<ul>
  <li>all variables are floating point, except PRI_jet_num which is integer</li>
  <li>variables prefixed with PRI (for PRImitives) are “raw” quantities about the bunch collision as measured by the detector.</li>
  <li>variables prefixed with DER (for DERived) are quantities computed from the primitive features, which were selected by the physicists of ATLAS.</li>
  <li>it can happen that for some entries some variables are meaningless or cannot be computed; in this case, their value is −999.0, which is outside the normal range of all variables.</li>
</ul>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>

<h3 id=""resources"">Resources</h3>

<h3 id=""prizes"">Prizes</h3>

<h3 id=""datasets-license"">Datasets License</h3>

"
150,"<p>In the field of <strong>computational neuroscience</strong>, evolutionary optimization algorithms are used more and more often to <strong>search for values of parameters</strong> that are not well directly constrained by experimental data. Due to the many <strong>non-linearities</strong> in <strong>detailed neuron models</strong>, the <strong>search space</strong> is extremely <strong>complex</strong>. Many different types of evolutionary optimization algorithms have been tried to tackle this problem over the years.</p>

<p>The aim of this competition is to <strong>apply optimization algorithms</strong> to neuroscientific experimental constraints 
and a model we provide. We hope to <strong>encourage</strong> a search for <strong>better algorithms</strong> to solve these kinds
of problems.</p>

<p>This competition is part of the <strong>Gecco conference</strong> in Berlin on July 15-19, 2017.</p>

<p>http://gecco-2017.sigevo.org/index.html/Competitions</p>

<h3 id=""organizers"">Organizers</h3>

<p><em>This</em> challenge is organized by the <a href=""http://bluebrain.epfl.ch""><strong>Blue Brain</strong></a> at EPFL.</p>

<h3 id=""evaluation-criteria"">Evaluation criteria</h3>
<p><strong>We will provide</strong> the participants with an API to access the <strong>evaluation function</strong> to optimize.
This function will run a biophysically detailed neuron model using the <a href=""https://github.com/BlueBrain/BluePyOpt"">BluePyOpt</a> and <a href=""https://www.neuron.yale.edu"">NEURON</a> software.</p>

<p>The <strong>parameters</strong> and <strong>objectives</strong> will be <strong>floating point numbers</strong>.
The parameters represent the <strong>densities of the ion channels</strong> at different locations on the membrane of a neuron.
When given a set of densities values, the <strong>evaluation function</strong> runs an <strong>electrical model simulation</strong> and records the <strong>membrane potential</strong> while the neuron is reacting to a set of injection protocols.
The recorded voltage traces are analysed after the simulation, and compared to experimental data. A set of <strong>scores</strong> is calculated based on how far the <strong>model output deviates from experiments</strong>.
The scores need to be <strong>minimized</strong> with a lower bound of 0.</p>

<p>An <strong>example</strong> of a similar optimization problem can be found in the <strong>Neocortical Pyramidal Cell</strong> use case section of:</p>

<p><a href=""http://journal.frontiersin.org/article/10.3389/fninf.2016.00017/full"">Van Geit W, Gevaert M, Chindemi G, Rössert C, Courcol J, Muller EB, Schürmann F, Segev I and Markram H (2016). BluePyOpt: Leveraging open source software and cloud infrastructure to optimise model parameters in neuroscience. Front. Neuroinform. 10:17. doi: 10.3389/fninf.2016.00017</a></p>

<p>and in the following Jupyter notebook:
https://github.com/BlueBrain/BluePyOpt/blob/master/examples/l5pc/L5PC.ipynb</p>

<h3 id=""resources"">Resources</h3>

<h3 id=""prizes"">Prizes</h3>

<p>TBA</p>

<h3 id=""datasets-license"">Datasets License</h3>

"
